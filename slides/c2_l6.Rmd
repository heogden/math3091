---
title: "MATH3012: Chapter 2, Lecture 6"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```


## Recap

- We have seen how to use general asymptotic results about the MLE
to find standard errors of estimates, find confidence intervals,
and test the hypothesis that $\beta_j = 0$, for a single parameter $\beta_j$.
- Now we'll look at testing more general hypotheses in a GLM, using
our general theory about log likelihood ratio tests.
- We'll also look at a test of whether the model appears to fit
the data.

## Comparing candidate models

If we have a set of competing generalised linear models
which might explain the dependence of the response
on the explanatory variables, we will want to determine which of
the models is most appropriate.

As with linear models, we proceed by comparing models pairwise
using a generalised likelihood ratio test.

This is restricted to situations where
one of the models, $H_0$, is *nested* in the other, $H_1$.

For generalised linear models,  'nested' means that $H_0$ and $H_1$ are

1. based on the same exponential family distribution, and
1. have the same link function, but
1. the explanatory variables present in $H_0$ are a subset of
those present in $H_1$.

## Model setup

We will assume that model $H_1$ contains $p$ linear parameters and
model $H_0$ a subset of $q<p$ of these.
Without loss of generality, we can think of $H_1$ as the model
$$
\eta_i=\sum_{j=1}^p x_{ij} \beta_j \qquad i = 1, \ldots, n
$$
and $H_0$ is the same model with
\[\beta_{q+1}=\beta_{q+2}=\cdots=\beta_p=0.\]

Then model $H_0$ is a special case of model $H_1$, where certain coefficients
are set equal to zero, and therefore
$\Theta^{(0)}$, the set of values of the canonical parameter $\bm{\theta}$
allowed by $H_0$, is a subset of $\Theta^{(1)}$, the set of values
allowed by $H_1$.

## Log likelihood ratio statistic

The log likelihood ratio statistic for a test of $H_0$ against $H_1$ is
\[L_{01} \equiv 2\log \left({{\max_{\bm{\theta}\in \Theta^{(1)}} L(\bm{\theta})}\over
{\max_{\bm{\theta}\in \Theta^{(0)}}L(\bm{\theta})}}\right)
=2\log L(\hat{\bm{\theta}}^{(1)})-2\log L(\hat{\bm{\theta}}^{(0)}),\]
where 
\[\hat \theta^{(j)} = (b')^{-1}\left(g^{-1}(\bm x_i^T \hat{\bm \beta^{(j)}})\right), \quad j = 0, 1,\]
 and $\hat{\bm \beta^{(j)}}$ is the MLE of $\bm \beta$ under model $j$.

Here we assume that $a(\phi_i),\;i = 1, \ldots, n$ are known.

\pause

We reject $H_0$ in favour of $H_1$ when $L_{01}$ is  'too large'
(the observed data are much more probable under $H_1$ than $H_0$).

Asymptotically, under $H_0$, $L_{01} \sim \chi^2_{p-q}$, so we 
 we reject $H_0$ in favour of $H_1$ when
$L_{01}$ is greater than the $100(1- \alpha)\%$ point of the  $\chi^2_{p-q}$ distribution.


## Comparing models in `R` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
```{r, include = FALSE}
shuttle <- read.csv("shuttle.csv")
shuttle$n <- rep(6, nrow(shuttle))
```
\scriptsize
```{r}
shuttle_glm0 <- glm(n_damaged / n ~ temp,
                    data = shuttle, family = binomial, weights = n)
shuttle_glm1 <- glm(n_damaged / n ~ temp + orbiter,
                    data = shuttle, family = binomial, weights = n)
```

\normalsize
Here `temp` is a continuous variable, and `orbiter` is a factor
with $4$ levels. What is $q$ (the number of parameters in `shuttle_glm0`?)

- 1
- 2
- 3
- 4
- 5
- 6
- 7

## Comparing models in `R` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
```{r, include = FALSE}
shuttle <- read.csv("shuttle.csv")
shuttle$n <- rep(6, nrow(shuttle))
```
\scriptsize
```{r}
shuttle_glm0 <- glm(n_damaged / n ~ temp,
                    data = shuttle, family = binomial, weights = n)
shuttle_glm1 <- glm(n_damaged / n ~ temp + orbiter,
                    data = shuttle, family = binomial, weights = n)
```

\normalsize
Here `temp` is a continuous variable, and `orbiter` is a factor
with $4$ levels. What is $p$ (the number of parameters in `shuttle_glm1`?)

- 1
- 2
- 3
- 4
- 5
- 6
- 7


## Comparing models in `R` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Now we conduct a log likelihood ratio test to compare the two models.
\scriptsize
```{r}
anova(shuttle_glm0, shuttle_glm1)
```
\normalsize
Here $L_{01}$ is $1.0238$, given in the deviance column.

Under $H_0$, what is the (asymptotic) distribution of $L_{01}$?

- $\chi^2_1$
- $\chi^2_3$
- $\chi^2_{18}$
- $\chi^2_{21}$

## Finding the critical value ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

\scriptsize
```{r}
anova(shuttle_glm0, shuttle_glm1)
```

\normalsize
We should reject $H_0$ if $L_{01} > k$. Which of the following
commands could be used to find $k$, for a test at the $5\%$ level?

- `dchisq(0.95, df = 3)`
- `pchisq(0.95, df = 3)`
- `qchisq(0.95, df = 3)`

## Finding the $p$-value ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

\scriptsize
```{r}
anova(shuttle_glm0, shuttle_glm1, test = "LRT")
```

\normalsize
The final column gives a $p$-value: the probability that
$L_{01} \geq 1.0238$, if $H_0$ is true.
Here the $p$-value is $0.7955$, so we do not reject $H_0$.

Which of the following
commands could be used to find the $p$-value?

- `dchisq(1.0238, df = 3)`
- `1 - dchisq(1.0238, df = 3)`
- `pchisq(1.0238, df = 3)`
- `1 - pchisq(1.0238, df = 3)`
- `qchisq(1.0238, df = 3)`
- `1 - qchisq(1.0238, df = 3)`


## The saturated model

Consider a model where $\bm{\beta}$ is $n$-dimensional, and therefore
$\bm{\eta}=\bm{X}\bm{\beta}$. Assuming that $\bm{X}$ is invertible, then this model
places no constraints on the linear predictor $\bm{\eta}=(\eta_1,\ldots ,\eta_n)$.
It can take any value in $\mathbb{R}^n$.

Correspondingly the means $\bm{\mu}$ and the canonical parameters $\bm{\theta}$ are
unconstrained.

The model is of dimension $n$ and can be parameterised equivalently
using $\bm{\beta}$, $\bm{\eta}$, $\bm{\mu}$ or $\bm{\theta}$.
Such a model is called the *saturated* model.

## Maximum likelihood estimation in the saturated model
As the canonical parameters $\bm{\theta}$ are unconstrained, we can calculate their
maximum likelihood estimates $\hat{\bm{\theta}}$ directly from their likelihood
(without first having to calculate $\hat{\bm{\beta}}$)
\[\ell(\bm{\theta})=\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}
\over{a(\phi_i)}}+\sum_{i=1}^nc(y_i,\phi_i).\]

\pause

We obtain $\hat{\bm{\theta}}$ by first differentiating with respect to
$\theta_1,\ldots ,\theta_n$ to give
$$
u_k(\bm \theta) = {\partial\over{\partial\theta_k}}\ell(\bm{\theta})={{y_k-b'(\theta_k)}
\over{a(\phi_k)}},\quad k=1,\ldots ,n.
$$
So $b'(\hat{\theta}_k)=y_k,\;k=1,\ldots ,n$, so $\hat{\mu}_k=y_k,\;k=1,\ldots ,n$. 

\pause
The saturated model fits
the data perfectly, as the *fitted values* $\hat{\mu}_k$ and observed
values $y_k$ are the same for every observation $k=1,\ldots ,n$.

## Using the saturated model to check goodness of fit

The saturated model is rarely of any scientific interest in its own right.
It is highly parameterised, having as many parameters as there are
observations. This goes against our desire for parsimony in a model.

However, every other model is nested in the saturated model,
and a test comparing a model $H_0$ against the saturated model $H_S$ can be
used as a goodness of fit test. 

If the saturated model, which fits the
observed data perfectly, does not provide a significantly better fit than
model $H_0$, we can conclude that $H_0$ is an acceptable fit to the data.

## Testing $H_0$ against $H_S$

The log likelihood ratio statistic for a test of $H_0$ against $H_S$ is
$$
L_{0s}=2\log L(\hat{\bm{\theta}}^{(s)})-2\log L(\hat{\bm{\theta}}^{(0)}),
$$
where $\hat \theta^{(s)}_k = (b')^{-1}(y_k)$.

\pause

Under $H_0$, $L_{0s}$ has an asymptotic $\chi^2_{n-q}$ distribution. 

Therefore, if $L_{0s}$ is  'too large'
(for example, larger than the 95\% point of the $\chi^2_{n-q}$ distribution)
then we reject $H_0$ as a plausible model for the data, as it does not
fit the data adequately.

The *degrees of freedom*
of model $H_0$ is defined to be the degrees of freedom for this test, $n-q$,
the number of observations minus the number of parameters in $H_0$.

## The scaled deviance

We call $L_{0s}$ the *scaled deviance* (`R` calls it
the *residual deviance*) of model $H_0$.

We can write the scaled deviance of model $H_0$ as
\[L_{0s}=2\sum_{i=1}^n{{y_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i]
-[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]}
\over{a(\phi_i)}}\]
which can be calculated using the observed data, provided that $a(\phi_i),\;
i = 1, \ldots, n$ is known.

## Scaled deviance in `R`

\scriptsize
```{r}
summary(shuttle_glm0)
```

## Interpreting the scaled deviance

For the model `shuttle_glm0`, the scaled deviance (called residual deviance in `R`)
is $18.086$ on $21$ degrees of freedom.

We have
```{r}
qchisq(0.95, df = 21)
```
which is greater than $18.086$.

The value of the scaled deviance is consistent with the model
`shuttle_glm0`, and does not cause us to be concerned about
lack of fit to the data.

## Conclusion

- We have used the general results about the asymptotic distribution
of the log likelihood ratio statistic to conduct generalised likelihood
ratio tests for GLMs.
- We have seen how to use the scaled deviance as a goodness of fit test.
- Next time, we'll see how to use the scaled deviance to calculate 
log likelihood ratio statistics, and look at an alternative measure of model fit.
- We'll also look at what happens when $a(\phi)$ is unknown.
- You should now be able to complete Questions 1 and 2 on problem sheet 4.