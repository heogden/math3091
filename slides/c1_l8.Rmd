---
title: "MATH3012: Chapter 1, Lecture 8"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```


## Recap

- We have seen how the likelihood can be used to estimate parameters (MLE),
and we have used large-sample results to find approximate confidence
intervals and hypothesis tests based on the likelihood.
- Last time, we reviewed the linear model from MATH2010. The linear model
depends on unknown parameters, and we could use general likelihood
results to estimate the parameters, and to describe our uncertainty about them.

## Maximum likelihood estimation

The linear model has unknown parameters $\bm \theta = (\beta_1, \ldots, \beta_p, \sigma)$.
How can we estimate those parameters?

The likelihood for a linear model is
\[L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{x}_i^T\bm{\beta})^2\right).\]
This is maximised with respect to $(\bm{\beta},\sigma^2)$ at
$$
\hat{\bm{\beta}}=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
$$
and
$$
\hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n \left(y_i-\bm{x}_i^T\hat{\bm{\beta}}\right)^2.
$$

## Fitted values and residuals
The corresponding fitted values are
\[\hat{\bm{y}}=\bm{X}\hat{\bm{\beta}}=\bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}\]
or
\[\hat{y}_i=\bm{x}_i^T\hat{\bm{\beta}}, \quad i = 1, \ldots, n.\]

The residuals $\bm{r} = (r_1, \ldots, r_n)$ are
$\bm{r}=\bm{y}-\bm{\hat y}$
or $r_i=y_i-\bm{x}_i^T\hat{\bm{\beta}}$ for $i = 1, \ldots, n.$.
These residuals describe the variability in the observed responses $y_1, \ldots, y_n$ which
has not been explained by the linear model.
We call
\[D= \sum_{i=1}^n r_i^2 = \sum_{i=1}^n \left(y_i-\bm{x}_i^T\hat{\bm{\beta}}\right)^2\]
the *residual sum of squares* or *deviance* for the linear model.

## Distribution of $\hat{\bm \beta}$

As $\bm Y$ is normally distributed, and $\hat{\bm{\beta}}= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{Y}$ is
a linear function of $\bm Y$, then $\hat{\bm{\beta}}$
 must also be normally distributed.
 We have
$E(\hat{\bm{\beta}})=\bm{\beta}$ and $\text{Var}(\hat{\bm{\beta}})=\sigma^2(\bm{X}^T\bm{X})^{-1}$,
so
 \[\hat{\bm{\beta}} \sim N(\bm{\beta}, \sigma^2(\bm{X}^T\bm{X})^{-1}).\]

## Distribution of $\hat \sigma^2$
It is possible to prove (although we shall not do so here)
that
$$
{D\over\sigma^2}\sim\chi^2_{n-p}
$$
which implies that
$$
E(\hat \sigma^2)= E\left(\frac{D}{n}\right) = {{n-p}\over n}\sigma^2,
$$
so the maximum likelihood estimator is biased for $\sigma^2$ (although
still asymptotically unbiased as ${{n-p}\over n}\to 1$ as $n\to\infty$).
\pause

We often use the unbiased estimator of $\sigma^2$
$$
\tilde \sigma^2={D\over {n-p}}={1\over {n-p}}\sum_{i=1}^n r_i^2.
$$
The denominator $n-p$, the number of observations minus the number of
linear coefficients in the model is called the *degrees of freedom* of the model.


## Comparing linear models

Suppose we want to compare a pair of candidate linear models,
where the models, $H_0$, is *nested* in the other, $H_1$.

This usually means that the explanatory variables
present in $H_0$ are a subset of those present in $H_1$.
In this case model $H_0$ is a special case of model $H_1$, where certain coefficients
are set equal to zero.

Let $\bm \theta$ represent the collection of linear parameters for model $H_1$,
together with the residual variance $\sigma^2$, and let $\Theta^{(1)}$ be the
unrestricted parameter space for $\bm \theta$.
Then $\Theta^{(0)}$ is the parameter space corresponding
to model $H_0$, *i.e.* with the appropriate coefficients constrained to
zero.

## Setting up the hypotheses 

Suppose that $H_1$ contains $p$ linear parameters and
model $H_0$ a subset of $q<p$ of these.
We can think of $H_1$ as the model
$$
Y_i=\sum_{j=1}^p x_{ij} \beta_j + \epsilon_i, \quad i = 1, \ldots, n
$$
and $H_0$ being the same model with
$$\beta_{q+1}=\beta_{q+2}=\cdots=\beta_p=0.
$$

## Generalised likelihood ratio test

Recall a generalised likelihood ratio test of $H_0$ against $H_1$
has a critical region of the form
$$
C=\left\{ \bm{y}:{{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(1)}}L(\bm{\beta},\sigma^2)}\over
{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(0)}}L(\bm{\beta},\sigma^2)}} >k\right\}
$$
where $k$ is determined by $\alpha$, the size of the test, so
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm{\beta},\sigma^2)=\alpha.
$$

## Generalised likelihood ratio test for a linear model

For a linear model,
$$
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{x}_i^T\bm{\beta})^2\right).
$$
This is maximised at
$\bm{\beta}=\hat{\bm{\beta}}$ and $\sigma^2=\hat \sigma^2=D/n$.
Therefore
\begin{align*}
\max_{\bm{\beta},\sigma^2} L(\bm{\beta},\sigma^2)&=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over{2D}} \sum_{i=1}^n (y_i-\bm{x}_i^T\hat{\bm{\beta}})^2\right)\cr
&=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over2}\right).
\end{align*}
This form applies for both $\bm \theta\in\Theta^{(0)}$ and $\bm \theta\in\Theta^{(1)}$, with only the model changing. 

## The $F$ statistic

Let the deviances under models $H_0$ and $H_1$ be $D_0$ and $D_1$
respectively.
Then the critical region for the generalised likelihood ratio test
is 
\[{{(2\pi D_1/n)^{-{n\over 2}}}\over{(2\pi D_0/n)^{-{n\over 2}}}}>k\]
so
\[\left({{D_0}\over{D_1}}\right)^{n\over 2}>k,\]
and
\[\left({{D_0}\over{D_1}}-1\right){{n-p}\over{p-q}}>k'\]
for some $k'$. Rearranging,
\[{{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}>k'.\]
We refer to the left hand side of this inequality as the $F$-statistic.
We reject the simpler model $H_0$ in favour of the more complex model $H_1$ if
$F$ is 'too large'.

## The distribution of the $F$ statistic

The $F$ statistic is 
\[F = {{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}\]

As $H_0$ is nested in $H_1$, $F \sim F_{p-q, \, n-p}$
when $H_0$ is true. 

\pause

To see this, note that
$$
{{D_0}\over\sigma^2}={{D_0-D_1}\over\sigma^2}+{{D_1}\over\sigma^2}.
$$
Under $H_0$,
$D_1/\sigma^2 \sim \chi^2_{n-p}$ and
$D_0/\sigma^2 \sim \chi^2_{n-q}$.
It is possible to show (although we will not do so here)
that under $H_0$, $(D_0-D_1)/\sigma^2$ and $D_0/\sigma^2$ are independent.

From the properties of the chi-squared distribution, 
under $H_0$, $(D_0-D_1)/\sigma^2 \sim \chi^2_{p-q}$, and
$F \sim F_{p-q,\,n-p}$.

## The $F$ test
Therefore, the precise critical region can be evaluated given the
size, $\alpha$,  of the test.
We reject $H_0$ in favour of $H_1$ when
$$
F = {{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}>k
$$
where $k$ is the $100(1-\alpha)\%$ point of the $F_{p-q,\,n-p}$ distribution.

## Log likelihood ratio test

An alternative hypothesis test is a log likelihood ratio test.

We have
\[L_{01} = 2 \log {{\max_{\bm \theta\in \Theta^{(1)}} L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}} = 2 \log \left({{D_0}\over{D_1}}\right)^{n\over 2}
= \frac{n}{2}\left(\log D_0 - \log D_1 \right).\]

Under $H_0$, approximately $L_{01} \sim \chi^2_{p - q}$, so we would
reject $H_0$ if $L_{01}$ exceeds the $100 (1- \alpha)$ point
of the $\chi^2_{p - q}$ distribution.

\pause

We should prefer to use the $F$ test rather than the log likelihood
ratio test in linear models, as it has exactly the correct size,
rather than relying on large-sample approximations.

## Hypothesis testing example
Let's look at an example data set:

```{r, echo = FALSE, fig.width = 5, fig.height = 4}
set.seed(1)
n <- 200
x <- rnorm(n)
mu <- 0.5 - 2 * x + 0.05 * x^2
y <- rnorm(n, mu, sd = 1.5)
plot(y ~ x)
```

## Some possible models ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

The null model
```{r}
mod_null <- lm(y ~ 1)
```

The simple linear model
```{r}
mod_simple <- lm(y ~ x)
```

The quadratic model
```{r}
mod_quad <- lm(y ~ x + I(x^2))
```

Which model do you prefer (after just looking at the data)?

## Comparing `mod_simple` with `mod_quad`

$H_0$: `mod_simple`
against
$H_1$: `mod_quad`

We find $D_0$ is
```{r}
D0 <- deviance(mod_simple)
D0
```
and $D_1$ is
```{r}
D1 <- deviance(mod_quad)
D1
```

## An $F$ test
Here $p = 3$, $q = 2$, and $n = 200$, so the $F$ statistic is
```{r}
(D0 - D1)/(D1 / 197)
```

Compare with 95% point of a $F_{1, 197}$:
```{r}
qf(0.95, 1, 197)
```

So do not reject $H_0$. We prefer `mod_simple`.


## An easier way to do the $F$ test

\small
```{r}
anova(mod_simple, mod_quad)
```

## An alternative method of conducting the $F$ test in `R`

\small
```{r}
anova(mod_quad)
```

## A log likelihood ratio test

The log likelihood ratio statistic is
```{r}
100 * (log(D0) - log(D1))
```
Compare with 95% point of a $\chi^2_{1}$
```{r}
qchisq(0.95, 1)
```

So (again) do not reject $H_0$.

## A $t$ test

If our null hypothesis is that a single
parameter equals zero (i.e. if $p - q = 1$), we could use a $t$ test 
rather than an $F$ test.

Recall that
 \[\hat{\bm{\beta}} \sim N(\bm{\beta}, \sigma^2(\bm{X}^T\bm{X})^{-1})\]
so
\[\hat \beta_i \sim N\left(\beta_i, \sigma^2\left[(\bm{X}^T\bm{X})^{-1}\right]_{ii}\right).\]

So 
\[s.e.(\hat \beta) = \hat \sigma^2\left[(\bm{X}^T\bm{X})^{-1}\right]_{ii},\]
and
\[t = \frac{\hat \beta}{s.e.(\hat \beta)} \sim t_{n-p}.\]

So we reject $H_0$ if $t$ is less than $100 \frac{\alpha}{2}\%$ point
or greater than $100 (1 - \frac{\alpha}{2})\%$
of $t_{n-p}$ distribution.

## Conducintg $t$ tests in `R`

\footnotesize
```{r}
summary(mod_quad)
```

## Comparing `mod_null` with `mod_simple`

$H_0$: `mod_null`
against
$H_1$: `mod_simple`

\small
```{r}
anova(mod_null, mod_simple)
```

## Possible models revisited ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

The null model
```{r}
mod_null <- lm(y ~ 1)
```

The simple linear model
```{r}
mod_simple <- lm(y ~ x)
```

The quadratic model
```{r}
mod_quad <- lm(y ~ x + I(x^2))
```

Which model do you prefer (after conducting these hypothesis tests)?

## Conclusion

- We have found the MLE for $\bm \beta$, and found that this
is the same as the least squares estimate used in MATH2010.
- We have found the $F$ test for hypothesis testing in linear models,
and compared this with the log likelihood ratio test.
- We were able to use the $F$ test because the linear model is
sufficiently simple that we can calculate the distribution of the $F$
statistic exactly. In more complex models, this will not be possible,
and we use the log likelihood ratio test instead.
- You should now be able to attempt all exercises on problem sheet 2.

