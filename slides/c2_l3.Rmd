---
title: "MATH3012: Chapter 2, Lecture 3"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap: exponential family

A distribution is a member of the exponential family if its p.d.f. or p.f.
is of the form
\begin{equation}
  f_Y(y;\theta,\phi)=\exp\left({{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\right).
  \label{eq:ef}
\end{equation}
We have 
\[\mu = E(Y) = b'(\theta)\]
and
\[\text{Var}(Y) = a(\phi)b''(\theta).\]

## Recap: random component of a GLM
In a GLM, all $Y_i$s are assumed to have distribution coming
from the same exponential family, but with a potentially different
value of the parameters.

We have
$$
f_{Y_i}(y_i;\theta_i,\phi_i)=
\exp\left({{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+c(y_i,\phi_i)\right)
$$
for $i = 1, \ldots, n$.

## Recap: linear predictor and link function

The distribution of the response variable
$Y_i$ depends on $\bm{x}_i$ through the *linear predictor* 
\[\eta_i = \bm{x}_i^T \bm{\beta}.\]

The link between the distribution of $\bm{Y}$ and the linear predictor $\bm{\eta}$
is provided by the *link function* $g$,
$$
\eta_i=g(\mu_i),\quad i = 1, \ldots, n,
$$
where $\mu_i\equiv E(Y_i),\;i = 1, \ldots, n$.

We can invert the link to give the mean in terms of the linear predictor:
\[\mu_i = g^{-1}(\eta_i) = g^{-1}(\bm{x}_i^T \bm{\beta}).\]

## The canonical parameters in terms of explanatory variables
Recall that for a random variable $Y$ with a distribution from the
exponential family, $E(Y)=b'(\theta)$. Hence, for a GLM
$$
\mu_i=E(Y_i)=b'(\theta_i),\quad i = 1, \ldots, n.
$$
Therefore
$$
\theta_i=b^{'-1}(\mu_i),\quad i = 1, \ldots, n
$$
and as $g(\mu_i)=\eta_i=\bm{x}_i^T\bm{\beta}$, then
\[\theta_i=b^{'-1}(g^{-1}[\bm{x}_i^T\bm{\beta}]),\quad i = 1, \ldots, n.\]

We already have an expression for $L(\bm \theta)$, and we can convert this 
to the likelihood for $\bm{\beta}$ by substituting
$\theta_i=b^{'-1}(g^{-1}[\bm{x}_i^T\bm{\beta}])$.

## Likelihood for $\bm \beta$

The likelihood
function for $\bm{\theta}$ and $\bm{\phi}$ is
\[L(\bm{\theta}, \bm{\phi}) = \exp\left(\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\right).\]
So the likelihood function for $\bm{\beta}$ and $\bm{\phi}$ is
\vspace{-0.7cm}
\begin{align*} &L(\bm{\beta}, \bm{\phi})  \\
&= \exp\left(\sum_{i=1}^n{{y_i b^{'-1}(g^{-1}[\bm{x}_i^T\bm{\beta}])-b(b^{'-1}(g^{-1}[\bm{x}_i^T\bm{\beta}]))}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\right)
\end{align*}
We will assume for now that $\bm \phi$ is known. Later, we will also with cases where
$\phi_i = \sigma^2/m_i$, where $\sigma^2$ is an unknown dispersion parameter,
and $m_i$ are known weights.

## Canonical link
We have
\[\theta_i=b^{'-1}(g^{-1}[\bm{x}_i^T\bm{\beta}]),\quad i = 1, \ldots, n.\]

If $g$ and $b^{'-1}$ are identical, then
$$
\theta_i=\bm{x}_i^T\bm{\beta}\qquad i = 1, \ldots, n
$$
and the resulting likelihood is
$$
L(\bm{\beta})=
\exp\left(\sum_{i=1}^n{{y_i\bm{x}_i^T\bm{\beta}-b(\bm{x}_i^T\bm{\beta})}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\right).
$$
The link function
$$
g(\mu)\equiv b^{'-1}(\mu)
$$
is called the *canonical* link function. Under the canonical link, the
canonical parameter is equal to the linear predictor.

## The linear model

The linear model is also a generalised linear
model. We assume ${Y_1,\ldots ,Y_n}$ are independent normally distributed
random variables, and the normal distribution is a member of the
exponential family.

Furthermore, the explanatory variables enter a linear model
through the linear predictor
$$
\eta_i=\bm{x}_i^T\bm{\beta}, \quad i = 1, \ldots, n.
$$

Finally, the link between $E(\bm{Y})=\bm{\mu}$ and the linear predictor
$\bm{\eta}$ is through the (canonical) identity link function
$$
\mu_i=\eta_i, \quad i = 1, \ldots, n.
$$

## Bernoulli/Binomial example

If $Y_i \sim \text{Bernoulli}(p_i)$, or if $Y_i = Y_i^*/n_i$ where
$Y_i^* \sim \text{Binomial}(n_i, p_i)$,
we have $\mu_i = E(Y_i) = p_i$.

Recall we want a link function $g(p_i) = \eta_i$ such that

- $\eta_i \in \mathbb{R}$, $p_i \in (0, 1)$, so $g: (0, 1) \to \mathbb{R}$
- As $\eta_i \to \infty$, $p_i = g^{-1}(\eta_i) \to 1$.
- As $\eta_i \to -\infty$, $p_i = g^{-1}(\eta_i) \to 0$.

## Bernoulli/Binomial example: the canonical link

Recall that
$b(\theta) = \log(1 + \exp(\theta))$, 
so
\[\mu = b'(\theta)= \frac{\exp(\theta)}{1 + \exp(\theta)},\]
and
\[\theta = (b')^{-1}(\mu) = \log \frac{\mu}{1-\mu}.\]
This is the canonical **logit** link:
\[g(\mu) = \log \left(\frac{\mu}{1 - \mu}\right) \equiv \text{logit}(\mu) ,\]
so 
\[\log \left(\frac{p_i}{1 - p_i}\right) = \eta_i = \bm x_i^T \bm \beta,\]
and inverting gives
\[p_i = \text{logit}^{-1}(\eta_i) = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} = \frac{\exp(\bm x_i^T \bm \beta)}{1 + \exp(\bm x_i^T \bm \beta)}.\]

Often called
a logistic regression model.

## Bernoulli/Binomial example: inverse of the canonical link

\small
```{r, fig.width = 6, fig.height = 4}
curve(exp(x) / (1 + exp(x)), from = -10, to = 10, 
      xlab = "linear predictor, eta", ylab = "mean, p")
```

## Alternative inverse link? ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
curve(pnorm(x), from = -10, to = 10, 
      xlab = "linear predictor, eta", ylab = "mean, p")
```

Is this a suitable inverse link function for the Bernoulli/Binomial case?


## Probit link

This is the inverse "probit" link function
\[g^{-1}(\eta) = \Phi(\eta),\]
where $\Phi(.)$ is the cumulative distribution function of a $N(0, 1)$ random variable.

A Bernoulli/Binomial GLM with a probit link is sometimes called
a probit regression model.

## Alternative inverse link? ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
curve(0.5 * (0.1 * x + 1), from = -10, to = 10, 
      xlab = "linear predictor, eta", ylab = "mean, p")
```

Is this a suitable inverse link function for the Bernoulli/Binomial case?

## Alternative inverse link? ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
curve(2 * exp(x) / (1 + exp(x)), from = -10, to = 10, 
      xlab = "linear predictor, eta", ylab = "mean, p")
```

Is this a suitable inverse link function for the Bernoulli/Binomial case?

## Alternative inverse link? ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
curve(exp(x + 3) / (1 + exp(x + 3)), from = -10, to = 10, 
      xlab = "linear predictor, eta", ylab = "mean, p")
```

Is this a suitable inverse link function for the Bernoulli/Binomial case?

## Effect of link functions in practice

The choice of link function (e.g. between the logit
or probit link) will have a large impact on the
estimated parameters $\bm \beta$.
```{r, echo = FALSE}
beetle <- read.csv("beetle.csv")
beetle$prop_killed <- beetle$killed / beetle$exposed
```
\footnotesize
```{r}
beetle_logit <- glm(prop_killed ~ dose, data = beetle, 
                    family = binomial, weights = exposed)
beetle_probit <- glm(prop_killed ~ dose, data = beetle, 
                     family = binomial("probit"), weights = exposed)
coef(beetle_logit)
coef(beetle_probit)
```


## Fitted probabilities for the `beetle` data

But choice of link often has only a small effect on
the fitted success probabilities:

```{r, fig.width = 6, fig.height = 4, echo = FALSE}
beetle <- read.csv("beetle.csv")
beetle$prop_killed <- beetle$killed / beetle$exposed
beetle_logit <- glm(prop_killed ~ dose, data = beetle, family = binomial,
                    weights = exposed)
beetle_probit <- glm(prop_killed ~ dose, data = beetle, family = binomial(link = "probit"),
                     weights = exposed)
newdata <- data.frame(dose = seq(1.6, 2, length = 30))
plot(prop_killed ~ dose, data = beetle, ylim = c(0, 1), xlim = c(1.6, 2))
lines(newdata$dose, predict(beetle_logit, newdata = newdata, type = "response"), 
      col = 1)
lines(newdata$dose, predict(beetle_probit, newdata = newdata, type = "response"), 
      col = 2)
legend("topleft", c("Logistic regression", "Probit regression"), lty = 1, col = c(1, 2))
```

## Poisson example

If $Y_i \sim \text{Poisson}(\lambda_i)$, we have $\theta_i = \log \lambda_i$,
and $b(\theta_i) = \exp(\theta_i)$
so
\[\mu_i = b'(\theta_i)= \exp(\theta_i) = \lambda_i,\]
and
\[\theta_i = (b')^{-1}(\mu) = \log \mu.\]

The canonical link is
\[g(\mu) = \log \mu,\]
so 
\[\log \lambda_i = \eta_i = \bm x_i^T \bm \beta,\]
and inverting gives
\[\lambda_i = \exp (\eta_i) = \exp(\bm x_i^T \bm \beta).\]

A Poisson GLM with canonical (log) link is often called
a log-linear regression model.

## Poisson example: inverse of the canonical link

\small
```{r, fig.width = 6, fig.height = 4}
curve(exp(x), from = -3, to = 3, 
      xlab = "linear predictor, eta", ylab = "mean, lambda")
```


## Conclusion

- We have reviewed all the components of a generalised linear model,
and seen how they fit together to give the overall model.
- We have found an expression for the likelihood of the unknown regression
parameters $\bm \beta$.
- We get a much simpler expression for the likelihood if we use
the canonical link function.
- Next time, we'll look in more detail at finding the MLE for $\bm \beta$
  in a generalised linear model.
- You should be able to complete questions 1-3 on problem sheet 3.

