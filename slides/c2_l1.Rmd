---
title: "MATH3012: Chapter 2, Lecture 1"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Linear model quiz 1 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

```{r, echo = FALSE, fig.width = 5, fig.height = 4}
set.seed(1)
n <- 200
x <- rnorm(n)
mu <- 0.5 - 2 * x 
y <- rnorm(n, mu, sd = 1.5)
plot(y ~ x)
```

Does it look like a linear model might be appropriate here?


## Linear model quiz 2 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

```{r, echo = FALSE, fig.width = 5, fig.height = 4}
n <- 200
x <- rnorm(n)
mu <- 0.5 + 2 * x 
p <- pnorm(mu)
y <- rbinom(n, 1, p)
plot(y ~ x)
```

Does it look like a linear model might be appropriate here?

## Linear model quiz 3 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

```{r, echo = FALSE, fig.width = 5, fig.height = 4}
n <- 200
x <- rbinom(n, 3, 0.5)
mu <- 0.5 + 2 * x 
y <- rnorm(n, mu, sd = 1.5)
plot(y ~ x)
```

Does it look like a linear model might be appropriate here?


## Linear model quiz 4 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

```{r, echo = FALSE, fig.width = 5, fig.height = 4}
n <- 200
x <- rnorm(n)
mu <- 1 + 0.5 * x 
y <- rpois(n, lambda = exp(mu))
plot(y ~ x)
```

Does it look like a linear model might be appropriate here?


## Motivation for generalised linear models

- A key assumption in the linear model is that each response $Y_i$ is normally distributed,
with a mean (${\bm x_i}^T {\bm \beta}$) which depends on the explanatory
variables ${\bm x_i}$.
- Generalised linear models extend the linear model to allow a 
non-normal response distribution.
- We would like to allow the response distribution to be any one
of a large family of distributions, which should include
the Normal, Poisson, Bernoulli and Binomial distributions 
as special cases.
- We will define the Exponential family, and show it contains all
of these distributions (and more).

## The Exponential family

A probability distribution is said to be a member of the exponential family if its probability density
function (or probability function, if discrete) can be written in the form
\begin{equation}
  f_Y(y;\theta,\phi)=\exp\left({{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\right).
  \label{eq:ef}
\end{equation}
The parameter $\theta$ is called the *natural*
or *canonical* parameter.
The parameter $\phi$ is usually assumed known. If it is unknown
then it is often called the *nuisance* parameter.


## Mean and variance of the Exponential family

The mean random variable $Y$ with
p.d.f. (or p.f.) of
the form \eqref{eq:ef} is
\[E(Y) = b'(\theta).\]
We often denote the mean by $\mu$, so $\mu=b'(\theta)$.

\pause

The variance is
\[\text{Var}(Y) = a(\phi)b''(\theta),\]
a product of two functions:

1. $b''(\theta)$
depends on the canonical parameter $\theta$ (and hence $\mu$) only
and is called the *variance function* ($V(\mu)\equiv b''(\theta)$)
1. $a(\phi)$ is sometimes of the form $a(\phi)=\sigma^2/w$ where $w$ is a known
*weight* and $\sigma^2$ is called the *dispersion parameter*
or *scale parameter*.

## Proof

The density \eqref{eq:ef} can be thought of as a likelihood resulting from a single
observation $y$. Then the log-likelihood is
\[\ell(\theta,\phi)={{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\]
and the score is
\[u(\theta)=\frac{\partial}{\partial \theta}\ell(\theta,\phi)
={{y-\frac{\partial}{\partial \theta} b(\theta)}\over{a(\phi)}}
={{y- b'(\theta)}\over{a(\phi)}}.\]
The Hessian is
\[H(\theta)=\frac{\partial^2}{\partial \theta^2}\ell(\theta,\phi)
=-{{\frac{\partial^2}{\partial \theta^2} b(\theta)}\over{a(\phi)}}
=-{{b''(\theta)}\over{a(\phi)}}\]
so the expected information is
\[{\cal I}(\theta)=E[-H(\theta)]={{b''(\theta)}\over{a(\phi)}}.\]

## Proof continued
We know that $E[U(\theta)]=0$, so
\[E\left[{{Y- b'(\theta)}\over{a(\phi)}}\right]=0,\]
so $E[Y]=b'(\theta)$. 

Furthermore,
$$
\text{Var}[U(\theta)]=
\text{Var}\left[{{Y- b'(\theta)}\over{a(\phi)}}\right]=
{{\text{Var}[Y]}\over{a(\phi)^2}},
$$
as $b'(\theta)$ and $a(\phi)$ are constants (not
random variables).

But $\text{Var}[U(\theta)]={\cal I}(\theta)$, so
$$
\text{Var}[Y]=a(\phi)^2\text{Var}[U(\theta)]=a(\phi)^2 {\cal I}(\theta)
= a(\phi)b''(\theta).
$$



## Example (Normal distribution)
Suppose $Y\sim N(\mu, \, \sigma^2)$. Then
\begin{align*}
f_Y(y;\mu,\sigma^2)&= {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y-\mu)^2\right)\quad\;\; y\in\mathbb{R};\;\;\mu\in\mathbb{R}\cr
&= \exp\left({{y\mu-{1\over 2}\mu^2}\over \sigma^2}-{1\over 2}\left[
{{y^2}\over\sigma^2}+\log(2\pi\sigma^2)\right]\right).
\end{align*}
This is in the form \eqref{eq:ef}, with $\theta=\mu$, $b(\theta)={1\over 2}\theta^2$,
$a(\phi)=\sigma^2$ and
$$c(y,\phi)=-{1\over 2}\left[
{{y^2}\over{a(\phi)}}+\log(2\pi a[\phi])\right].
$$
Therefore
\[E(Y)=b'(\theta)=\theta=\mu,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)=\sigma^2\]
and the variance function is
\[V(\mu)=1.\]

## Example (Poisson distribution)
Suppose $Y\sim \text{Poisson}(\lambda)$. Then
\begin{align*}
f_Y(y;\lambda)&= {{\exp(-\lambda)\lambda^y}\over{y!}}
\qquad y\in\{0,1,\ldots\};\quad\lambda\in{\cal R}_+\cr
&= \exp\left(y\log\lambda-\lambda-\log y!\right).
\end{align*}
This is in the form \eqref{eq:ef}, with $\theta=\log\lambda$,
$b(\theta)=\exp\theta$,
$a(\phi)=1$ and $c(y,\phi)=-\log y!$.
Therefore
\[E(Y)=b'(\theta)=\exp\theta=\lambda,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)=\exp\theta=\lambda\]
and the variance function is
\[V(\mu)=\mu.\]

## Example (Bernoulli distribution)
Suppose $Y\sim \text{Bernoulli}(p)$. Then
\begin{align*}
f_Y(y;p)&= p^y(1-p)^{1-y}\qquad y\in\{0,1\};\quad
p\in(0,1)\cr
&= \exp\left(y\log{p\over{1-p}}+\log(1-p)\right)
\end{align*}
This is in the form \eqref{eq:ef}, with $\theta=\log{p\over{1-p}}$,
$b(\theta)=\log(1+\exp\theta)$,
$a(\phi)=1$ and $c(y,\phi)=0$.
Therefore
\[E(Y)=b'(\theta)={{\exp\theta}\over{1+\exp\theta}}=p,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)={{\exp\theta}\over{(1+\exp\theta})^2}=p(1-p)\]
and the variance function is
\[V(\mu)=\mu(1-\mu).\]

## Example (Binomial distribution)

Suppose $Y^*\sim \text{Binomial}(n,p)$.
Here, $n$ is assumed known (as usual) and
the random variable $Y= Y^*/n$ is taken as the *proportion*
of successes, so
\begin{align*}
f_Y(y;p)&=\left({n\atop{ny}}\right) p^{ny} (1-p)^{n(1-y)}\quad
y\in\left\{0,{1\over n},{2\over n},\ldots ,1\right\};  \quad p\in(0,1)\cr
&= \exp\left({{y\log{p\over{1-p}}+\log(1-p)}\over{1\over n}}
+\log\!\left({n\atop{ny}}\right)\right).
\end{align*}
This is in the form \eqref{eq:ef}, with $\theta=\log{p\over{1-p}}$,
$b(\theta)=\log(1+\exp\theta)$,
$a(\phi)={1\over n}$ and $c(y,\phi)=\log\!\left({n\atop{ny}}\right)$.


## Example (Binomial distribution)

Therefore
\[E(Y)=b'(\theta)={{\exp\theta}\over{1+\exp\theta}}=p,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)={1\over n}{{\exp\theta}\over{(1+\exp\theta})^2}=
{{p(1-p)}\over n}.\]

The variance function is
\[V(\mu)=\mu(1-\mu).\]
Here, we can write $a(\phi)\equiv \sigma^2/w$ where the scale parameter
$\sigma^2=1$ and the weight $w$ is $n$, the binomial denominator.


## Conclusion

- We have defined the Exponential family, showed that it contains many
common distributions, and found simple expressions for the mean and variance.
- You should now be able to complete questions 1 and 2 on problem sheet 3.
- In a generalised linear model, the distribution of the response is
assumed to be a member of the exponential family.
- To complete the model, we still need to write down how the mean
$\mu_i$ depends on explanatory variables. We'll look at this next time.
