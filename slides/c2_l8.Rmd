---
title: "MATH3012: Chapter 2, Lecture 8"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap

We have seen how to do inference in a GLM:

- how to estimate the parameters $\bm \beta$.
- how to find confidence intervals for components of $\bm \beta$.
- how to compare nested models, using log likelihood ratio tests.

In all cases, we have so far assumed that $a(\phi)$ is known.
What if $a(\phi)$ is unknown?

## Models with unknown $a(\phi)$

So far we have assumed that $a(\phi)$ is known.

This is the case for both the Poisson distribution ($a(\phi)=1$) and
the binomial distribution ($a(\phi)=1/n$).

Neither the scaled deviance nor Pearson $X^2$ statistic
can be evaluated unless $a(\phi)$ is known.

Therefore, when $a(\phi)$ is not known, we cannot use the scaled deviance
as a measure of goodness of fit, or to compare models using log likelihood
ratio tests.

We can develop an alternative test for comparing nested models.

## The deviance of the model

Here we assume that $a(\phi_i)=\sigma^2/m_i,\;i = 1, \ldots, n$ where $\sigma^2$ is a common unknown
scale parameter and $m_1,\ldots ,m_n$ are known weights.

A linear model takes this form, as
$\text{Var}(Y_i)=\sigma^2,\;i = 1, \ldots, n$, so $m_i=1,\;i = 1, \ldots, n$.

Under this assumption
\[L_{0s}={2\over\sigma^2}\sum_{i=1}^nm_iy_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i]
-m_i[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]
={1\over\sigma^2}D_{0s},\]
where $D_{0s}$ is defined to be twice the sum above, which can
be calculated using the observed data. We call $D_{0s}$ the
*deviance* of the model.

## Comparing nested models: an $F$ test

In order to test nested models $H_0$ and $H_1$,
we calculate the test statistic
\vspace{-0.5cm}
\begin{align*}
&F={{L_{01}/(p-q)}\over{L_{1s}/(n-p)}}={{(L_{0s}-L_{1s})/(p-q)}\over{L_{1s}/(n-p)}}
\hbox{\hskip 1.2in}\cr
&\;={{\left({1\over\sigma^2}D_{0s}-{1\over\sigma^2}D_{1s}\right)/(p-q)}
\over{{1\over\sigma^2}D_{1s}/(n-p)}}
={{(D_{0s}-D_{1s})/(p-q)}\over{D_{1s}/(n-p)}}.
\end{align*}
This statistic does not depend on the unknown scale parameter $\sigma^2$,
so can be calculated using the observed data.


Asymptotically, under $H_0$, we have $L_{01} \sim \chi^2_{p-q}$ and 
$L_{1s} \sim \chi^2_{n-p}$. 

Furthermore, $L_{01}$ and $L_{1s}$ are independent
(not proved here) so $F$ has an asymptotic $F_{p-q, n-p}$ distribution.
Hence, we reject $H_0$ in favour of $H_1$
if $F$ is too large (for example, greater than the 95\% point of
the relevant $F$ distribution).

## Maximum likelihood estimation with unknown $a(\phi)$

The MLE of $\beta$ solves
 $\bm{u}(\hat{\bm{\beta}})={\bf 0}$,
 where
\[u_k(\bm{\beta})= \sum_{i=1}^n{{y_i-b'(\theta_i)}\over{a(\phi_i)}}
{{x_{ik}}\over{b''(\theta_i)g'(\mu_i)}}
=\sum_{i=1}^n{{y_i-\mu_i}\over{\text{Var}(Y_i)}}
{{x_{ik}}\over{g'(\mu_i)}}.\]

Recall we are assuming $a(\phi_i) = \sigma^2/ m_i$,
so $\text{Var}(Y_i) = V(\mu_i) \sigma^2 / m_i$.
So
\[u_k(\bm{\hat \beta}) = 
\sum_{i=1}^n \frac{m_i(y_i-\hat \mu_i)}{V(\hat \mu_i) \sigma^2}
{{x_{ik}}\over{g'(\hat \mu_i)}} = 0,\]
and multiplying through by $\sigma^2$ gives
\[u_k(\bm{\hat \beta}) = 
\sum_{i=1}^n \frac{m_i(y_i-\hat \mu_i)}{V(\hat \mu_i)}
{{x_{ik}}\over{g'(\hat \mu_i)}} = 0,\]
$k = 1, \ldots, p$, which we can solve (numerically) without knowing $\sigma^2$.

## Standard errors and confidence intervals

To find standard errors for $\hat \beta_j$, we do need knowledge of $\sigma^2$.

This is because asymptotically  $\text{Var}(\hat{\bm{\beta}})$ is the 
inverse of the Fisher information
matrix ${\cal I}(\bm{\beta})=\bm{X}^T\bm{W}\bm{X}$, and this
depends on $w_i={1\over{\text{Var}(Y_i)g'(\mu_i)^2}},$ where
$\text{Var}(Y_i)=a(\phi_i)b''(\theta_i)=\sigma^2 b''(\theta_i)/m_i$ here.

To calculate standard errors and confidence intervals,
we need to supply an estimate $\hat \sigma^2$ of $\sigma^2$.


## Estimating $\sigma^2$ based on the deviance

Often we do not use the MLE of $\sigma^2$.

Instead, we notice that $L_{0s}=D_{0s}/\sigma^2$, and we know that
asymptotically, if model $H_0$ is an adequate fit, $L_{0s} \sim \chi^2_{n-q}$. Hence
$$
E(L_{0s})=E\left({1\over{\sigma^2}}D_{0s}\right)=n-q\quad\Rightarrow\quad
E\left({1\over{n-q}}D_{0s}\right)=\sigma^2.
$$
Therefore the deviance of a model divided by its degrees of freedom
is an asymptotically unbiased estimator of the scale parameter $\sigma^2$.
Hence $\hat\sigma^2=D_{0s}/(n-q)$.

## Estimating $\sigma^2$ based on Pearson's $X^2$ statistic

An alternative estimator of $\sigma^2$ is based on the Pearson $X^2$ statistic.
As $\text{Var}(Y)=a(\phi)V(\mu)=\sigma^2 V(\mu)/m$ here, then
\[X^2={1\over\sigma^2}
\sum_{i=1}^n {{m_i(y_i-\hat{\mu}_i^{(0)})^2}\over{{V}(\hat{\mu}_i)}}.\]
Again, if $H_0$ is an adequate fit,  $X^2 \sim \chi^2_{n-q}$, so
$$
\hat \sigma^2={1\over{n-q}}
\sum_{i=1}^n {{m_i(y_i-\hat{\mu}_i^{(0)})^2}\over{{V}(\hat{\mu}_i)}}
$$
is an alternative unbiased estimator of $\sigma^2$.

This estimator tends to be more reliable in small samples.

## Example (Normal)

Suppose $Y_i\sim N(\mu_i,\sigma^2),\;i = 1, \ldots, n$.
Recall that $\theta=\mu$,
$b(\theta)=\theta^2/2$,
$\mu=b'(\theta)=\theta$ and
$\text{Var}(Y)=a(\phi)V(\mu)={\sigma^2}\cdot 1$, so $m_i=1,\;i = 1, \ldots, n$.

Therefore 
\[D_{0s}=2\sum_{i=1}^n y_i[\hat{\mu}^{(s)}_i-\hat{\mu}^{(0)}_i]
-[\frac{1}{2}{{\hat{\mu}}^{(s)^2}_i}-\frac{1}{2}{{\hat{\mu}}^{(0)^2}_i}]
=\sum_{i=1}^n [y_i-\hat{\mu}^{(0)}_i]^2,\]
which is just the residual sum of squares for model $H_0$. 

Therefore, we estimate
$\sigma^2$ for a normal GLM by its residual sum of squares for the model
divided by its degrees of freedom.

The estimate for $\sigma^2$ based on $X^2$ is identical.

## Residuals

Recall that for linear models, we define the residuals
to be the differences between the observed and fitted values
\[y_i-\hat{\mu}^{(0)}_i,\;i = 1, \ldots, n.\] 

Both the
scaled deviance
and Pearson $X^2$ statistic for a normal GLM (= linear model) are the sum
of the squared residuals divided by $\sigma^2$.

We can generalise this to define residuals
for other  generalised linear models in a natural way.

## Pearson residuals and deviance residuals

For any GLM we define the *Pearson residuals* to be
$$
r^P_i={{y_i-\hat{\mu}_i^{(0)}}\over{\hat{\text{Var}}(Y_i)^{1\over 2}}}\qquad i = 1, \ldots, n.
$$
Then $X^2$ is the sum of the squared Pearson residuals.


For any GLM we define the *deviance residuals* to be
\[r^D_i=\text{sign}(y_i-\hat{\mu}_i^{(0)})
\left[ 2 {{y_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i]
-[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]}
\over{a(\phi_i)}}\right]^{1\over 2},\]
$i=1, \ldots, n$, where $\text{sign}(x)=1$ if $x>0$ and $-1$ if $x<0$.
Then the scaled deviance, $L_{0s}$,
is the sum of the squared deviance residuals.

## Case of unknown $a(\phi)$

When $a(\phi)=\sigma^2/m$ and $\sigma^2$ is unknown,
the expressions above need to be
multiplied through by $\sigma^2$ to eliminate dependence on the unknown
scale parameter.

Therefore, for a normal GLM the Pearson and deviance residuals are both
equal to the usual residuals, $y_i-\hat{\mu}^{(0)}_i,\;i = 1, \ldots, n$.


## Residuals in `R`
\scriptsize
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
beetle <- read.csv("beetle.csv")
beetle$prop_killed <- beetle$killed / beetle$exposed
```
```{r}
beetle_logit <- glm(prop_killed ~ dose, data = beetle, family = binomial,
                    weights = exposed)
resid_pearson <- residuals(beetle_logit, type = "pearson")
resid_pearson
resid_deviance <- residuals(beetle_logit, type = "deviance")
resid_deviance
```

## Pearson's $X^2$ in `R`  ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Which of the following commands would find the Pearson's $X^2$ statistic in `R`?

- `sum(resid_pearson)`
- `sum(abs(resid_pearson))`
- `sum(resid_pearson^2)`
- `sum(resid_deviance)`
- `sum(abs(resid_deviance))`
- `sum(resid_deviance^2)`

\pause
```{r}
sum(resid_pearson^2)
```
 
## Pearson's $X^2$ in `R` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

For a goodness of fit test, which distribution should
we compare the Pearson's $X^2$ statistic ($10.0$) to here?

- $\chi^2_1$
- $\chi^2_2$
- $\chi^2_3$
- $\chi^2_4$
- $\chi^2_5$
- $\chi^2_6$
- $\chi^2_7$
- $\chi^2_8$

## Pearson's $X^2$ in `R`

We have $X^2 = 10.0$, and we should be worried about the fit of
the model if $X^2$ exceeds the $95\%$ point of the $\chi^2_6$
distribution:

```{r}
qchisq(0.95, df = 6)
```

So we have no particular reason to be worried about model fit on the basis
of the Pearson's $X^2$.

## Scaled deviance in `R`  ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Which of the following commands would find the scaled deviance in `R`?

- `sum(resid_pearson)`
- `sum(abs(resid_pearson))`
- `sum(resid_pearson^2)`
- `sum(resid_deviance)`
- `sum(abs(resid_deviance))`
- `sum(resid_deviance^2)`

\pause
```{r}
sum(resid_deviance^2)
```

## Comparing with residual deviance given by `R`
\scriptsize
```{r}
summary(beetle_logit)
```



## Residuals plots

A plot of deviance or Pearson residuals against the linear predictor
should produce something that looks like a random scatter. 

If not, then this may be due to incorrect link function, wrong scale for an explanatory
variable, or perhaps a missing polynomial term in an explanatory variable.

## Plotting the deviance residuals in `R`
```{r, fig.width = 6, fig.height = 4}
plot(fitted(beetle_logit), resid_deviance)
```

## Plot of the pearson residuals in `R`
```{r, fig.width = 6, fig.height = 4}
plot(fitted(beetle_logit), resid_pearson)
```

## "Automatic" plot of the pearson residuals in `R`

```{r, fig.width = 6, fig.height = 4}
plot(beetle_logit, which = 1)
```

## What would you do next? ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Having seen this residual plot, what would you do next?

- Try a probit link function instead of the logit.
- Try transforming the response variable (dose).
- Try including a quadratic term for `dose` in the model.
- Nothing, the model `beetle_logit` seems to fit the data fine.


## Conclusion

- We have seen two definitions of residuals for GLMs, and how
they relate to the scaled deviance and the Pearson's $X^2$ statistic.
- We have seen how to use residual plots to check for potential problems
in a model, and decide on possible other models.
- This brings us to the end of Chapter 2 on generalised linear models.
We'll review the whole of Chapter 2 in Monday's lecture, then make a start on 
Chapter 3 (which uses GLMs to analyse categorical data) on Wednesday. 
