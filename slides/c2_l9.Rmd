---
title: "MATH3012: Chapter 2, Lecture 9"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Overview

Last time, we completed Chapter 2 on generalised linear models.

In this lecture, we'll summarise some key points about
what we have learned:

- The exponential family
- The linear predictor and link functions
- Maximum Likelihood Estimation via Fisher Scoring
- Confidence intervals
- Hypothesis testing
- Model checking

## The Exponential family

A probability distribution is said to be a member of the exponential family if its probability density
function (or probability function, if discrete) can be written in the form
\[f_Y(y;\theta,\phi)=\exp\left({{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\right).\]

$\theta$ is the *canonical* parameter. $\phi$ is a nuisance parameter, which
is usually known.

We have
$E(Y) = b'(\theta)$ and $\text{Var}(Y) = a(\phi)b''(\theta).$

## Example: Poisson distribution
Suppose $Y\sim \text{Poisson}(\lambda)$. Then
\begin{align*}
f_Y(y;\lambda)&= {{\exp(-\lambda)\lambda^y}\over{y!}}
\qquad y\in\{0,1,\ldots\};\quad\lambda\in{\cal R}_+\cr
&= \exp\left(y\log\lambda-\lambda-\log y!\right).
\end{align*}
This is in exponential family form, with $\theta=\log\lambda$,
$b(\theta)=\exp\theta$,
$a(\phi)=1$ and $c(y,\phi)=-\log y!$.
Therefore
\[E(Y)=b'(\theta)=\exp\theta=\lambda,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)=\exp\theta=\lambda.\]



## The mean parameterisation

We write
$\mu = E(Y) = b'(\theta)$.

We could rewrite the whole distribution in terms of $\mu$ rather than $\theta$,
by substituting $\theta = \theta(\mu) = (b')^{-1}(\mu)$ into the exponential family form.

The variance function is
$V(\mu) = b''(\theta(\mu))$, so that
$\text{Var}(Y) = a(\phi) V(\mu)$.

## Example: Poisson distribution

We have $\mu = \exp \theta$, or $\theta =  \log \mu$.

and the variance function is
\[V(\mu)=b''(\theta)=\exp\theta = \mu.\]

## Generalised Linear Models

In a GLM, we have $Y_1, \ldots, Y_n$ which are 
independent samples from the same member of the exponential family,
but each with different parameters values $\theta_i$ (and
potentially different $\phi_i$).

We also have explanatory variables $\bm x_i$, and want
to model how $\theta_i$ depends on $\bm x_i$.

## The linear predictor and link function

The linear predictor is $\eta_i = {\bm x_i}^T {\bm \beta}$.

The link function $g(.)$ is such that
$\eta_i = g(\mu_i).$

Often, we want to work backwards, to express the mean
or the canonical parameters in
terms of the linear predictor.

We have
$\mu_i = g^{-1}(\eta_i)$ and
$\theta_i = (b')^{-1}(\mu_i) = (b')^{-1}(g^{-1}(\eta_i)).$

\pause

The **canonical** link $g(.)$ is chosen so that
the canonical parameter $\theta_i$ equals
the linear predictor $\eta_i$.

To get this, we must choose $g(\mu) = (b')^{-1}(\mu)$.

Since $\mu = b'(\theta)$, we can find the canonical link $(b')^{-1}(\mu)$
as the expression for $\theta$ in terms of $\mu$.

## Example: Poisson GLM

We have $\mu = \exp\{\theta\}$, or $\theta =  \log \mu$.

So the canonical link function is $g(\mu) = \log \mu$.

Under the canonical link

- $\eta_i = \log \mu_i$, so $\mu_i = \exp(\eta_i) = \exp({\bm x_i}^T {\bm \beta})$.
- $\theta_i = \eta_i = {\bm x_i}^T {\bm \beta}$ (this is the defining property of the canonical link).
- $\lambda_i = \mu_i = \exp({\bm x_i}^T {\bm \beta})$.


## Maximum Likelihood Estimation

We maximise the log-likelihood function
which can be written
\[\ell(\bm{\beta},\bm{\phi})=
\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\]
and depends on $\bm{\beta}$ through
$\theta_i = (b')^{-1}(g^{-1}({\bm x_i}^T {\bm \beta})).$

We can differentiate with respect to each component $\beta_j$
and set to zero, to give $p$ simultaneous equations, the
solution of which are the MLE $\bm{\hat \beta}$.


## Fisher scoring

These score equations typically have no closed-form
solution, so we use a numerical method instead.

Fisher scoring is an iterative numerical algorithm
to find the MLE:
\[\bm{\beta}^{(m+1)}=\bm{\beta}^{(m)}+{\cal I}(\bm{\beta}^{(m)})^{-1}\bm{u}(\bm{\beta}^{(m)}).\]

It is a version of the Newton-Raphson method
\[\bm{\beta}^{(m+1)}=\bm{\beta}^{(m)}-H(\bm{\beta}^{(m)})^{-1}\bm{u}(\bm{\beta}^{(m)}),\]
replacing the observed information $-H(\bm{\beta})$ with its expectation
${\cal I}(\bm \beta)$. 

For a canonical link, the observed information and Fisher information
are the same, so Fisher scoring is identical to the Newton-Raphson method.

## Large sample confidence intervals

For large $n$, we have approximately 
\[\hat{\bm \beta} \sim N(\bm{\beta}, {\cal I}(\bm{\beta})^{-1})\]

Let $z_q$ be the $q$-quantile of $N(0, 1)$, such that
$P(Z \leq z_q) = q$ if $Z \sim N(0, 1)$.
Then
$$P\left(-z_{1-\frac{\alpha}{2}}\le {{\hat{\beta}_i-\beta_i}\over{[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) \approx 1-\alpha.
$$
Therefore
$$
P\left(\hat{\beta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}\le\beta_i
\le\hat{\beta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}
\right) \approx 1-\alpha.
$$
Replacing ${\cal I}(\bm{\beta})$ by its MLE ${\cal I}(\hat{\bm{\beta}})$
gives the approximate large sample 100$(1-\alpha)$\% confidence interval
$$
[\hat{\beta}_i-[\mathcal{I}(\bm{\hat \beta})^{-1}]_{ii}^{1\over 2}z_{1-\frac{\alpha}{2}}\,,\,
\hat{\beta}_i+[\mathcal{I}(\bm{\hat \beta})^{-1}]_{ii}^{1\over 2}z_{1-\frac{\alpha}{2}}].
$$


## Comparing nested GLMs

We assume that model $H_1$ contains $p$ linear parameters and
model $H_0$ a subset of $q<p$ of these.
Without loss of generality, we can think of $H_1$ as the model
$$
\eta_i=\sum_{j=1}^p x_{ij} \beta_j \qquad i = 1, \ldots, n
$$
and $H_0$ is the same model with
\[\beta_{q+1}=\beta_{q+2}=\cdots=\beta_p=0.\]

## Log likelihood ratio statistic

The log likelihood ratio statistic for a test of $H_0$ against $H_1$ is
\[L_{01} 
=2\log L(\hat{\bm{\theta}}^{(1)})-2\log L(\hat{\bm{\theta}}^{(0)}),\]
where 
\[\hat \theta^{(j)} = (b')^{-1}\left(g^{-1}(\bm x_i^T \hat{\bm \beta^{(j)}})\right), \quad j = 0, 1,\]
 and $\hat{\bm \beta^{(j)}}$ is the MLE of $\bm \beta$ under model $j$.

Asymptotically, under $H_0$, $L_{01} \sim \chi^2_{p-q}$, so we 
 we reject $H_0$ in favour of $H_1$ when
$L_{01}$ is greater than the $100(1- \alpha)\%$ point of the  $\chi^2_{p-q}$ distribution.

## The scaled deviance

If $H_S$ is the saturated model, with $p = n$ free parameters, then
the log likelihood ratio test statistic for testing $H_0$ against $H_S$ is called the
scaled deviance:
$$
L_{0s}=2\log L(\hat{\bm{\theta}}^{(s)})-2\log L(\hat{\bm{\theta}}^{(0)}).
$$
If $L_{0s}$ is  'too large'
(for example, larger than the 95\% point of the $\chi^2_{n-q}$ distribution)
then we reject $H_0$ as a plausible model for the data, as it does not
fit the data adequately.

The log likelihood ratio statistic for testing $H_0$
against a non-saturated alternative $H_1$ can be written as
\[L_{01}=L_{0s}-L_{1s}.\]

## Hypothesis tests and scaled deviance with `anova`
```{r, include = FALSE}
shuttle <- read.csv("shuttle.csv")
shuttle$n <- rep(6, nrow(shuttle))
```
\small
```{r}
shuttle_glm0 <- glm(n_damaged / n ~ temp,
                    data = shuttle, family = binomial, 
                    weights = n)
shuttle_glm1 <- glm(n_damaged / n ~ temp + orbiter,
                    data = shuttle, family = binomial,
                    weights = n)
anova(shuttle_glm0, shuttle_glm1, test = "LRT")
```

## A note about the exam

- The external examiner pointed out that the past few years of
exam papers for MATH3012 have been rather similar, and has asked
me to make sure that this year's exam doesn't follow the same pattern.
- So please **do not** assume that this year's exam will follow exactly
the same pattern as in previous years. 
- There will not be a radical change of style of the exam paper, and 
attempting past papers can still form a useful part of your revision.
But I strongly recommend also checking that you can complete
all problem sheet questions and quizzes from the lectures.

## Survey ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Some of you have said that you would like more exercises to practice on.

To help decide which parts of the course we should focus on, 
please could you complete questions on the survey tab at meetoo?

This asks you to assess your confidence in tackling various types of questions.
In each case, 5 stars means you are very confident that you could 
correctly complete such a question, and 1 star means you would not have any idea how to tackle
such a question.

I'll leave the survey open until 5pm today.
I'll go through responses in the tutorial on Wednesday, 
where we can also revise any particularly challenging topics. I'll
bring copies of the additional question sheet to Wednesday's tutorial as well.

## Conclusion

- We'll make a start on 
Chapter 3 (which uses Poisson GLMs to analyse categorical data)
next time.
- The final problem sheet (sheet 5) is now available, but you won't be able
to start this until after Wednesday's lecture. The sheet
covers material from weeks 9-11, and I have extended the hand-in
date for this to Monday 13 May (hand in to get feedback on your solutions).
- Please complete the survey (https://web.meetoo.com, 108-197-366).

