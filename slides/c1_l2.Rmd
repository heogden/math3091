---
title: "MATH3012: Chapter 1, Lecture 2"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap

Last time, we

- reviewed the general purpose of regression models: to model how
the distribution of a response variable depends on explanatory variables.
-  saw some examples in which it is not
sensible to model the response with a normal distribution,
so the assumptions of the linear model are not met.

We will look at a few more examples now.

## `heart`: Treatment for heart attack

This data set represents the results of a clinical trial
to assess the effectiveness of a thrombolytic (clot-busting) treatment
for patients who have suffered an acute myocardial infarction (heart attack).
There are four categorical explanatory variables, representing

- the `site` of infarction: anterior, inferior or other
- the `time` between infarction and treatment: $\le 12$ or $>12$ hours
- whether the patient was already taking Beta-blocker medication prior to the infarction, `blocker`: yes or no
- the `treatment` the patient was given: active or placebo.

For each combination of these categorical variables, 
the dataset gives the total number of patients (`n_patients`),
and the number who survived for
for 35 days (`n_survived`). The aim is to find out how
these categorical variables affect a patient's chance
of survival. 

## Plotting the `heart` data

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
heart <- read.csv("heart.csv")
plot(n_survived / n_patients ~ site, data = heart)
```

## Plotting the `heart` data

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
plot(n_survived / n_patients ~ treatment, data = heart)
```


## `accident`: Road traffic accidents

This example concerns the number of road accidents (`number`) and the 
volume of traffic (`volume`), on each of two roads in Cambridge (`road`),
at various times of day (`time`, taking values `morning`, `midday` or `afternoon`).
We should be able to answer questions like:  

1. Is Mill Road more dangerous than Trumpington Road?
1. How does time of day affect the rate of road accident? 

## Plotting the `accident` data

```{r, echo = FALSE, fig.width = 6, fig.height = 4}
accident <- read.csv("accident.csv")
levels(accident$time) <- c("morning", "afternoon", "evening")
par(mfrow = c(1, 2))
plot(number / volume ~ road + time, data = accident)
```

## `lymphoma`: Lymphoma patients
The  `lymphoma` data set represents 30 lymphoma 
patients classified by sex (`Sex`), cell type of lymphoma (`Cell`) and response to 
treatment (`Remis`). It is an example of data which may be represented as
a three-way ($2\times 2\times 2$)
contingency table. 

\begin{center}
\begin{tabular}{lrrr}
\toprule
 & & \multicolumn{2}{c}{Remission} \\
 \cmidrule{3-4}
Cell Type & Sex  & No & Yes\\
\midrule
\multirow{2}{*}{Diffuse} & Female & 3 & 1\\
 & Male & 12 & 1\\
 \midrule
\multirow{2}{*}{Nodular} & Female & 2 & 6\\
 & Male & 1 & 4\\
\bottomrule
\end{tabular}
\end{center}

The aim here is to study the 
complex dependence structures between the three classifying factors. 

## Generalised linear models

We have seen many examples in which it is not
sensible to model the response with a normal distribution,
so the assumptions of the linear model are not met.

A generalised linear model allows the response to be a member
of a large family of distributions (called the exponential family),
and specifies how the expected value of the distribution
depends on explanatory variables.
\pause

Once we have written down the model (in Chapter 2), we want
to **estimate** the parameters of the model, express **uncertainty**
in estimates, make **predictions** and **compare** candidate models.

We can do all of these things using the **likelihood**. 


## Setup and notation

Suppose that data consist of $n$ observations  $\bm y=(y_1,\ldots ,y_n)^T$.

$\bm y$ observations of random variables
$\bm Y=(Y_1,\ldots ,Y_n)^T$, which have joint probability density 
function (p.d.f.) $f_{\bm Y}$ (joint
probability function (p.f.) for discrete variables).

We often assume that $y_1, \ldots, y_n$ are observations of
**independent** random variables.
Hence
$$
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
$$

## Introduction to the likelihood

In parametric statistical inference, we specify a joint
distribution $f_{\bm Y}$, for $\bm Y$, which is known, except for the values of
parameters $\theta_1,\theta_2,\ldots ,\theta_p$ (sometimes denoted by $\bm \theta$).

Then we use the observed data $\bm y$ to make inferences about
$\theta_1,\theta_2,\ldots ,\theta_p$. In this case, we usually write $f_{\bm Y}$ as
$f_{\bm Y}(\bm y;\bm \theta)$, to make explicit the dependence on the unknown
$\bm \theta$.

Often we think of the joint density $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm{y}$ for fixed $\bm \theta$, which describes the relative probabilities of different
possible values of $\bm y$, given a particular set of parameters $\bm \theta$.

However, in statistical inference, we have observed $\bm y$, 
and want to know (infer) $\bm \theta$: which values of
$\theta$ could plausibly have generated $\bm y$?

## The likelihood function

We can think of $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm \theta$ for fixed $\bm{y}$, which describes the relative *likelihoods* of
different possible (sets of) $\bm \theta,$ given observed data $y_1, \ldots, y_n$.
We write 
\[L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)\]
for this *likelihood*,
which is a function of the unknown parameter $\bm \theta$. For convenience, we often
drop $\bm y$ from the notation, and write $L(\bm \theta)$.

## Notes on the likelihood
1. Frequently it is more convenient to consider the *log-likelihood* function
  $\ell(\bm \theta) = \log L(\bm \theta)$.
\pause
1. Nothing in the definition of the likelihood requires $y_1, \ldots, y_n$ to be
  observations of independent random variables, although we shall frequently
  make this assumption.
\pause
1. Any factors which depend on $y_1, \ldots, y_n$ alone (and not on $\bm \theta$) can be ignored
  when writing down the likelihood. Such factors give no information about the   relative likelihoods of different possible values of $\bm \theta$.


## Example (Lion's appetite)

Suppose that the appetite of a lion has three different stages: 

\[\theta \in \Theta = \{ \text{hungry}, \text{moderate}, \text{lethargic} \} = 
\{ \theta_1, \theta_2, \theta_3\}.\]


Each night the lion eats $y$ people with a probability $f_Y(y; \theta)$ given by the following table: 

\begin{center}
\begin{tabular}{lrrrrr}
\toprule
  & \multicolumn{5}{c}{$y$} \\
   \cmidrule{2-6}
$\theta$ & 0 & 1 & 2 & 3 & 4 \\ 
\midrule
$\theta_1$ (hungry) & 0 & 0.05 & 0.05 & 0.8 & 0.1 \\
$\theta_2$ (moderate) & 0.05 & 0.05 & 0.8 &  0.1 & 0 \\
$\theta_3$ (lethargic) & 0.9 & 0.05 & 0.05  & 0 & 0 \\
\bottomrule
\end{tabular} 
\end{center}


## Lion's appetite quiz 1 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
\begin{center}
\begin{tabular}{lrrrrr}
\toprule
  & \multicolumn{5}{c}{$y$} \\
   \cmidrule{2-6}
$\theta$ & 0 & 1 & 2 & 3 & 4 \\ 
\midrule
$\theta_1$ (hungry) & 0 & 0.05 & 0.05 & 0.8 & 0.1 \\
$\theta_2$ (moderate) & 0.05 & 0.05 & 0.8 &  0.1 & 0 \\
$\theta_3$ (lethargic) & 0.9 & 0.05 & 0.05  & 0 & 0 \\
\bottomrule
\end{tabular} 
\end{center}

Suppose the lion ate two people last night. What is your estimate of $\theta$?

- $\theta_1$ (hungry)
- $\theta_2$ (moderate)
- $\theta_3$ (lethargic)

\pause
How confident are you that your choice is correct?

## Lion's appetite quiz 2 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
\begin{center}
\begin{tabular}{lrrrrr}
\toprule
  & \multicolumn{5}{c}{$y$} \\
   \cmidrule{2-6}
$\theta$ & 0 & 1 & 2 & 3 & 4 \\ 
\midrule
$\theta_1$ (hungry) & 0 & 0.05 & 0.05 & 0.8 & 0.1 \\
$\theta_2$ (moderate) & 0.05 & 0.05 & 0.8 &  0.1 & 0 \\
$\theta_3$ (lethargic) & 0.9 & 0.05 & 0.05  & 0 & 0 \\
\bottomrule
\end{tabular} 
\end{center}

Suppose the lion ate four people last night. What is your estimate of $\theta$?

- $\theta_1$ (hungry)
- $\theta_2$ (moderate)
- $\theta_3$ (lethargic)

How confident are you that your choice is correct?


## Lion's appetite quiz 3 ([web.meetoo.com](https://web.meetoo.com), 108-197-366)
\begin{center}
\begin{tabular}{lrrrrr}
\toprule
  & \multicolumn{5}{c}{$y$} \\
   \cmidrule{2-6}
$\theta$ & 0 & 1 & 2 & 3 & 4 \\ 
\midrule
$\theta_1$ (hungry) & 0 & 0.05 & 0.05 & 0.8 & 0.1 \\
$\theta_2$ (moderate) & 0.05 & 0.05 & 0.8 &  0.1 & 0 \\
$\theta_3$ (lethargic) & 0.9 & 0.05 & 0.05  & 0 & 0 \\
\bottomrule
\end{tabular} 
\end{center}

Suppose the lion ate one person last night. What is your estimate of $\theta$?

- $\theta_1$ (hungry)
- $\theta_2$ (moderate)
- $\theta_3$ (lethargic)

How confident are you that your choice is correct?

## Example (Bernoulli)

$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, independent identically distributed
(i.i.d.) Bernoulli$(p)$ random
variables. Here $\theta=(p)$ and the likelihood is
$$
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}.
$$
\pause

The log-likelihood is
$$
\ell(p) = \log L(p) =n\bar y\log p+n(1-\bar y)\log(1-p).
$$


## Plotting the log-likelihood with `R`
```{r}
lfun <- function(p, y) {
  ybar <- mean(y)
  n <- length(y)
  n * ybar * log(p) + n * (1 - ybar) * log(1 - p)
}
```
\pause

e.g. suppose $\bm y$ is
```{r}
y <- c(1, 0, 0, 0, 1, 0, 0, 0, 1, 0)
```

We can plot the log-likelihood with
```{r, eval = FALSE}
curve(lfun(x, y), from = 0, to = 1, ylim = c(-12, -6),
      xlab = "p", ylab = "log-likelihood")
```

## Plotting the log-likelihood

```{r, echo = FALSE, fig.width = 5, fig.height = 3.5}
curve(lfun(x, y), from = 0, to = 1, ylim = c(-12, -6),
      xlab = "p", ylab = "log-likelihood")
```

How would you estimate $p$ in this example? \pause
How confident are you of your estimate? (e.g. could $p = 0.5$ plausibly
have generated the data?)

## Example (Normal)
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and the likelihood is
\begin{align*}
L(\mu,\sigma^2) &=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right).
\end{align*}
\pause

The log-likelihood is
\[\ell(\mu, \sigma^2) = \log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]

## Maximum likelihood estimation

We call the value of $\bm \theta$ which maximises the likelihood $L(\theta)$
the *maximum likelihood estimate* (MLE) of $\bm \theta$, denoted by
$\hat{\bm \theta}$.
\pause

$\hat{\bm \theta}$ depends on $\bm y$, as different observed data samples
lead to different likelihood functions.

\pause
The corresponding function of $\bm Y$ is called the
*maximum likelihood estimator*  and is also denoted by $\hat{\bm \theta}$.

## Some properties of the MLE
 
As $\bm \theta=(\theta_1, \ldots, \theta_p)$, the MLE for any component
of $\bm \theta$ is given by the corresponding component of
$\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T$.

The MLE for any function of parameters $g(\bm \theta)$ is given
by $g(\hat{\bm \theta})$.

As $\log$ is a strictly increasing
function, the value of $\bm \theta$ which maximises 
$L(\bm \theta)$ also maximises $\ell(\bm \theta) = \log L (\bm \theta)$.
It is almost always easier to maximise $\ell(\bm \theta)$.

## "Usual" recipe for finding the MLE

1. Write down the likelihood $L(\bm \theta)$.
1. Take logs to find the log-likelihood $\ell(\bm \theta) = \log L(\bm \theta)$.
1. Find a stationary point $\hat{\bm \theta}$ by differentiating
$\ell(\bm \theta)$ with respect to $\theta_1, \ldots, \theta_p$, and solving the resulting $p$
simultaneous equations.
1. Check that the stationary point $\hat{\bm \theta}$ is a maximum 
(rather than a minimum or point of inflection) of the log-likelihood.

## Example (Bernoulli)
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
Bernoulli$(p)$ random variables. Here $\bm \theta=(p)$ and
the log-likelihood is
\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]

  Differentiating with respect to $p$,
  \[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]
    so the MLE $\hat p$ solves
    \[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]
  Solving this for $\hat{p}$ gives $\hat{p}=\bar y$.
  Note that
  \[\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}<0\]
   everywhere, so the stationary point is clearly a maximum.

## Checking with `R`

In our test case, where $\bm y$ is
```{r}
y
```
let's plot out the likelihood, and add on the MLE:
```{r, eval = FALSE, fig.width = 5, fig.height = 4}
curve(lfun(x, y), from = 0, to = 1, ylim = c(-12, -6),
      xlab = "p", ylab = "log-likelihood")
ybar <- mean(y)
points(ybar, lfun(ybar, y))
```

## Checking with `R`

```{r, echo = FALSE, fig.width = 5, fig.height = 4}
curve(lfun(x, y), from = 0, to = 1, ylim = c(-12, -6),
      xlab = "p", ylab = "log-likelihood")
ybar <- mean(y)
points(ybar, lfun(ybar, y))
```

## Example (Normal)
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
 and the log-likelihood is
\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]

## Differentiating the log-likelihood
   Differentiating with respect to $\mu$
\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]
  so $(\hat \mu, \hat \sigma^2)$ solve
\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  \label{eq:normalScoreMu}
\end{equation}
\pause

  Differentiating with respect to $\sigma^2$
\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]
so
\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  \label{eq:normalScoreSs}
\end{equation}

## Finding a stationary point
Solving \eqref{eq:normalScoreMu} and \eqref{eq:normalScoreSs}, we obtain
$\hat{\mu}=  \bar y$ and
\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]
\pause

To show that this stationary point is a maximum, we need to show that the
Hessian matrix $\bm{H}(\bm \theta)$, with elements
$[\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)$)
is negative definite at $\bm \theta=\hat{\bm \theta}$, that is $\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}<0$ for
every
$\bm{a}\ne {\bf 0}$.
\pause

Here
$$
\bm{H}(\hat{\mu},\hat \sigma^2)= \begin{pmatrix}
- \frac{n}{\hat \sigma^2 } & 0 \cr
0   &-\frac{n}{2(\hat \sigma^2)^2} \end{pmatrix}
$$
which is negative definite.


## Conclusion

- We have reintroduced the likelihood: “the probability that we
would have seen the data we actually did, for each value of the
parameter”.
- We have reviewed the “usual” recipe for finding maximum
likelihood estimates: find a stationary point of the
log-likelihood (and check it is a maximum).
- You should now be able to attempt Questions 1 and 3, and part of Question 2,
on problem sheet 1.
- Likelihood is very general: we can find a likelihood function for
any probability model. The difficult part is often to choose an
appropriate model.