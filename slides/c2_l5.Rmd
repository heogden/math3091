---
title: "MATH3012: Chapter 2, Lecture 5"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap: Fisher scoring

Fisher scoring is an iterative numerical algorithm
to find the MLE:
\[\bm{\beta}^{(m+1)}=\bm{\beta}^{(m)}+{\cal I}(\bm{\beta}^{(m)})^{-1}\bm{u}(\bm{\beta}^{(m)}).\]

Substituting the form of $u(\bm \beta)$ and $\cal I(\bm \beta)$ for a GLM,
we get
\[\bm{\beta}^{(m+1)} =[\bm{X}^T\bm{W}^{(m)}\bm{X}]^{-1}\bm{X}^T\bm{W}^{(m)}[\bm{\eta}^{(m)}+\bm{z}^{(m)}],\]
where
\[\bm{W} = \text{diag}(w_i),\]
\[w_i={1\over{\text{Var}(Y_i)g'(\mu_i)^2}}\]
\[z_i=(y_i-\mu_i)g'(\mu_i)\]
and
\[\bm \eta = \bm X \bm \beta.\]

## Fisher scoring and Newton-Raphson

Recall that Fisher scoring is a modification of the Newton-Raphson method,
where we replace $H(\bm \beta)$ with its expectation, $- \cal{I}(\bm \beta)$.

We have
\[[\bm{H}(\bm{\beta})]_{jk}
=\sum_{i=1}^n x_{ij} w_i x_{ik} +\sum_{i=1}^n(y_i-\mu_i){\partial\over{\partial\beta_j}}
\left[{{x_{ik}}\over{\text{Var}(Y_i)
g'(\mu_i)}}\right]\]

and
\[[-{\cal I}(\bm{\beta})]_{jk}
=\sum_{i=1}^n x_{ij} w_i x_{ik}.\]


## Simplification for the canonical link 

Recall that the canonical link function is 
 $g(\mu)=b^{'-1}(\mu)$ and with this link $\eta_i=g(\mu_i)=\theta_i$.
Then
$$
{1\over{g'(\mu_i)}}={{\partial\mu_i}\over{\partial\eta_i}}
={{\partial\mu_i}\over{\partial\theta_i}}=b''(\theta_i),\quad i = 1, \ldots, n.
$$
Therefore $\text{Var}(Y_i)g'(\mu_i)=a(\phi_i)$ which does not depend on $\bm{\beta}$,
and hence
$$
{\partial\over{\partial\beta_j}}\left[{{x_{ik}}\over{\text{Var}(Y_i)g'(\mu_i)}}\right]=0
$$
for all $j=1,\ldots ,p$.

It follows that $\bm{H}(\bm{\beta})=-\mathcal{I}(\bm{\beta})$ and, for the canonical link, Newton-Raphson and
Fisher scoring are equivalent.

## Fisher scoring for the linear model

The linear model is a generalised linear model with
identity link, $\eta_i=g(\mu_i)=\mu_i$ and $\text{Var}(Y_i)=\sigma^2$ for all $i = 1, \ldots, n$.

Therefore $w_i=[\text{Var}(Y_i)g'(\mu_i)^2]^{-1}=\sigma^{-2}$ and
$z_i=(y_i-\mu_i)g'(\mu_i)=y_i-\eta_i$ for $i = 1, \ldots, n$.

Hence $\bm{z}+\bm{\eta}=\bm{y}$ and $\bm{W}=\sigma^{-2}\bm{I}$, neither of which
depend on $\bm{\beta}$. 

So the Fisher scoring algorithm converges in a
single iteration to the usual least squares estimate.

## Asymptotic distribution of $\hat{\bm \beta}$

We are not just interested in finding an estimate of $\bm \beta$:
we might also want to quantify the uncertainty in our
estimate. We can use the general asymptotic results about
the MLE to do this.

Using those general results, we know $\hat{\bm{\beta}}$ has
asymptotic
\[N(\bm{\beta}, {\cal I}(\bm{\beta})^{-1})\]
distribution.

For  'large enough $n$' we  treat this distribution as
an approximation.


## Standard errors

Standard errors (estimated standard deviations) are given by
$$
s.e.(\hat{\beta}_i)=[{\cal I}(\hat{\bm{\beta}})^{-1}]_{ii}^{{1\over 2}}
=[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{ii}^{{1\over 2}}
\qquad i=1,\ldots ,p.
$$
where the diagonal matrix $\hat{\bm{W}}={\rm diag}(\hat{\bm{w}})$ is evaluated at
$\hat{\bm{\beta}}$, that is
$\hat{w}_i=(\hat{\text{Var}}(Y_i)g'(\hat{\mu}_i)^2)^{-1}$ where $\hat{\mu}_i$
and $\hat{\text{Var}}(Y_i)$ are evaluated at $\hat{\bm{\beta}}$ for $i = 1, \ldots, n$.

If $\text{Var}(Y_i)$ depends on an unknown scale parameter,
then this too must be estimated in the standard error.


## Large sample confidence intervals

For large $n$, we have approximately 
\[\hat{\bm \beta} \sim N(\bm{\beta}, {\cal I}(\bm{\beta})^{-1})\]


For given $\alpha$ we can find $z_{1-\frac{\alpha}{2}}$ such that
$$
P\left(-z_{1-\frac{\alpha}{2}}\le {{\hat{\beta}_i-\beta_i}\over{[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) =1-\alpha.
$$
Therefore
$$
P\left(\hat{\beta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}\le\beta_i
\le\hat{\beta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}
\right) =1-\alpha.
$$
The endpoints of this interval cannot be evaluated
because they also depend on the unknown parameter vector $\bm{\beta}$.
However, if we replace ${\cal I}(\bm{\beta})$ by its MLE ${\cal I}(\hat{\bm{\beta}})$
we obtain the approximate large sample 100$(1-\alpha)$\% confidence interval
$$
[\hat{\beta}_i-s.e.(\hat{\beta}_i)z_{1-\frac{\alpha}{2}}\,,\,
\hat{\beta}_i+s.e.(\hat{\beta}_i)z_{1-\frac{\alpha}{2}}].
$$

## Hypothesis testing: single parameter case

Suppose we want to test the hypothesis 
$H_0: \beta_j = 0$ 
against the alternative
$H_1: \text{"$\beta_j$ is unrestricted"}$,
for a single component $\beta_j$ of $\bm \beta$.

Under $H_0$, for large samples we have approximately
\[z = \frac{\hat \beta_j}{s.e.(\hat \beta_j)} \sim N(0, 1),\] 
so we reject $H_0$ if $|z|$ exceeds the $(1 - \frac{\alpha}{2})$
point of the $N(0, 1)$ distribution.

## A redacted `summary` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

\scriptsize
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
beetle <- read.csv("beetle.csv")
beetle$prop_killed <- beetle$killed / beetle$exposed
beetle_logit <- glm(prop_killed ~ dose, data = beetle, family = binomial,
                    weights = exposed)
summary_mod <- capture.output(summary(beetle_logit))
summary_mod[12] <- "(Intercept)  -60.717      5.181   [ A ]   ----- ---"

summary_mod[13] <-  "dose          34.270      [ B ]   11.77   [ C ] ---"
cat(summary_mod[2:14], fill = 1)
```

\normalsize
What is the value A here?

- -20.32
- -11.72
- 1.96
- 11.72
- 20.32

## A redacted `summary` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

\scriptsize
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
beetle <- read.csv("beetle.csv")
beetle$prop_killed <- beetle$killed / beetle$exposed
beetle_logit <- glm(prop_killed ~ dose, data = beetle, family = binomial,
                    weights = exposed)
summary_mod <- capture.output(summary(beetle_logit))
summary_mod[12] <- "(Intercept)  -60.717      5.181   [ A ]   ----- ---"

summary_mod[13] <-  "dose          34.270      [ B ]   11.77   [ C ] ---"
cat(summary_mod[2:14], fill = 1)
```

\normalsize
What is the value B here?

- -5.181
- -2.912
- 1.96
- 2.912
- 5.181


## A redacted `summary` ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

\scriptsize
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
cat(summary_mod[2:14], fill = 1)
```

\normalsize
Which of the following would calculate C correctly?

- `pnorm(11.77)`
- `1 - pnorm(11.77)`
- `2 * pnorm(11.77)`
- `2 * (1 - pnorm(11.77))`

## Confidence interval example


\scriptsize
```{r, fig.width = 6, fig.height = 4, echo = FALSE}
summary_mod <- capture.output(summary(beetle_logit))
cat(summary_mod[2:14], fill = 1)
```

\normalsize
Which of the following is a $95\%$ confidence interval for
$\beta_2$, the coefficient for `dose` in the `beetle_logit`
model?

```{r, include = FALSE}
CI_true <- coef(beetle_logit)[2] + c(-1, 1) * qnorm(0.975) * 2.912
CI_2 <- coef(beetle_logit)[2] + c(-1, 1) * qnorm(0.975) * 11.77
CI_3 <- coef(beetle_logit)[2] + c(-1, 1) * 2.912

```

- (11.2, 57.3)
- (28.6, 40.0)
- (31.4, 37.2)

## Conclusion

- We have reviewed the Fisher scoring algorithm, and seen that
in the special case of a canonical link, this is the same
as the Newton-Raphson algorithm. If we have a normal linear model,
Fisher scoring goes straight to the MLE in a single iteration.
- We have seen how to use general asymptotic results about the MLE
to find standard errors of estimates, find confidence intervals,
and test the hypothesis that $\beta_j = 0$, for a single parameter $\beta_j$.
- Next time, we'll look at testing more general hypotheses in a GLM.
- You should now be able to complete Question 1 on problem sheet 4.