---
title: "MATH3012: Chapter 2, Lecture 2"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap

A distribution is a member of the exponential family if its p.d.f. or p.f.
is of the form
\begin{equation}
  f_Y(y;\theta,\phi)=\exp\left({{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\right).
  \label{eq:ef}
\end{equation}
We have 
\[\mu = E(Y) = b'(\theta)\]
and
\[\text{Var}(Y) = a(\phi)b''(\theta).\]

## Example (Bernoulli distribution)
Suppose $Y\sim \text{Bernoulli}(p)$. Then
\begin{align*}
f_Y(y;p)&= p^y(1-p)^{1-y}\qquad y\in\{0,1\};\quad
p\in(0,1)\cr
&= \exp\left(y\log{p\over{1-p}}+\log(1-p)\right)
\end{align*}
This is in the form \eqref{eq:ef}, with $\theta=\log{p\over{1-p}}$,
$b(\theta)=\log(1+\exp\theta)$,
$a(\phi)=1$ and $c(y,\phi)=0$.
Therefore
\[E(Y)=b'(\theta)={{\exp\theta}\over{1+\exp\theta}}=p,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)={{\exp\theta}\over{(1+\exp\theta})^2}=p(1-p)\]
and the variance function is
\[V(\mu)=\mu(1-\mu).\]

## Example (Binomial distribution)

Suppose $Y^*\sim \text{Binomial}(n,p)$.
Here, $n$ is assumed known (as usual) and
the random variable $Y= Y^*/n$ is taken as the *proportion*
of successes, so
\begin{align*}
f_Y(y;p)&=\left({n\atop{ny}}\right) p^{ny} (1-p)^{n(1-y)}\quad
y\in\left\{0,{1\over n},{2\over n},\ldots ,1\right\};  \quad p\in(0,1)\cr
&= \exp\left({{y\log{p\over{1-p}}+\log(1-p)}\over{1\over n}}
+\log\!\left({n\atop{ny}}\right)\right).
\end{align*}
This is in the form \eqref{eq:ef}, with $\theta=\log{p\over{1-p}}$,
$b(\theta)=\log(1+\exp\theta)$,
$a(\phi)={1\over n}$ and $c(y,\phi)=\log\!\left({n\atop{ny}}\right)$.


## Example (Binomial distribution)

Therefore
\[E(Y)=b'(\theta)={{\exp\theta}\over{1+\exp\theta}}=p,\]
\[\text{Var}(Y)=a(\phi)b''(\theta)={1\over n}{{\exp\theta}\over{(1+\exp\theta})^2}=
{{p(1-p)}\over n}.\]

The variance function is
\[V(\mu)=\mu(1-\mu).\]
Here, we can write $a(\phi)\equiv \sigma^2/w$ where the scale parameter
$\sigma^2=1$ and the weight $w$ is $n$, the binomial denominator.



## Components of a generalised linear model (GLM)


As in a linear model, the aim is to determine the pattern of dependence of a response variable
on explanatory variables.

We denote the $n$ observations of the response by
$\bm{y}=(y_1,y_2,\ldots ,y_n)^T$, assumed to be observations
of random variables $\bm{Y}=(Y_1,Y_2,\ldots ,Y_n)^T$

Associated with each $y_i$ is a vector $\bm{x}_i=(x_{i1},x_{i2},\ldots ,x_{ip})^T$
of $p$ explanatory variables.

We want to model how the distribution of $Y_i$ depends on $\bm{x}_i$.

## The random component

In a GLM, each $Y_i$ is
assumed to be an independent random variable.
All the $Y_i$s are assumed to have distribution coming
from the same exponential family, but with a potentially different
value of the parameters.

The functions $a(.)$, $b(.)$ and $c(.)$ are the same for each $Y_i$, but the
canonical parameter $\theta$ (and sometimes the scale parameter $\phi$) may differ.
We have
$$
f_{Y_i}(y_i;\theta_i,\phi_i)=
\exp\left({{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+c(y_i,\phi_i)\right)
$$
for $i = 1, \ldots, n$, where 
 $\bm{\theta}=(\theta_1,\ldots ,\theta_n)^T$ is the collection
of canonical parameters and $\bm{\phi}=(\phi_1,\ldots ,\phi_n)^T$
is the collection of nuisance parameters (where they exist).

## Likelihood for $\bm{\theta}$ and $\bm{\phi}$

For a particular sample of observed
responses, $\bm{y}=(y_1,y_2,\ldots ,y_n)^T$, the likelihood
function for $\bm{\theta}$ and $\bm{\phi}$ is
\begin{align*}
L(\bm{\theta}, \bm{\phi}) &= \prod_{i=1}^n f_{Y_i}(y_i;\theta_i,\phi_i) \\
&= \exp\left(\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\right)
\end{align*}

\pause
But we **do not** want to estimate $\bm \theta$ and $\bm \phi$
directly.

## Bernoulli example

Suppose I ask the entire MATH3012 class a question, e.g.:

"What is the MLE of the Poisson rate parameter $\lambda$,
given samples $y_1, \ldots, y_n$?"

\pause

Let
\[Y_i = \begin{cases} 1 & \text{if person $i$ gets question correct} \\
0 & \text{otherwise}. \end{cases}\]

We can model $Y_i \sim \text{Bernoulli}(p_i)$, where $p_i$ is the
unknown probability that person $i$ will get the question correct.

The response distribution belongs to the exponential family, with
canonical parameter $\theta_i = \log[p_i/(1-p_i)]$, and with no scale parameter
($a(\phi) = 1$).

## MLE of $p_1$ ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Suppose $n = 10$, and $\bm y = (1, 0, 0, 1, 1, 1, 1, 0, 0, 1)^T$.

What is the MLE of $p_1$?

- $\hat p_1 = 0$
- $\hat p_1 = 1 - \bar y = 0.4$
- $\hat p_1 = \bar y = 0.6$
- $\hat p_1 = 1$

## MLE of $p_2$ ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Suppose $n = 10$, and $\bm y = (1, 0, 0, 1, 1, 1, 1, 0, 0, 1)^T$.

What is the MLE of $p_2$?

- $\hat p_2 = 0$
- $\hat p_2 = 1 - \bar y = 0.4$
- $\hat p_2 = \bar y = 0.6$
- $\hat p_2 = 1$

\pause
We are not really interested in the individual $p_i$.
Instead, we might be interested in modelling each $p_i$
as a function of explanatory variables ${\bm x}_i$
(e.g. number of lectures person $i$ attended, whether person $i$ completed problem sheet 1, ...).

## The systematic (or structural) component

In a GLM, the distribution of the response variable
$Y_i$ depends on $\bm{x}_i$ through the *linear predictor* $\eta_i$
where
\begin{align*}
\eta_i &=\beta_1 x_{i1} +\beta_2 x_{i2} +\ldots + \beta_p x_{ip} \notag \\
&= \sum_{j=1}^p x_{ij} \beta_j \notag \\
&=  \bm{x}_i^T \bm{\beta} \notag \\
&= [\bm{X}\bm{\beta}]_i,\qquad i=1,\ldots ,n,
\end{align*}
where, as with a linear model,
$$
\bm{X}=\begin{pmatrix} \bm{x}_1^T\cr\vdots\cr \bm{x}_n^T \end{pmatrix}
=\begin{pmatrix}
x_{11}&\cdots&x_{1p}\cr\vdots&\ddots&\vdots\cr x_{n1}&\cdots&x_{np}\end{pmatrix}
$$
and
$\bm{\beta}=(\beta_1,\ldots ,\beta_p)^T$ is a vector of unknown parameters
describing the dependence of $Y_i$ on $\bm{x}_i$.

## The design matrix

These four ways of describing the linear predictor, but
the most economical is the matrix form
\[\bm{\eta}=\bm{X}\bm{\beta}.\]

Again, we call the $n\times p$ matrix $\bm{X}$ the *design matrix*.
The $i$th row of $\bm{X}$ is $\bm{x}_i^T$, the
explanatory data corresponding to the $i$th observation of the response.
The $j$th column of $\bm{X}$ contains the $n$ observations of the $j$th
explanatory variable.

## The link function

For specifying the pattern of dependence of the response variable
on the explanatory variables, the canonical parameters
$\theta_1,\ldots,\theta_n$ are not of direct interest.

We have already specified that the distribution
of $Y_i$ should depend on $\bm{x}_i$ through the linear predictor $\eta_i$.
It is the parameters $\beta_1,\ldots ,\beta_p$ of the linear predictor
which are of primary interest.

The link between the distribution of $\bm{Y}$ and the linear predictor $\bm{\eta}$
is provided by the *link function* $g$,
$$
\eta_i=g(\mu_i),\quad i = 1, \ldots, n,
$$
where $\mu_i\equiv E(Y_i),\;i = 1, \ldots, n$.

So the dependence of the distribution on explanatory variables is
$$
g(E[Y_i])=g(\mu_i)=\eta_i=\bm{x}_i^T\bm{\beta},\quad i = 1, \ldots, n.
$$

## Bernoulli example

Returning to our previous example, where
\[Y_i = \begin{cases} 1 & \text{if person $i$ gets question correct,} \\
0 & \text{otherwise}, \end{cases}\]
and $Y_i \sim \text{Bernoulli}(p_i)$, we have $\mu_i = E(Y_i) = p_i$.

Let 
\[x_i = \begin{cases} 1 & \text{if person $i$ has completed problem sheet 1,} \\
0 & \text{otherwise}, \end{cases}\]
and
\[\eta_i = \beta_1 + \beta_2 x_i.\]

What link $g(p_i) = \eta_i$ should we choose?

- $\eta_i \in \mathbb{R}$, $p_i \in (0, 1)$, so $g: (0, 1) \to \mathbb{R}$
- As $\eta_i \to \infty$, $p_i = g^{-1}(\eta_i) \to 1$.
- As $\eta_i \to -\infty$, $p_i = g^{-1}(\eta_i) \to 0$.


## Choosing the link function
In principle, the link function $g$ can be any one-to-one
differentiable function. 

However, $\eta_i$ can
take any value in $\mathbb{R}$ (as we make no restriction on possible values
taken by explanatory variables or model parameters).

However, for some exponential family distributions $\mu_i$ is restricted.
For example, for the Poisson distribution $\mu_i\in\mathbb{R}_+$;
for the Bernoulli distribution $\mu_i\in(0,1)$.

If $g$ is not chosen carefully, then there may exist a possible $\bm{x}_i$
and $\bm{\beta}$ such that $\eta_i\ne g(\mu_i)$ for any possible value of $\mu_i$.

Therefore,  'sensible' choices of link function map
the set of allowed values for $\mu_i$ onto $\mathbb{R}$.


## Conclusion

- We have introduced the components of a generalised linear model:
a response distribution coming from an exponential family, a linear
predictor depending on a linear combination of explanatory variables,
and the relationship between the mean $\mu_i$ and the linear predictor
through the link function.
- Next time, we will look a choice of link function which simplifies
the likelihood substantially, and look at the likelihood for
the regression parameters.
- You should be able to complete questions 1 and 2 on problem sheet 3.

