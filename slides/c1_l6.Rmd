---
title: "MATH3012: Chapter 1, Lecture 6"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap

Last time, we found one of the most important results about the 
MLE: it has asymptotic 
$N(\theta, [{\cal I}(\theta)]^{-1})$ distribution.

How can we use this result to find confidence intervals 
for the parameters, and to test hypotheses?

## Standard errors

A *standard error* is an estimate of
the standard deviation of an estimator.

If $p = 1$, a standard error of the MLE $\hat \theta$ is 
$$
s.e.(\hat{\theta})={1\over{{\cal I}(\hat{\theta})^{{1\over 2}}}},
$$
and for a vector parameter $\bm \theta$
$$
s.e.(\hat{\theta}_i)=[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{{1\over 2}},
\quad i=1,\ldots ,p.
$$


## Constructing large sample confidence intervals

Asymptotically,
$\hat{\theta}_i \sim N(\theta_i,[\mathcal{I}(\bm \theta)^{-1}]_{ii})$ 
and we can find $z_{1-\frac{\alpha}{2}}$ such that
$$
P\left(\bm{- z_{1-\frac{\alpha}{2}}}\le {{\hat{\theta}_i-\theta_i}\over{[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) = 1- \alpha.
$$
Therefore
$$
P\left(\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}\le\theta_i
\le\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}
\right) = 1- \alpha.
$$
**Correct typo in notes.**


## Constructing large sample confidence intervals

The endpoints of this interval cannot be evaluated
because they also depend on the unknown parameter vector $\bm \theta$.
However, if we replace $\mathcal{I}(\bm \theta)$ by its MLE ${\cal I}(\hat{\bm \theta})$
we obtain the approximate large sample $\bm{100(1 - \alpha)}\%$ confidence interval
$$
[\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2},
\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2}].
$$
For $\bm{\alpha=0.1,0.05,0.01}$, $z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58$.

**Correct typo in notes.**

## Example (Bernoulli)
If $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables then asymptotically $\hat{p}=\bar y$ has a  $N(p,{p(1-p)}/ n)$
distribution, and a large sample 95\% confidence interval
for $p$ is
\begin{align*}
& [\hat{p}- 1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2},
\hat{p}+1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2}]
\cr
&=
[\hat{p}-1.96[\hat{p}(1-\hat{p})/n]^{1\over 2},
\hat{p}+1.96[\hat{p}(1-\hat{p})/n]^{1\over 2}]\cr
&=
[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}].
\end{align*}



## Quiz: how many heads? ([web.meetoo.com](https://web.meetoo.com), 108-197-366)

Toss a coin 10 times. How many "heads" did you get?

\pause
Can work out $[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}]$ for each possible number of heads:

\small
```{r, echo = FALSE}
n <- 10
sum_y <- 0:n
y_bar <- sum_y / n
I_hat <- y_bar * (1 - y_bar) / n
lower <- y_bar - 1.96 * sqrt(I_hat)
upper <- y_bar + 1.96 * sqrt(I_hat)
CI <- cbind(lower, upper)
rownames(CI) <- 0:n
CI
```
\pause
\vspace{-0.3cm}
\normalsize
For the number of heads you got, does the interval contain $0.5$?

## Checking the coverage ($n = 10$, $\alpha = 0.05$)

```{r}
n <- 10
sum_y <- rbinom(10000, size = n, prob = 0.5)
y_bar <- sum_y / n
I_hat <- y_bar * (1 - y_bar) / n
lower <- y_bar - 1.96 * sqrt(I_hat)
upper <- y_bar + 1.96 * sqrt(I_hat)
coverage <- mean(lower < 0.5 & upper > 0.5)
coverage
```
\pause The actual coverage is lower than the nominal $95\%$ level.

## Checking the coverage ($n = 100$, $\alpha = 0.05$)

```{r}
n <- 100
sum_y <- rbinom(10000, size = n, prob = 0.5)
y_bar <- sum_y / n
I_hat <- y_bar * (1 - y_bar) / n
lower <- y_bar - 1.96* sqrt(I_hat)
upper <- y_bar + 1.96* sqrt(I_hat)
coverage <- mean(lower < 0.5 & upper > 0.5)
coverage
```
\pause The actual coverage is close to the nominal $95\%$ level.
The confidence interval is designed using an approximation which will
work well for large $n$, so this is expected.



## Comparing statistical models

Suppose we have a set of competing probability models
which might have generated the observed data. Which model
is most appropriate?

Suppose that we have two competing alternatives, $f^{(0)}_{\bm Y}$ (model
$H_0$) and $f^{(1)}_{\bm Y}$  (model $H_1$) for $f_{\bm Y}$,
the joint distribution of $Y_1, \ldots, Y_n$.

The most common situation is where $H_0$ and $H_1$
both take the same parametric form, $f_{\bm Y}(\bm{y};\bm \theta)$ but
with $\bm \theta\in\Theta^{(0)}$ for $H_0$ and $\bm \theta\in\Theta^{(1)}$ for $H_1$,
where $\Theta^{(0)}$ and $\Theta^{(1)}$ are alternative sets of possible values
for $\bm \theta$.

## Hypothesis testing

A hypothesis test provides a mechanism for comparing the two competing statistical
models, $H_0$ and $H_1$.

The null hypothesis $H_0$ is the reference model, and will be assumed to
be appropriate unless the observed data strongly indicate that $H_0$ is
inappropriate, and that $H_1$ (the *alternative* hypothesis) should be
preferred.

Hence, the fact that a hypothesis test does not reject $H_0$ should not be taken
as evidence that $H_0$ is true and $H_1$ is not, or that $H_0$ is  better
supported by the data than $H_1$, merely that the data does not provide
sufficient evidence to reject $H_0$ in favour of $H_1$.

## Critical region
A hypothesis test is defined by its *critical region* or 
*rejection region*, which we shall denote by $C$. 

- If $\bm{y} \in C$, $H_0$ is rejected in favour of $H_1$;
- If $\bm{y} \not\in C$, $H_0$ is not rejected.

## Size and power of a test

We define the *size* (or *significance level*) of the test
\[\alpha = \max_{\bm \theta\in\Theta^{(0)}}P(\bm Y\in C;\bm \theta)\]
This is the maximum probability of erroneously rejecting $H_0$, over all
possible distributions for $\bm Y$ implied by $H_0$.

We also define the power function 
\[\omega(\bm \theta)= P(\bm Y\in C;\bm \theta)\]
It represents the probability of rejecting $H_0$
for a particular value of $\bm \theta$.

A good test will
have small size, but large power.

## Fixing the size, maximising the power

In general, we fix $\alpha$ to be some small
value (often 0.05), so that the probability of erroneous rejection of $H_0$ is
limited. In doing this, we are giving $H_0$ precedence over $H_1$.

Given our specified $\alpha$, we try to choose a test
to make $\omega(\bm \theta)$ as large  as possible for
$\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}$.


## Generalised likelihood ratio test

Suppose that $H_0$ and $H_1$ both take the same parametric form,
$f_{\bm Y}(\bm{y};\bm \theta)$ with $\bm \theta\in\Theta^{(0)}$ for $H_0$ and $\bm \theta\in\Theta^{(1)}$
for $H_1$, where $\Theta^{(0)}$ and $\Theta^{(1)}$ are alternative sets of
possible values for $\bm \theta$.

A *generalised likelihood ratio test* of $H_0$ against $H_1$ has a critical region
of the form
\[C=\left\{ \bm{y}: 
\frac{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)} 
>k\right\}\]
where $k$ is determined by $\alpha$, the size of the test, so
\[\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.\]
We only reject
$H_0$ if the observed data are
much more probable under some distribution in $H_1$ 
than any distribution under $H_0$.

## Example (Bernoulli)
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Suppose that we require a size $\alpha$ test of the hypothesis
$H_0$: $p=p_0$ against the general alternative $H_1$: '$p$ is unrestricted' where
$\alpha$ and
$p_0$ are specified.

Here $\bm \theta=(p)$, $\Theta^{(0)}=\{p_0\}$ and $\Theta^{(1)}=(0,1)$ and the
generalised likelihood ratio test rejects $H_0$ when
\[{{\max_{p\in(0,1)}L(p)}\over{\max_{p=p_0}L(p)}} > k\]
or equivalently when
\[{{\bar y^{\sum_i y_i}(1-\bar y)^{n-\sum_i y_i}}\over {p_0^{\sum_i y_i}(1-p_0)^{n-\sum_i y_i}}} > k.\]

## Finding an equivalent critical region.
We reject $H_0$ when
\[{{\bar y^{\sum_i y_i}(1-\bar y)^{n-\sum_i y_i}}\over {p_0^{\sum_i y_i}(1-p_0)^{n-\sum_i y_i}}} > k.\]
Equivalently
\begin{equation}
\left({{\bar y}\over{p_0}}\right)^{n\bar y}
\left({{1-\bar y}\over{1-p_0}}\right)^{n(1-\bar y)} >k. 
\label{eq:her1}
\end{equation}
Now the left hand side of \eqref{eq:her1} is minimised as a function of $\bar y$
at $\bar y=p_0$ and increases as
$\bar y$ moves away from $p_0$ in either direction.
Therefore, the rejection region \eqref{eq:her1} is equivalent to
$$
C=\left\{ \bm{y}:\bar y > k' \text{ or } \bar y < k''\right\}
$$
where $k'$ and $k''$ are chosen so that
$$
P(\bm{y}\in C;p_0)=\alpha.
$$

## Choosing the cutoff points for the critical region
We want to choose $k'$ and $k''$ so that
if $Y_i \sim \text{Bernoulli}(p_0)$ then
\[P(\bar Y > k' \text{ or } \bar Y < k'')=\alpha.\]
or
\[P(k'' \leq \bar Y \leq k') = 1 - \alpha\]
or
\[P(n k'' \leq \sum_{i=1}^n Y_i \leq n k') = 1 - \alpha.\]
Therefore, we can use the binomial$(n,p_0)$ distribution to find a precise rejection
region for a test of specified size $\alpha$.

## Choosing the cutoff points for the critical region

Suppose $n = 10$, $p_0 = 0.5$ and $\alpha = 0.05$. So we want
to find $k'$ and $k''$ so that
\[P(10 k'' \leq \sum_{i=1}^{10} Y_i \leq 10 k') = 0.95,\]
if $Y_i \sim \text{Bernoulli}(0.5)$.

So we set $10 k''$ to be the $0.025$ point of a $\text{binomial}(10, 0.5)$
distribution, and $10 k'$ to be the $0.975$ point.
```{r}
qbinom(0.025, size = 10, prob = 0.5)
qbinom(0.975, size = 10, prob = 0.5)
```

## Choosing the cutoff points for the critical region

In this case $k' = 0.8$ and $k'' = 0.2$, so 
we reject $H_0: p = 0.5$ if 
$\bar y > 0.8$ or $\bar y < 0.2$.

In this case, we could work out $P(\mathbf{Y} \in C | H_0)$ exactly,
and choose the critical region $C$ to make this probability no more than $\alpha$.

In other cases, it will not be possible
to work out these probabilities exactly, and we will need an alternative
approach.

## Conclusion

- We have seen how to use the asymptotic distribution of the MLE
  to construct (approximate) confidence intervals.
- We have written down the form of a generalised likelihood ratio test.
  In some scenarios, you can find a critical region for a generalised likelihood
  ratio test to give $P(\mathbf{Y} \in C | H_0) = \alpha$.
- More commonly, it is not possible to find this exactly.
  Next time, we will see a general way to find a critical region with
  $P(\mathbf{Y} \in C | H_0) \approx \alpha$, in large samples.
- You should now be able to attempt exercise 1 on problem sheet 2.


