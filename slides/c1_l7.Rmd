---
title: "MATH3012: Chapter 1, Lecture 7"
date: ""
author: Helen Ogden
header-includes:
- \usepackage{bm}
- \usepackage{booktabs, multirow}
output:
  beamer_presentation
---


```{r, echo = FALSE}
knitr::opts_knit$set(root.dir = '../datasets')
```

## Recap

We have seen how to use the likelihood for parameter estimation,
and can quantify the uncertainty in our estimates by 
using the asymptotic distribution
of the MLE. Last time, we used this asymptotic distribution
to construct large-sample confidence intervals for parameters.

We can also use the likelihood for hypothesis testing,
by using a generalised likelihood ratio test.

Let's review this now.

## Generalised likelihood ratio test: recap

A *generalised likelihood ratio test* of $H_0$ against $H_1$ has a critical region
of the form
$$
C=\left\{ \bm{y}:{{\max_{\bm \theta\in \Theta^{(1)}} L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}} >k\right\}
$$
where $k$ is determined by $\alpha$, the size of the test, so
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
$$

Therefore, in order to determine $k$, we need
to know the distribution of the likelihood ratio, or an equivalent statistic,
under $H_0$.
In general, this will not be available to us.
However, we can make use of an important asymptotic result.

## The log likelihood ratio statistic

First we notice that, as $\log$ is a strictly increasing function, the rejection
region is equivalent to

$$
C=\left\{ \bm{y}: 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right) >k'\right\}
$$
where
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
$$
We call 
$$
L_{01}\equiv 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)
$$
the *log likelihood ratio statistic*

## Asymptotic distribution of the log likelihood ratio statistic

If $H_0$ is *nested within* $H_1$, in other words
$\Theta^{(0)}\subset\Theta^{(1)}$ ($\Theta^{(0)}$ is a subspace of
$\Theta^{(1)}$) then under
$H_0$: $\bm \theta\in\Theta^{(0)}$, asymptotically as
$n\to\infty$, $L_{01}$
has a chi-squared distribution with degrees of freedom equal to the difference in
the dimensions of $\Theta^{(1)}$ and $\Theta^{(0)}$.

A sketch proof is in the notes, but is not examinable.

\pause
So a *log likelihood ratio test* rejects $H_0$ if $L_{01}$
exceeds the $100(1 - \alpha)\%$ point of the relevant chi-squared 
distribution.

## Example (Bernoulli)
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Suppose that we require a size $\alpha$ test of the hypothesis
$H_0$: $p=p_0$ against the general alternative $H_1$: '$p$ is unrestricted' where
$\alpha$ and
$p_0$ are specified.

Here $\bm \theta=(p)$, $\Theta^{(0)}=\{p_0\}$ and $\Theta^{(1)}=(0,1)$ and the
log likelihood ratio statistic is
$$
L_{01}=2n\bar y\log\left({{\bar y}\over{p_0}}\right)
+2n(1-\bar y)\log\left({{1-\bar y}\over{1-p_0}}\right).
$$
As $d_1=1$ and $d_0=0$, under $H_0$, the log likelihood ratio statistic
has an asymptotic $\chi^2_1$ distribution.

## Finding the critical value

We reject $H_0$ if $L_{01}$ is 'too large' to have come from a
$\chi^2_1$ distribution.

If $\alpha = 0.05$, then we should reject $H_0$
if the test statistic is greater than 
the 95\% point of the
$\chi^2_1$ distribution:
```{r}
qchisq(0.95, df = 1)
```

## Comparing with an exact test

Last lecture,  we constructed an exact test 
for $H_0: p = p_0$ vs. $H_1$: '$p$ is unrestricted'.

In the case
 $n = 10$, $p_0 = 0.5$ and $\alpha = 0.05$, we
 found we should reject $H_0$ at the $5\%$ level
 if $\bar y < 0.2$ or $\bar y > 0.8$.
 

Using the log likelihood ratio test, we will reject $H_0$
if 
$$
L_{01}=20 \bar y\log\left(2 {\bar y}\right)
+20(1-\bar y)\log\left(2 (1-\bar y)\right) > 3.84
$$

## Comparing with an exact test

\scriptsize
```{r}
L01 <- function(ybar) {
  20 * ybar * log(2 * ybar) + 20 * (1 - ybar) * log(2 * (1 - ybar))
}
root_fun <- function(ybar) {
  L01(ybar) - qchisq(0.95, df = 1)
}
crit1 <- uniroot(f = root_fun, interval = c(0.01, 0.5))$root
crit2 <- uniroot(f = root_fun, interval = c(0.5, 0.99))$root
c(crit1, crit2)
```
\normalsize
So the log likelihood ratio test rejects 
$H_0$ if $\bar y \geq 0.8$ or $\bar y \leq 0.2$.

This is similar to the exact test,
but the exact test does not reject $H_0$ 
if $\bar y = 0.2$ or $\bar y = 0.8$.


# Linear Models (a brief revision)

## Setup

The aim of a linear model is to determine the pattern of 
dependence of the response variable on the explanatory variables.

We denote the $n$ observations of the response variable by $\bm{y}=(y_1,y_2,\ldots ,y_n)^T$.
These are assumed to be observations of *random variables*
$\bm Y=(Y_1,Y_2,\ldots ,Y_n)^T$.
Associated with each $y_i$ is a vector $\bm{x}_i=(x_{i1},x_{i2},\ldots ,x_{ip})^T$ of values of $p$
explanatory variables.

## The linear model
In a linear model, we assume that 
\vspace{-0.8cm}
\begin{align*}
Y_i&= \beta_1 x_{i1} +\beta_2 x_{i2} +\ldots + \beta_p x_{ip} + \epsilon_i \cr
&= \sum_{j=1}^p x_{ij} \beta_j + \epsilon_i \cr
&=  \bm{x}_i^T\bm{\beta} + \epsilon_i \cr
&= [\bm{X}\bm{\beta}]_i + \epsilon_i,\qquad i=1,\ldots ,n  
\end{align*}
where $\epsilon_i \sim N(0, \sigma^2)$ independently,
\[\bm{X}= \begin{pmatrix}
\bm{x}_1^T \\
\vdots \\
\bm{x}_n^T
\end{pmatrix}
=\begin{pmatrix}
x_{11} & \cdots &x_{1p}\cr
\vdots & \ddots &\vdots\cr
x_{n1} &\cdots &x_{np}
\end{pmatrix}\]
is a $n\times p$ *design matrix*, 
and
$\bm{\beta}=(\beta_1,\ldots ,\beta_p)^T$ is a vector of unknown parameters
describing the dependence of $Y_i$ on $\bm{x}_i$.

## Matrix form of the linear model

The linear model may be written in matrix form as
\begin{equation}
\bm{Y}=\bm{X}\bm{\beta} + \bm{\epsilon}. 
\label{eq:lmMat}
\end{equation}
where $\bm{\epsilon}=(\epsilon_1,\epsilon_2,\ldots ,\epsilon_n)^T$.

The error vector $\bm{\epsilon} \sim N(\bf 0, \sigma^2\bm{I})$
since $\text{Var}(\epsilon_i)=\sigma^2$, and $\text{Cov}(\epsilon_i,\epsilon_j)=0$
as $\epsilon_1, \ldots, \epsilon_n$ are independent of one another.

It follows from \eqref{eq:lmMat} that $\bm Y\sim N(\bm{X}\bm{\beta},\sigma^2\bm{I})$.

## Example: The null model
$$
Y_i=\beta_1 + \epsilon_i \qquad i = 1, \ldots, n
$$
$$
\bm{X}=\begin{pmatrix}
1\cr
1\cr
\vdots
\cr 1
\end{pmatrix}
\qquad
\bm{\beta}=(\beta_1).
$$
One (dummy) explanatory variable. In practice, this variable is present
in all models.

## Example: simple linear regression
$$
Y_i=\beta_1+\beta_2 x_i + \epsilon_i \qquad i = 1, \ldots, n
$$
$$
\bm{X}=\begin{pmatrix} 1&x_1\cr 1&x_2\cr \vdots&\vdots\cr 1&x_n \end{pmatrix}
\qquad \bm{\beta}=\begin{pmatrix} \beta_1\cr\beta_2 \end{pmatrix}
$$
Two explanatory variables; the dummy variable
and one 'real' variable.

## Example: Polynomial regression
$$
Y_i=\beta_1+\beta_2 x_i+\beta_3 x_i^2 +\ldots +\beta_p x_i^{p-1} + \epsilon_i \qquad i = 1, \ldots, n
$$
$$
\bm{X}= \begin{pmatrix}
1&x_1&x_1^2&\cdots&x_1^{p-1}\cr
1&x_2&x_2^2&\cdots&x_2^{p-1}\cr
\vdots&\vdots&\vdots&\ddots&\vdots\cr
1&x_n&x_n^2&\cdots&x_n^{p-1} \end{pmatrix}
\qquad \bm{\beta}= \begin{pmatrix} \beta_1\cr\beta_2\cr\vdots\cr\beta_p \end{pmatrix}
$$
$p$ explanatory variables; the dummy variable
and one 'real' variable, transformed to $p-1$ variables.

## Example: Multiple regression
$$
Y_i=\beta_1+\beta_2 x_{i1}+\beta_3 x_{i2} +\ldots +\beta_p x_{i\,p-1} + \epsilon_i \qquad i = 1, \ldots, n
$$
$$
\bm{X}= \begin{pmatrix} 1&x_{11}&x_{12}&\cdots&x_{1\,p-1}\cr
1&x_{21}&x_{22}&\cdots&x_{2\,p-1}\cr
\vdots&\vdots&\vdots&\ddots&\vdots\cr
1&x_{n1}&x_{n2}&\cdots&x_{n\,p-1} 
\end{pmatrix}
\qquad \bm{\beta}=\begin{pmatrix} \beta_1\cr\beta_2\cr\vdots\cr\beta_p \end{pmatrix}
$$
$p$ explanatory variables; the dummy variable
and $p-1$ 'real' variables.

## Conclusion

- We have seen how to use a log likelihood ratio test to compare 
  nested hypotheses. The log likelihood ratio test statistic 
  will have approximately chi-squared distribution under $H_0$,
  in large samples.
- We have briefly reviewed the linear model.
- You should now be able to attempt exercises 1 and 2 on problem sheet 2.
- Next time, we'll look at likelihood inference and hypothesis testing in
linear models.


