---
title: "Solutions to MATH3012 problem sheet 3"
author: ""
date: ""
output: pdf_document
fontsize: 12pt
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\benum

\item We have
\[f_Y(y; \lambda) = \lambda e^{-\lambda y} 
= \exp\left(- \lambda y + \log \lambda \right).\]
This is of exponential family form, with $\theta = - \lambda$,
$b(\theta) = - \log \lambda = - \log(-\theta)$,
$a(\phi) = 1$ and $c(y, \phi) = 0$.

So
\[E(Y) = b'(\theta) = \frac{d}{d \theta}\left( - \log(-\theta)\right) = -\frac{1}{\theta} = \frac{1}{\lambda} = \mu,\]
\[\text{Var}(Y) = a(\phi) b''(\theta) = 1 \times \frac{d}{d \theta}\left(- \theta^{-1}\right) 
= \theta^{-2} = \lambda^{-2},\]
and the variance function is
\[V(\mu) = \theta^{-2} = [-\mu^{-1}]^2 = \mu^2.\]

\item We have
\begin{align*}
f_Y(y; p) &= p (1 - p)^{y-1} \\
&= \exp\left(\log p + (y-1) \log(1 - p)\right) \\
&= \exp\left(y \log(1 - p) + \log \frac{p}{1-p}\right).
\end{align*}
This is of exponential family form, with $\theta = \log(1 - p)$
(so $p = 1 - e^\theta$),
\[b(\theta) = - \log\left(\frac{p}{1-p}\right) = -\log \left( \frac{1 - e^\theta}{e^\theta}\right) = -\log(e^{-\theta} - 1),\]
$a(\phi) = 1$ and $c(y, \phi) = 0$.

So
\[E(Y) = b'(\theta) = \frac{d}{d \theta}\left(-\log(e^{-\theta} - 1)\right) = \frac{e^{-\theta}}{e^{-\theta} - 1}
= \frac{1}{1 - e^\theta} = \frac{1}{p} = \mu,\]
\[\text{Var}(Y) = a(\phi) b''(\theta) = 1 \times \frac{d}{d \theta}\left(\frac{1}{1 - e^\theta} \right) 
= \frac{e^\theta}{(1 - e^\theta)^2} = \frac{1 - p}{p^2},\]
and the variance function is
\[V(\mu) = \frac{e^\theta}{(1 - e^\theta)^2} = \frac{1 - \mu^{-1}}{\left(\mu^{-1}\right)^2} = \mu(\mu - 1).\]

\item 
For the exponential distribution, we have $b'(\theta) = - \theta^{-1} = \mu$, so 
the canonical link is
\[g(\mu) = b'^{-1}(\mu) = - \mu^{-1}.\]

For the geometric distribution, we have $b'(\theta) = (1 - e^\theta)^{-1} = \mu$.
Solving for $\theta$, we find 
\[1 - e^{\theta} = \mu^{-1}\]
so
\[\theta = \log(1 - \mu^{-1}),\] so 
the canonical link is
\[g(\mu) = b'^{-1}(\mu) = \log(1 - \mu^{-1}).\]

\item
\benum
\item We have
\[\mu_i = \mu_i(\bm \beta) = \exp(\beta_1 + \beta_2 x_i),\]
and the p.f. for the $i$th observation is
\[f_{Y_i}(y_i; \mu_i) = \frac{\mu_i^{y_i} e^{-\mu_i}}{y_i!}.\]
The likelihood is
\[L(\bm \beta) = \prod_{i=1}^n \frac{\mu_i(\bm \beta)^{y_i} e^{-\mu_i(\bm \beta)}}{y_i!} = \frac{\mu_i(\bm \beta)^{\sum_{i=1}^n y_i} e^{-\sum_{i=1}^n \mu_i(\bm \beta)}}{\prod_{i=1}^n y_i!}.\]
The log-likelihood is
\[\ell(\bm \beta) = \sum_{i=1}^n y_i \log \mu_i(\bm \beta) - \sum_{i=1}^n \mu_i(\bm \beta) - \sum_{i=1}^n \log y_i!.\]
Differentiating with respect to $\beta_i$, $i = 1, 2$, gives
\[\frac{\partial}{\partial \beta_i} \ell(\bm \beta) = \sum_{i=1}^n \frac{y_i}{\mu_i(\bm \beta)} \frac{\partial \mu_i}{\partial \beta_i} - \sum_{i=1}^n \frac{\partial \mu_i}{\partial \beta_i},\]
where
\[\frac{\partial \mu_i}{\partial \beta_1} = \exp(\beta_1 + \beta_2 x_i) = \mu_i(\bm \beta)\]
and
\[\frac{\partial \mu_i}{\partial \beta_2} = x_i \exp(\beta_1 + \beta_2 x_i) = x_i \mu_i(\bm \beta).\]
So
\[\frac{\partial}{\partial \beta_1} \ell(\bm \beta) = \sum_{i=1}^n y_i - \sum_{i=1}^n \mu_i(\bm \beta)
=  \sum_{i=1}^n y_i - \sum_{i=1}^n \exp(\beta_1 + \beta_2 x_i),\]
and
\[\frac{\partial}{\partial \beta_2} \ell(\bm \beta) = \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \mu_i(\bm \beta)
=  \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i).\]
So the MLE $\hat{\bm \beta} = (\hat \beta_1, \hat \beta_2)^T$ satisfies
\[\sum_{i=1}^n \exp(\hat \beta_1 + \hat \beta_2 x_i) = \sum_{i=1}^n y_i\]
and
\[\sum_{i=1}^n x_i \exp(\hat \beta_1 + \hat \beta_2 x_i) = \sum_{i=1}^n x_i y_i.\]
\item
Let $\bm \eta = X \bm \beta$, where 
\[X = \begin{pmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{pmatrix},
\]
then $Y_i \sim \text{Poisson}(\mu_i)$, where
$\mu_i = \exp(\eta_i)$.
\eenum

\eenum
