---
title: "Solutions to MATH3012 problem sheet 1"
author: ""
date: ""
output: pdf_document
fontsize: 12pt
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\benum

\item The p.f. of each $Y_i$ is
\[f_Y(y; \lambda) = \frac{\lambda e^{-\lambda y}}{y!},\]
so the likelihood is
\[L(\lambda) = \prod_{i=1}^n f_Y(y_i; \lambda)
= \prod_{i=1}^n \frac{\lambda e^{-\lambda y_i}}{y_i!} 
= \frac{e^{-n\lambda}\lambda^{\sum_{i=1}^n y_i}}{\prod_{i=1}^n y_i!}.\]
The log-likelihood is
\[\ell(\lambda) = \log L(\lambda) = -n\lambda+ \left(\sum_{i=1}^n y_i\right)\log{\lambda}- \sum_{i=1}^n\log(y_i!).\]
Differentiating with respect to $\lambda$ gives
\[\frac{\partial \ell(\lambda)}{\partial\lambda} = 
-n+ \frac{\sum_{i=1}^ny_i}{\lambda}.\]
So a stationary point $\hat \lambda$ solves
\[-n+ \frac{\sum_{i=1}^ny_i}{\hat \lambda} = 0,\]
which gives \[\hat \lambda = \frac{1}{n} \sum_{i=1}^n y_i = \bar y.\]
We have
\[\frac{\partial^2 \ell(\lambda)}{\partial\lambda} = 
-\frac{\sum_{i=1}^ny_i}{\lambda^2},\]
which is $\leq 0$ for all values of $\lambda$, so $\hat \lambda = \bar y$
is the MLE.

\item 
\benum
\item The likelihood is
\[L(\theta) = \prod_{i=1}^n f_Y(y_i; \theta) 
= \prod_{i=1}^n \theta \exp(-\theta y_i)
= \theta^n \exp(- \theta \sum_{i=1}^n y_i).\]
The log-likelihood is
\[\ell(\theta) = \log L(\theta) = n \log \theta - \theta \sum_{i=1}^n y_i.\]
The score is
\[u(\theta) = \frac{\partial}{\partial \theta} \ell(\theta)
= \frac{n}{\theta} - \sum_{i=1}^n y_i.
\]
So a stationary point of the log-likelihood $\hat \theta$ solves
\[u(\hat \theta) = \frac{n}{\hat \theta} - \sum_{i=1}^n y_i = 0,\]
which gives
\[\hat \theta = \frac{n}{\sum_{i=1}^n y_i} = \frac{1}{\bar y}.\]
The Hessian is
\[H(\theta) = \frac{\partial^2}{\partial \theta^2} \ell(\theta)
= - \frac{n}{\theta^2} < 0 \]
for all $\theta$, so $\hat \theta$ is the MLE.
The Fisher information is
\[{\cal I}(\theta) = E[-H(\theta)] 
= E\left[\frac{n}{\theta^2}\right]
= \frac{n}{\theta^2}.\]
\item 
The likelihood is
\[L(\theta) = \prod_{i=1}^n f_Y(y_i; \theta) 
= \prod_{i=1}^n \theta y_i^{\theta - 1}
= \theta^n \prod_{i=1}^n y_i^{\theta - 1}.\]
The log-likelihood is
\[\ell(\theta) = \log L(\theta) = n \log \theta - (\theta - 1) \sum_{i=1}^n \log y_i.\]
The score is
\[u(\theta) = \frac{n}{\theta} - \sum_{i=1}^n \log y_i
\]
So a stationary point of the log-likelihood $\hat \theta$ solves
\[u(\hat \theta) = \frac{n}{\hat \theta} - \sum_{i=1}^n \log y_i = 0,\]
which gives
\[\hat \theta = \frac{n}{\sum_{i=1}^n \log y_i}.\]
The Hessian is
\[H(\theta) =  - \frac{n}{\theta^2} < 0 \]
for all $\theta$, so $\hat \theta$ is the MLE.
The Fisher information is
\[{\cal I}(\theta) = E[-H(\theta)] 
= E\left[\frac{n}{\theta^2}\right]
= \frac{n}{\theta^2}.
\]

\item 
The likelihood is
\[L(\theta) = \prod_{i=1}^n f_Y(y_i; \theta) 
= \prod_{i=1}^n \theta(1 - \theta)^{y_i - 1}
= \theta^n (1 - \theta)^{\sum_{i=1}^n y_i - n}.
\]
The log-likelihood is
\[\ell(\theta) = \log L(\theta) = n \log \theta + \left(\sum_{i=1}^n y_i - n\right) \log (1-\theta).\]
The score is
\[u(\theta) = \frac{n}{\theta} - \frac{\sum_{i=1}^n y_i - n}{1 - \theta}
\]
So a stationary point of the log-likelihood $\hat \theta$ solves
\[u(\hat \theta) = \frac{n}{\hat \theta} - \frac{\sum_{i=1}^n y_i - n}{1 - \hat \theta} = 0,\]
which gives
\[\hat \theta = \frac{n}{\sum_{i=1}^n y_i} = \frac{1}{\bar y}.\]
The Hessian is
\[H(\theta) =  - \frac{n}{\theta^2} - \frac{\sum_{i=1}^n y_i - n}{(1 - \theta)^2}< 0 \]
for all $\theta$, as each $y_i \geq 1$ so $\sum_{i=1}^n y_i - n \geq 0$. So 
$\hat \theta$ is the MLE.

The Fisher information is
\begin{align*}
{\cal I}(\theta) &= E[-H(\theta)]  \\
&= E\left[\frac{n}{\theta^2} + \frac{\sum_{i=1}^n Y_i - n}{(1 - \theta)^2}\right] \\
&= \frac{n}{\theta^2} + \frac{n E(Y_i)}{(1 - \theta)^2} - \frac{n}{(1 - \theta)^2} \\
&=  \frac{n}{\theta^2} + \frac{n}{\theta(1 - \theta)^2}  - \frac{n}{(1 - \theta)^2} \\
&= \frac{n[(1 - \theta)^2 + \theta - \theta^2]}{\theta^2(1 - \theta^2)} \\
&= \frac{n (1 - \theta)}{\theta^2 (1 - \theta)^2} \\
&= \frac{n}{\theta^2 (1 - \theta)}. 
\end{align*}
\eenum

\item We have $\bm \theta = (\beta_0, \beta_1, \sigma^2)$, and
\[f_{Y_i}(y_i; \bm \theta) = 
\frac{1}{\sqrt{2 \pi \sigma^2}} 
\exp\left\{- \frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right\}\]
(the $N(\beta_0 + \beta_1 x_i, \sigma^2)$ p.d.f.),
so the likelihood is
\begin{align*}
L(\bm \theta) &= \prod_{i=1}^n f_Y(y_i; \lambda) \\
&= \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp\left\{- \frac{1}{2 \sigma^2}(y_i - \beta_0 - \beta_1 x_i)^2\right\} \\
&= (2 \pi \sigma^2)^{-\frac{n}{2}} \exp\left\{- \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2\right\}
\end{align*}
The log-likelihood is
\[\ell(\bm \theta) = \log L(\bm \theta) = -\frac{n}{2} \log (2 \pi \sigma^2)
- \frac{1}{2 \sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2.\]
Differentiating with respect to each component of $\bm \theta$, we have
\[\frac{\partial \ell(\bm \theta)}{\partial \beta_0} = 
\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i),\]
\[\frac{\partial \ell(\bm \theta)}{\partial \beta_1} = 
\frac{1}{\sigma^2} \sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i),\]
and
\[\frac{\partial \ell(\bm \theta)}{\partial \sigma^2} = 
- \frac{n}{2 \sigma^2} + \frac{1}{2 (\sigma^2)^2} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2.\]

So a stationary point 
$\hat {\bm \theta} = (\hat \beta_0, \hat \beta_1, \hat \sigma^2)$ solves
\begin{equation}
\frac{1}{\hat \sigma^2} \sum_{i=1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i) = 0,
\label{eq:beta_0}
\end{equation}
\begin{equation}
\frac{1}{\hat \sigma^2} \sum_{i=1}^n x_i (y_i - \hat \beta_0 - \hat \beta_1 x_i) = 0,
\label{eq:beta_1}
\end{equation}
and
\begin{equation}
- \frac{n}{2 \hat \sigma^2} + \frac{1}{2 (\hat \sigma^2)^2} \sum_{i=1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2 = 0
\label{eq:sigma^2}
\end{equation}
Rearraging \eqref{eq:beta_0}
\[\hat \beta_0 = \bar y - \hat \beta_1 \bar x.\]
Rearranging \eqref{eq:beta_1} gives
\[\overline{x y} - \hat \beta_0 \bar x - \hat \beta_1 \overline{x^2} = 0, \]
where $\overline{x y} = \frac{1}{n} \sum_{i = 1}^n x_i y_i$
and $\overline{x^2} = \frac{1}{n} \sum_{i=1}^n x_i^2$.
Substituting $\hat \beta_0$, we get
\[\overline{x y} - (\bar y - \hat \beta_1 \bar x) \bar x - \hat \beta_1 \overline{x^2} = 0,\]
which gives
\[\hat \beta_1 = \frac{\bar x \bar y - \overline{xy}}{\bar x^2 - \overline{x^2}}.\]
Rearranging \eqref{eq:sigma^2} gives
and \[\hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2.\]
For this question, we assume that this stationary point is a maximum,
so the MLE is $(\hat \beta_0, \hat \beta_1, \hat \sigma^2)$.

The MLE of $\sigma$ is
\[\hat \sigma = \sqrt{\hat \sigma^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat \beta_0 - \hat \beta_1 x_i)^2} .\]


\item 
\benum
\item The score is
\[U(\theta) = \frac{n}{\theta} - \sum_{i=1}^n Y_i.\]
Since $E(U(\theta)) = 0$,
\[E\left(\sum_{i=1}^n Y_i\right) = \frac{n}{\theta},\]
so \[E(\bar Y) = \frac{1}{\theta},\]
and $\bar Y$ is an unbiased estimator of $\theta^{-1}$.
\item The score is
\[U(\theta) = \frac{n}{\theta} - \sum_{i=1}^n \log Y_i.\]
Since $E(U(\theta)) = 0$,
\[E\left(\sum_{i=1}^n \log Y_i\right) = \frac{n}{\theta},\]
so \[E\left(\frac{1}{n} \sum_{i=1}^n \log Y_i\right) = \frac{1}{\theta},\]
and $\frac{1}{n} \sum_{i=1}^n \log Y_i$ is an unbiased estimator
of $\theta^{-1}$.
\item The score is
\[u(\theta) = \frac{n}{\theta} - \frac{\sum_{i=1}^n Y_i - n}{1 - \theta}.\]
Since $E(U(\theta)) = 0$,
\[\frac{E\left(\sum_{i=1}^n Y_i\right) - n}{1 - \theta} = \frac{n}{\theta},\]
so we have
\[\frac{E(\bar Y) - 1}{1 - \theta} = \frac{1}{\theta},\]
and \[E(\bar Y) = \frac{1}{\theta}.\]
So $\bar Y$ is an unbiased estimator of $\theta^{-1}$.
\eenum
\eenum
