---
title: "MATH3012: Solutions to additional exercises on Chapters 1 and 2"
author: ""
date: ""
output: pdf_document
fontsize: 12pt
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = file.path('..', 'datasets'))
```


\benum
\item 
\benum
\item \label{pt:gamma_ef} 
We have
\[f_y(y; \lambda) = \exp \{2 \log \lambda + \log y - \lambda y \}.\]
So $\theta = \lambda$, $a(\phi) = -1$
\[b(\theta) = 2 \log \lambda = 2 \log \theta,\]
and $c(y, \phi) = \log y$. So
\[E(Y) = b'(\theta) = \frac{2}{\theta} = \frac{2}{\lambda}\]
and
\[\text{Var}(Y) = a(\phi) b''(\theta) = -1 \times -2 \theta^{-2} = 2 \theta^{-2} = 2 \lambda^{-2}.\]

\item
The likelihood is
\[L(\lambda) = \prod_{i=1}^n \lambda^2 y_i e^{-\lambda y_i}\]
so the log-likelihood is
\begin{align*}
\ell(\lambda) &=  \sum_{i=1}^n (2 \log \lambda + \log y_i - \lambda y_i)\\
&= 2 n \log \lambda + \sum_{i=1}^n \log y_i - \lambda \sum_{i=1}^n y_i.
\end{align*}
The score is
\[u(\lambda) = \frac{d}{d \lambda} \ell (\lambda) = \frac{2n}{\lambda} - \sum_{i=1}^n y_i,\]
so a stationary point of the log-likelihood $\hat \lambda$ satisfies
\[\frac{2n}{\hat \lambda} - \sum_{i=1}^n y_i = 0,\]
or $\hat \lambda = 2 (\bar y)^{-1}$.
We have 
\[H(\lambda) = \frac{d^2}{d\lambda^2} \ell (\lambda) = \frac{-2n}{\lambda^2} < 0\]
for all $\lambda$, so $\hat \lambda$ is a maximum of the loglikelihood, and hence is
the MLE.
The Fisher information is
\[I(\lambda) = E\left(-H(\lambda)\right) = E\left(\frac{2n}{\lambda^2}\right) = \frac{2n}{\lambda^2}.\]
The asymptotic distribution of $\hat \lambda$ is
\[\hat \lambda \sim N\left(\lambda, \frac{\lambda^2}{2n} \right).\]

\item
Asymptotically, we have
\[\frac{\sqrt{2n}(\hat \lambda - \lambda)}{\lambda} \sim N(0, 1),\]
so 
\[P(z_{0.035} \leq \frac{\sqrt{2n}(\hat \lambda - \lambda)}{\lambda} \leq z_{0.965}) \approx 0.97,\] 
or
\[P(\hat \lambda - \frac{\lambda}{\sqrt{2 n}}z_{0.965}\leq \lambda \leq \hat \lambda - \frac{\lambda}{\sqrt{2 n}}z_{0.035}) \approx 0.97.\]
Replacing $\lambda$ with $\hat \lambda$ in the two endpoints
\[  \left[\hat \lambda - \frac{\hat \lambda}{\sqrt{2 n}}z_{0.965}, \hat \lambda - \frac{\hat \lambda}{\sqrt{2 n}}z_{0.035}\right]\]
is a $97\%$ confidence interval for $\lambda$, where
$z_{0.035} = - z_{0.965}$, and $z_{0.965}$ is
```{r}
qnorm(0.965)
```

\item We have 
\[L_{01} = 2[\ell(\hat \lambda) - \ell(0.5)],\]
where
\[\ell(\lambda) = 2 n \log \lambda + \sum_{i=1}^n \log y_i - \lambda \sum_{i=1}^n y_i\]
and $\hat \lambda = 2 (\bar y)^{-1}$.
Under $H_0$, $L_{01} \sim \chi^2_1$, so 
for a test of size $\alpha = 0.01$, 
we reject $H_0$ if $L_{01} > k$, where we can find $k$
in `R` with
```{r}
qchisq(0.99, df = 1)
```

\eenum
\item
\benum
\item From question \ref{pt:gamma_ef}, we have
\begin{equation}
\mu = b'(\theta) = 2 \theta^{-1}.
\label{eq:mu_theta}
\end{equation}
The canonical link is $\eta = g(\mu)$, such that
$g(\mu) = (b')^{-1}(\mu)$. Writing $\theta$ in terms of $\mu$
from \eqref{eq:mu_theta} gives $\theta = 2 / \mu$, so the canonical link is
$g(\mu) = 2 / \mu$.
\item
\benum
\item 
$\mu_i = b'(\eta_i) = 2 \eta_i^{-1} = \frac{2}{{\bm x}_i^T {\bm \beta}}.$
\item $\theta_i = \eta_i = {\bm x}_i^T {\bm \beta}$.
\item $\lambda_i = \theta_i = {\bm x}_i^T {\bm \beta}$.
\eenum
\item We need $\lambda_i > 0$, but $\eta_i \in \mathbb{R}$, so with the canonical
link $\lambda_i = \eta_i$ could take any real value, which is not appropriate.
\item A better model would be
\[\lambda_i = \exp({\bm x}_i^T {\bm \beta}) = \exp(\eta_i).\]
We have $\lambda_i = \theta_i$, so to get this, set
\[\theta_i = (b')^{-1}(\mu_i) = (b')^{-1}(g^{-1}(\eta_i)) = \exp(\eta_i).\]
So
\[g^{-1}(\eta_i) = b'(\exp(\eta_i)) = \frac{2}{\exp(\eta_i)}.\]
Solving $g^{-1}(\eta) = \mu$ for $\eta$, we get
$\eta = \log \frac{\mu}{2}$, so
$g(\mu)= \log \frac{\mu}{2}$ is the required link function.
\item We have
\[\lambda_i = \exp(\beta_1 + \beta_2 x_i),\]
so the likelihood is 
\[L({\bm \beta}) = \prod_{i=1}^n [\exp(\beta_1 + \beta_2 x_i)]^2 y_i 
\exp(-\exp(\beta_1 + \beta_2 x_i)y_i).\]
The log-likelihood is
\[\ell({\bm \beta}) = \sum_{i=1}^n 2(\beta_1 + \beta_2 x_i) + \log y_i
- \exp(\beta_1 + \beta_2 x_i) y_i,\]
so 
\[u_1({\bm \beta}) = \sum_{i=1}^n (2 - \exp(\beta_1 + \beta_2 x_i)y_i)\]
and
\[u_2({\bm \beta}) = \sum_{i=1}^n (2 x_i - x_i \exp(\beta_1 + \beta_2 x_i) y_i).\]
So the MLEs $\hat \beta_1$ and $\hat \beta_2$ satisfy
\[ \sum_{i=1}^n (2 - \exp(\hat \beta_1 + \hat \beta_2 x_i)y_i) = 0\]
and
\[\sum_{i=1}^n (2 x_i - x_i \exp(\hat\beta_1 + \hat \beta_2 x_i) y_i) = 0.\]
\item The observed information matrix is
\[- H({\bm \beta}) = \begin{pmatrix}
\sum_{i=1}^n \exp(\beta_1 + \beta_2 x_i) y_i & \sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) y_i \\
\sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) y_i & \sum_{i=1}^n x_i^2 \exp(\beta_1 + \beta_2 x_i) y_i
\end{pmatrix}.\]
The Fisher information matrix is
\begin{align*}
{\cal I}({\bm \beta}) &= E[- H({\bm \beta})]  \\
&= \begin{pmatrix}
\sum_{i=1}^n \exp(\beta_1 + \beta_2 x_i) E(Y_i) & \sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) E(Y_i) \\
\sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) E(Y_i) & \sum_{i=1}^n x_i^2 \exp(\beta_1 + \beta_2 x_i) E(Y_i)
\end{pmatrix} \\
&= \begin{pmatrix}
\sum_{i=1}^n 2 & \sum_{i=1}^n 2 x_i  \\
\sum_{i=1}^n 2 x_i & \sum_{i=1}^n 2 x_i^2 
\end{pmatrix}
\end{align*}
\item Model (2) is the special case of model (1)
with $\beta_2 = \beta_3 = 0$, so model (2) is nested
in model (1). So to compare the two models, use the null
hypothesis $H_0: \beta_2 = \beta_3 = 0$
against the alternative
$H_1: \text{"$\beta_2$ and $\beta_3$ unrestricted"}$.
\item We have $L_{01} = 11.0 - 5.0 = 6.0$.
Under $H_0$, $L_{01}$ has approximate $\chi^2_2$ distribution.
\eenum
\eenum
