---
title: "MATH3012: Solutions to additional exercises on Chapters 1 and 2"
author: ""
date: ""
output: pdf_document
fontsize: 12pt
header-includes:
- \newcommand{\benum}{\begin{enumerate}}
- \newcommand{\eenum}{\end{enumerate}}
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
- \usepackage{bm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = file.path('..', 'datasets'))
```


\benum
\item 
\benum
\item \label{pt:gamma_ef} 
We have
\[f_y(y; \lambda) = \exp \{2 \log \lambda + \log y - \lambda y \}.\]
So $\theta = \lambda$, $a(\phi) = -1$
\[b(\theta) = 2 \log \lambda = 2 \log \theta,\]
and $c(y, \phi) = \log y$. So
\[E(Y) = b'(\theta) = \frac{2}{\theta} = \frac{2}{\lambda}\]
and
\[\text{Var}(Y) = a(\phi) b''(\theta) = -1 \times -2 \theta^{-2} = 2 \theta^{-2} = 2 \lambda^{-2}.\]

\item
The likelihood is
\[L(\lambda) = \prod_{i=1}^n \lambda^2 y_i e^{-\lambda y_i}\]
so the log-likelihood is
\begin{align*}
\ell(\lambda) &=  \sum_{i=1}^n (2 \log \lambda + \log y_i - \lambda y_i)\\
&= 2 n \log \lambda + \sum_{i=1}^n \log y_i - \lambda \sum_{i=1}^n y_i.
\end{align*}
The score is
\[u(\lambda) = \frac{d}{d \lambda} \ell (\lambda) = \frac{2n}{\lambda} - \sum_{i=1}^n y_i,\]
so a stationary point of the log-likelihood $\hat \lambda$ satisfies
\[\frac{2n}{\hat \lambda} - \sum_{i=1}^n y_i = 0,\]
or $\hat \lambda = 2 (\bar y)^{-1}$.
We have 
\[H(\lambda) = \frac{d^2}{d\lambda^2} \ell (\lambda) = \frac{-2n}{\lambda^2} < 0\]
for all $\lambda$, so $\hat \lambda$ is a maximum of the log-likelihood, and hence is
the MLE.
The Fisher information is
\[I(\lambda) = E\left(-H(\lambda)\right) = E\left(\frac{2n}{\lambda^2}\right) = \frac{2n}{\lambda^2}.\]
The asymptotic distribution of $\hat \lambda$ is
\[\hat \lambda \sim N\left(\lambda, \frac{\lambda^2}{2n} \right).\]

\item
Asymptotically, we have
\[\frac{\sqrt{2n}(\hat \lambda - \lambda)}{\lambda} \sim N(0, 1),\]
so 
\[P(z_{0.035} \leq \frac{\sqrt{2n}(\hat \lambda - \lambda)}{\lambda} \leq z_{0.965}) \approx 0.97,\] 
or
\[P(\hat \lambda - \frac{\lambda}{\sqrt{2 n}}z_{0.965}\leq \lambda \leq \hat \lambda - \frac{\lambda}{\sqrt{2 n}}z_{0.035}) \approx 0.97.\]
Replacing $\lambda$ with $\hat \lambda$ in the two endpoints
\[  \left[\hat \lambda - \frac{\hat \lambda}{\sqrt{2 n}}z_{0.965}, \hat \lambda - \frac{\hat \lambda}{\sqrt{2 n}}z_{0.035}\right]\]
is a $97\%$ confidence interval for $\lambda$, where
$z_{0.035} = - z_{0.965}$, and $z_{0.965}$ is
```{r}
qnorm(0.965)
```

\item We have 
\[L_{01} = 2[\ell(\hat \lambda) - \ell(0.5)],\]
where
\[\ell(\lambda) = 2 n \log \lambda + \sum_{i=1}^n \log y_i - \lambda \sum_{i=1}^n y_i\]
and $\hat \lambda = 2 (\bar y)^{-1}$.
Under $H_0$, $L_{01} \sim \chi^2_1$, so 
for a test of size $\alpha = 0.01$, 
we reject $H_0$ if $L_{01} > k$, where we can find $k$
in `R` with
```{r}
qchisq(0.99, df = 1)
```

\eenum
\item
\benum
\item From question \ref{pt:gamma_ef}, we have
\begin{equation}
\mu = b'(\theta) = 2 \theta^{-1}.
\label{eq:mu_theta}
\end{equation}
The canonical link is $\eta = g(\mu)$, such that
$g(\mu) = (b')^{-1}(\mu)$. Writing $\theta$ in terms of $\mu$
from \eqref{eq:mu_theta} gives $\theta = 2 / \mu$, so the canonical link is
$g(\mu) = 2 / \mu$.
\item
\benum
\item 
$\mu_i = b'(\eta_i) = 2 \eta_i^{-1} = \frac{2}{{\bm x}_i^T {\bm \beta}}.$
\item $\theta_i = \eta_i = {\bm x}_i^T {\bm \beta}$.
\item $\lambda_i = \theta_i = {\bm x}_i^T {\bm \beta}$.
\eenum
\item We need $\lambda_i > 0$, but $\eta_i \in \mathbb{R}$, so with the canonical
link $\lambda_i = \eta_i$ could take any real value, which is not appropriate.
\item A better model would be
\[\lambda_i = \exp({\bm x}_i^T {\bm \beta}) = \exp(\eta_i).\]
We have $\lambda_i = \theta_i$, so to get this, set
\[\theta_i = (b')^{-1}(\mu_i) = (b')^{-1}(g^{-1}(\eta_i)) = \exp(\eta_i).\]
So
\[g^{-1}(\eta_i) = b'(\exp(\eta_i)) = \frac{2}{\exp(\eta_i)}.\]
Solving $g^{-1}(\eta) = \mu$ for $\eta$, we get
$\eta = \log \frac{\mu}{2}$, so
$g(\mu)= \log \frac{\mu}{2}$ is the required link function.
\item We have
\[\lambda_i = \exp(\beta_1 + \beta_2 x_i),\]
so the likelihood is 
\[L({\bm \beta}) = \prod_{i=1}^n [\exp(\beta_1 + \beta_2 x_i)]^2 y_i 
\exp(-\exp(\beta_1 + \beta_2 x_i)y_i).\]
The log-likelihood is
\[\ell({\bm \beta}) = \sum_{i=1}^n 2(\beta_1 + \beta_2 x_i) + \log y_i
- \exp(\beta_1 + \beta_2 x_i) y_i,\]
so 
\[u_1({\bm \beta}) = \sum_{i=1}^n (2 - \exp(\beta_1 + \beta_2 x_i)y_i)\]
and
\[u_2({\bm \beta}) = \sum_{i=1}^n (2 x_i - x_i \exp(\beta_1 + \beta_2 x_i) y_i).\]
So the MLEs $\hat \beta_1$ and $\hat \beta_2$ satisfy
\[ \sum_{i=1}^n (2 - \exp(\hat \beta_1 + \hat \beta_2 x_i)y_i) = 0\]
and
\[\sum_{i=1}^n (2 x_i - x_i \exp(\hat\beta_1 + \hat \beta_2 x_i) y_i) = 0.\]
\item The observed information matrix is
\[- H({\bm \beta}) = \begin{pmatrix}
\sum_{i=1}^n \exp(\beta_1 + \beta_2 x_i) y_i & \sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) y_i \\
\sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) y_i & \sum_{i=1}^n x_i^2 \exp(\beta_1 + \beta_2 x_i) y_i
\end{pmatrix}.\]
The Fisher information matrix is
\begin{align*}
{\cal I}({\bm \beta}) &= E[- H({\bm \beta})]  \\
&= \begin{pmatrix}
\sum_{i=1}^n \exp(\beta_1 + \beta_2 x_i) E(Y_i) & \sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) E(Y_i) \\
\sum_{i=1}^n x_i \exp(\beta_1 + \beta_2 x_i) E(Y_i) & \sum_{i=1}^n x_i^2 \exp(\beta_1 + \beta_2 x_i) E(Y_i)
\end{pmatrix} \\
&= \begin{pmatrix}
\sum_{i=1}^n 2 & \sum_{i=1}^n 2 x_i  \\
\sum_{i=1}^n 2 x_i & \sum_{i=1}^n 2 x_i^2 
\end{pmatrix}
\end{align*}
\item Model (2) is the special case of model (1)
with $\beta_2 = \beta_3 = 0$, so model (2) is nested
in model (1). So to compare the two models, use the null
hypothesis $H_0: \beta_2 = \beta_3 = 0$
against the alternative
$H_1: \text{"$\beta_2$ and $\beta_3$ unrestricted"}$.
\item We have $L_{01} = 11.0 - 5.0 = 6.0$.
Under $H_0$, $L_{01}$ has approximate $\chi^2_2$ distribution.
\eenum
\item
\benum
\item 
 Model 1 is nested in all the other models. To see why Model 1 is nested within Models 4 and 5, we can
rewrite Model 1 as $p_i = \Phi(\beta_1^*)$, where
\[\beta_1^* = \Phi^{-1}\left(\frac{\exp(\beta_1)}{1 + \exp(\beta_1)}\right).\]
 Model 2 is nested in Model 3.
 Model 4 is nested in Model 5.

So the full set of pairs $(j, k)$ such that Model $j$ is nested in 
Model $k$ is
\[\{(1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 3), (4, 5)\}.\]
\item
For comparing Model 1 and Model 3, we assume the larger model
(Model 3), and test the null
hypothesis $H_0: \beta_2 = \beta_3 = 0$
against the alternative
$H_1: \text{"$\beta_2$ and $\beta_3$ unrestricted"}$.
\item We have $L_{01} = 136.7 - 96.98 = 39.72$. 
Under $H_0$, $L_{01}$ has approximate $\chi^2_2$ distribution.
\item
\bitem
\item [A] is $100 - 1 = 99$.
\item [B] is $136.7$.
\item [C] is $100 - 3 = 97$.
\item [D] is $96.98$.
\item [E] is $99 - 97 = 2$.
\item [F] is $136.7 - 96.98 = 39.7$
\eitem
Since $39.7 > 5.99$, the $95\%$ point of a $\chi^2_2$
distribution, we reject $H_0$, and prefer Model 3 to 
Model 1.
\item To compare Model 2 and Model 3, we assume
the larger model (Model 3), and test $H_0: \beta_3 = 0$
against the alternative $H_1 : \text{``$\beta_3$ is unrestricted"}$.
We compute $L_{01} = 97.42 - 96.68 = 0.74$. Since $L_{01} < 3.84$,
the $95 \%$ point of a $\chi^2_1$ distribution, we do not reject
$H_0$, and prefer Model 2 to Model 3.
\item The Bernoulli distribution is a member of the exponential family
(see the notes for the proof of this). Let
${\bm x}_i = (1, x_i)^T$, and $\bm \beta = (\beta_1, \beta_2)^T$,
so we have a linear predictor $\eta_i = {\bm x}_i^T {\bm \beta}$.
We have $\mu_i = E(Y_i) = p_i$, so $\mu_i = \Phi(\eta_i)$, 
or $\Phi^{-1}(\mu_i) = \eta_i$. So Model 4 is a GLM, with link
function $g(\mu) = \Phi^{-1}(\mu)$.


\eenum
\eenum
