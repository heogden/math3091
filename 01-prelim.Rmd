# Preliminaries {#prelim}

## Elements of statistical modelling


Probability and statistics can be characterised as the study of variability.
In particular, statistical inference is the  science of analysing
statistical data, viewed as the outcome of some random process, in order to
draw conclusions about that random process.

Statistical models help us to *understand* the random process by which observed data
have been generated. This may be of interest in itself, but also allows us to make
*predictions* and perhaps most importantly *decisions* contingent on
our inferences concerning the process.

It is also important, as part of the modelling process, to acknowledge
that our conclusions are only based on a (potentially small) sample
of possible observations of the process and are therefore subject to error.
The science of statistical inference therefore involves assessment of the uncertainties
associated with the conclusions we draw.

Probability theory is the mathematics associated with randomness and
uncertainty. We usually try to describe random processes using probability
models. Then, statistical inference may involve estimating any unspecified
features of a model, comparing competing models, and assessing the
appropriateness of a model; all in the light of observed data.

In order to identify 'good' statistical models, we require some principles on which
to base our modelling procedures. In general, we have three requirements of a statistical model

* Plausibility
* Parsimony
* Goodness of fit

The first of these is not a statistical consideration, and
a subject-matter expert usually needs to be
consulted about this. For some objectives, like prediction, 
it might be considered unimportant. Parsimony and goodness of 
fit are statistical issues. Indeed,
there is usually a trade-off between the two and our statistical 
modelling strategies will take
account of this.


## Regression Models
Almost all statistical models, and all the ones we shall deal 
with in MATH3012, can 
be formulated as *regression* models.

In practical applications, we often distinguish between a *response*
variable and a group of *explanatory* variables. The aim is to determine
the pattern of dependence of the response variable on the explanatory
variables. General regression models often provide an attractive and 
convenient way to quantify the relationships between the two sets 
of variables.  A regression model has the general form

> response = function(structure and randomness)

The structural part of the model describes how the response depends 
on the explanatory variables and the random part defines the probability
distribution of the response. Together, they produce the response and the
statistical modeller's task is to 'separate' these out. 

## Example data to be analysed

### `nitric`: Nitric acid

This data set represents 21 successive days of operation of a plant
oxidising ammonia to nitric acid. The variables `x1`, `x2` and `x3`
are respectively, flow of air to the plant, temperature of the cooling water
entering the absorption tower, and concentration of nitric acid in the
absorbing liquid. The response `y` is ten times the percentage of ingoing
ammonia that is lost as unabsorbed nitric acid (an indirect measure of the
yield). These data will be analysed  in worksheet 2 using multiple linear 
regression models.   


### `birth`: Weight of newborn babies

This data set contains weights of 24 newborn babies. There are two explanatory 
variables,  sex and gestational age  in weeks together with the response
variable, birth weight in grams.  The aim here is to study how birth weight
depends on sex and  gestational age. This data set will be analysed  in
worksheet 3 by using the multiple linear regression models that can include
both categorical and continuous explanatory variables.  

```{r, root.dir = "datasets"}
birth <- read.table("datasets/birth.txt", header = TRUE)
head(birth)
plot(birth$Age, birth$Weight, xlab="Age (weeks)", ylab="Birthweight (g)",
     col = birth$Sex)
legend("bottomright", legend = c("Male", "Female"), pch = 1, col = 1:2)
```

### `survival`: Time to death

This data set, analysed in worksheet 4,  represents survival 
times (in 10 hour units) of 48 rats each allocated to one of 12 combinations
of  3 poisons and 4 treatments. The aim here is to study the analysis of
variance techniques with multiple factors. 

### `beetle`: Mortality from carbon disulphide

This data set represents the number of beetles exposed ($n$) and number 
killed ($y$) in eight groups exposed to different doses ($x$)
of a particular insecticide. Interest is focussed on how mortality is 
related to dose. It seems sensible to model the number of beetles 
killed in each group as the binomial random variable with 
probability of death depending on dose. 
This will be discussed in worksheet 5.

### `shuttle`: Challenger disaster

This data set concerns the 23 space shuttle flights
before the Challenger disaster.  The disaster is thought to have 
been caused by the failure of a number of O-rings, 
of which there are six in total. The data 
consist of four columns, the number $Y$ of
damaged O-rings for each
pre-Challenger flight, together with the launch temperature $T$ in degrees
Fahrenheit, the pressure $P$ at which the pre-launch test of O-ring leakage was
carried out and the name of the orbiter ($S$; coded $1=$ "Atlantis",
$2=$ "Challenger", $3=$ "Columbia", $4=$ "Discovery").
The Challenger launch
temperature on 20th January 1986 was 31F.
By fitting a **GLM** to this data, we can 
predict the probability of
O-ring damage at the Challenger launch. 
 This will be discussed in worksheet 6.


### `heart`: Treatment for heart attack

This data set represents the results of a clinical trial
to assess the effectiveness of a thrombolytic (clot-busting) treatment
for patients who have suffered an acute myocardial infarction (heart attack).
The data set provides the number of patients who did and did not survive
for 35 days. There are four categorical explanatory variables, representing

- the site of infarction, $S$: anterior, inferior or other
- time between infarction and treatment, $T$: $\le 12$ or $>12$ hours
- whether the patient was already taking Beta-blocker medication prior to the infarction,
$B$: yes or no
- the treatment the patient was given, $R$: active or placebo.

These data will be analysed in worksheet 7.

### `accident`: Road traffic accidents

This example concerns the number of road accidents and the 
volume of traffic observed on Mill Road and Trumptington 
Road in Cambridge during morning, midday and afternoon. 
By fitting a GLM, we should be able to answer questions like:  

1. Is Mill Road more dangerous than Trumpington Road?
1. How does time of day affect the rate of road accident? 

These issues will be considered in worksheet 8. 


### `lymph`: Lymphoma patients
The  `lymphoma` data set represents 30 lymphoma 
patients classified by sex, cell type of lymphoma and response to 
treatment and it is an example of a three-way ($2\times 2\times 2$)
contingency table. The aim here is to study the 
complex dependence structures between the three clasifying factors. 
This is taken up in worksheet 9. 

### `car`: Car insurance claims

The data set `car` is based on  one-year vehicle insurance policies 
taken out in 2004-2005. There are 67,856 policies, of which 4624 (6.8\%) 
had at least one claim. We shall model the probability of at 
least one claim based on the amount of car usage, the gender, age and the 
area of residence of the driver, and other factors such as the car body type.
We should be able to answer questions like 

1. how much more likely the younger drivers are to make claim than the older 
  drivers? 
1. Does the area of residence significantly affect the  probability of a claim?
1. Are female drivers better than male drivers?

### `credit`: Credit scoring

We shall also analyse a  `credit` scoring data set where the response 
variable takes two values: 1 for credit-worthy and 0 for not. The modelling
problem here is how the probability of credit-worthiness can be related to
demographic variables such as age, sex, marital status, occupation and many
other relevant factors. 


## Likelihood based statistical theory

### The likelihood function

Probability distributions like the binomial,  Poisson  and normal,
enable us to calculate probabilities, and other quantities of interest 
(e.g. expectations) for a probability model of a random process.
Therefore, given the model, we can make statements about possible
outcomes of the process.

Statistical inference is concerned with the inverse problem.
Given outcomes of a random process (observed data), what conclusions
(inferences) can we draw about the process itself?

We assume that the $n$ observations of the response
$\bm y=(y_1,\ldots ,y_n)^T$ are observations of random variables
$\bm Y=(Y_1,\ldots ,Y_n)^T$, which have joint p.d.f. $f_{\bm Y}$ (joint
p.f. for discrete variables).
We use the observed data $\bm y$ to make inferences about $f_{\bm Y}$.

We usually make certain assumptions about $f_{\bm Y}$. In particular,
we often assume that $y_1, \ldots, y_n$ are observations of
*independent* random variables.
Hence
$$
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
$$


In parametric statistical inference, we specify a joint
distribution $f_{\bm Y}$, for $\bm Y$, which is known, except for the values of
parameters $\theta_1,\theta_2,\ldots ,\theta_p$ (sometimes denoted by $\bm \theta$).
Then we use the observed data $\bm y$ to make inferences about
$\theta_1,\theta_2,\ldots ,\theta_p$. In this case, we usually write $f_{\bm Y}$ as
$f_{\bm Y}(\bm y;\bm \theta)$, to make explicit the dependence on the unknown
$\bm \theta$.

An important concept for parametric statistical inference,
particularly for complex statistical models is the *likelihood*.

Until now, we have thought of the joint density $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm{y}$ for fixed $\bm \theta$, which describes the relative probabilities of different
possible (sets of) $\bm y$, given a particular set of parameters $\bm \theta$.
However, in statistical inference, we have observed $y_1, \ldots, y_n$ (values of $Y_1, \ldots, Y_n$).
Knowledge of the probability of alternative possible realisations of $\bm Y$
is largely irrelevant.
What we want to know about is $\bm \theta$.

Our only link between the observed data $y_1, \ldots, y_n$ and
$\bm \theta$ is through the function $f_{\bm Y}(\bm y;\bm \theta)$. 
Therefore, it seems sensible
that parametric statistical inference should be based on this function.
We can think of $L(\theta)$ as a function
of $\bm \theta$ for fixed $\bm{y}$, which describes the relative *likelihoods* of
different possible (sets of) $\bm \theta,$ given observed data $y_1, \ldots, y_n$.
We write $L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)$ for this *likelihood*,
which is a function of the unknown parameter $\bm \theta$. For convenience, we often
drop $\bm y$ from the notation, and write $L(\bm \theta)$.

The likelihood function is of central importance in parametric statistical
inference.
It provides a means for comparing different possible values of $\bm \theta$, based on
the probabilities (densities) that they assign to the observed data $y_1, \ldots, y_n$.

1. Frequently it is more convenient to consider the log-likelihood function
  $\log L(\bm \theta)$. This is often denoted by $\ell(\bm \theta)$.
1. Nothing in the definition of the likelihood requires $y_1, \ldots, y_n$ to be
  observations of independent random variables, although we shall frequently
  make this assumption.
1. Any factors which depend on $y_1, \ldots, y_n$ alone (and not on $\bm \theta$) can be ignored
  when writing down the likelihood. Such factors give no information about the   relative likelihoods of different possible values of $\bm \theta$.

#### Bernoulli example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, independent identically distributed
(i.i.d.) Bernoulli$(p)$ random
variables. Here $\theta=(p)$ and
$$
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}
$$
$$
\log L(p)=n\bar y\log p+n(1-\bar y)\log(1-p).
$$


We can plot the log-likelihood function in R
```{r, bernoulli_loglikelihood}
loglikelihood <- function(p, y) {
  y_bar <- mean(y)
  n <- length(y)
  n * y_bar * log(p) + n* (1 - y_bar) * log(1 - p)
} 

y <- c(1, 0, 1, 0, 0, 1, 0, 0, 0, 0)
curve(loglikelihood(x, y), from = 0, to = 1, 
      xlab = "p", ylab = "loglikelihood")
```


#### Normal example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
L(\mu,\sigma^2) &=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
\end{align*}
so
\[\log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]


### Maximum  likelihood estimation 

One of the primary tasks of parametric statistical inference is
*estimation* of the unknown parameters $\theta_1, \ldots, \theta_p$.
Consider the value of $\bm \theta$, which maximises the
likelihood function. This is the 'most likely' value of $\bm \theta$, the one which makes
the observed data 'most probable'. When we are searching for an estimate of
$\bm \theta$, this would seem to be a good candidate.

We call the value of $\bm \theta$ which maximises the likelihood $L(\theta)$
the *maximum likelihood estimate* (MLE) of $\bm \theta$, denoted by
$\hat{\bm \theta}$.
$\hat{\bm \theta}$ depends on $\bm y$, as different observed data samples
lead to different likelihood functions.
The corresponding function of $\bm Y$ is called the
*maximum likelihood estimator*  and is also denoted by $\hat{\bm \theta}$.

Note that as $\bm \theta=(\theta_1, \ldots, \theta_p)$, the MLE for any component
of $\bm \theta$ is given by the corresponding component of
$\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T$.
Similarly, the MLE for any function of parameters $g(\bm \theta)$ is given
by $g(\hat{\bm \theta})$.

As $\log$ is a strictly increasing
function, the value of $\bm \theta$ which maximises 
$L(\bm \theta)$ also maximises $\ell(\bm \theta) = \log L (\bm \theta)$.
It is almost always easier to maximise $\ell(\bm \theta)$.
This is achieved in the usual way; finding a stationary point by differentiating
$\ell(\bm \theta)$ with respect to $\theta_1, \ldots, \theta_p$, and solving the resulting $p$
simultaneous equations. It should also be checked that the stationary point is a
maximum.

#### Bernoulli example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
Bernoulli$(p)$ random variables. Here $\bm \theta=(p)$ and
the log-likelihood is
\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]
  Differentiating with respect to $p$,
  \[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]
    so the MLE $\hat p$ solves
    \[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]
  Solving this for $\hat{p}$ gives $\hat{p}=\bar y$.
  Note that
  $\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}<0$ 
   everywhere, so the stationary point is clearly a maximum.

#### Normal example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
 and the log-likelihood is
\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]
   Differentiating with respect to $\mu$
\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]
  so $(\hat \mu, \hat \sigma^2)$ solve
\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  (\#eq:normalScoreMu)
\end{equation}
  Differentiating with respect to $\sigma^2$
\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]
so
\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  (\#eq:normalScoreSs)
\end{equation}
Solving \@ref(eq:normalScoreMu) and \@ref(eq:normalScoreSs), we obtain
$\hat{\mu}=  \bar y$ and
\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]


Strictly,
to show that this stationary point is a maximum, we need to show that the
Hessian matrix (the matrix of second derivatives with elements
$[\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)$)
is negative definite at $\bm \theta=\hat{\bm \theta}$, that is $\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}<0$ for
every
$\bm{a}\ne {\bf 0}$.
Here
$$
\bm{H}(\hat{\mu},\hat \sigma^2)= \left( \begin{array}{cc}
- \frac{n}{\hat \sigma^2 } & 0 \cr
0   &-\frac{n}{2(\hat \sigma^2)^2} \end{array} \right)
$$
which is clearly negative definite.

### Score {#score}

Suppose that $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, whose joint p.d.f.
$L(\theta)$ is completely specified except
for the values of $p$ unknown parameters $\bm \theta=(\theta_1, \ldots, \theta_p)^T$.
Let
$$
u_i(\bm \theta)\equiv{{\partial}\over{\partial\theta_i}} \ell(\theta)\qquad i=1,\ldots ,p
$$
and $\bm{u}(\bm \theta)\equiv[u_1(\bm \theta),\ldots ,u_p(\bm \theta)]^T$. Then we call $\bm{u}(\bm \theta)$ the
*vector of scores* or *score vector*.
Where $p=1$ and $\bm \theta=(\theta)$, the *score* is the scalar defined as
$$
u(\theta)\equiv{{\partial}\over{\partial\theta}}\log L(\theta).
$$
The maximum likelihood estimate $\hat{\bm \theta}$ satisfies
\[u(\hat{\bm \theta})={\bm 0},\]
that is,
\[u_i(\hat{\bm \theta})=0\quad i=1,\ldots ,p.\]
Note that $u(\bm{\theta})$ is a function of $\bm \theta$ for fixed (observed) $\bm y$.
However, if we replace $y_1, \ldots, y_n$ in $u(\bm{\theta})$, by the corresponding random variables
$Y_1, \ldots, Y_n$ then we obtain a vector of random variables $U(\bm{\theta})\equiv
[U_1(\bm \theta),\ldots ,U_p(\bm \theta)]^T$.

An important result in likelihood theory is that the expected score
at the true (but unknown) value of $\bm \theta$ is zero, *i.e.*
\[E[U(\bm{\theta})]={\bf 0}\]
or
\[E[U_i(\bm \theta)]= 0,
\quad i=1,\ldots ,p,\]
provided that

1. The expectation exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

#### Proof (continuous $\bm y$ -- in discrete case replace $\int$ by $\sum$) {-}
For each $i=1, \ldots, n$
\begin{align*}
E[U_i(\bm \theta)]&=\int U_i(\bm \theta)f_{\bm Y}(\bm y, \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}}\int f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}} 1 =0.
\end{align*}

#### Bernoulli example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\[u(p)=n\bar y/ p-n(1-\bar y)/(1-p).\]
Since $E[U(p)] = 0$, we must have $E[\bar Y]=p$ (which we already
know is correct).

#### Normal example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
$N(\mu,\sigma^2)$ random variables. Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2)&= {{n(\bar y-\mu)}/{\sigma^2}}\cr
u_2(\mu,\sigma^2)&= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2
\end{align*}
Since $E[\bm U(\mu,\sigma^2)] = {\bm 0}$, we must have
$E[\bar Y]=\mu$ and 
$E[{\textstyle{1\over n}}\sum(Y_i-\mu)^2]=\sigma^2.$

### Information {#info}

Suppose that $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, whose joint p.d.f.
$L(\theta)$ is completely specified except
for the values of $p$ unknown parameters $\bm \theta=(\theta_1, \ldots, \theta_p)^T$.
Previously, we defined the Hessian matrix $H(\bm{\theta})$ to be the matrix with
components
$$
[H(\bm{\theta})]_{ij}\equiv{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$
We call the matrix $-H(\bm{\theta})$ the *observed information matrix*.
Where $p=1$ and $\bm \theta=(\theta)$, the *observed information* is a
scalar defined as
$$
-H(\theta)\equiv-{{\partial}\over{\partial\theta^2}}\log L(\theta).
$$

<!-- Here, we are interpreting $\bm \theta$ as the true (but unknown) value of -->
<!-- the parameter. -->
As with the score, if we replace $y_1, \ldots, y_n$ in $H(\bm{\theta})$, by the
corresponding random variables
$Y_1, \ldots, Y_n$, we obtain a matrix of random variables.
Then, we define the *expected information matrix* or
*Fisher information matrix*
$$
[\mathcal{I}(\bm \theta)]_{ij}=E(-[H(\bm{\theta})]_{ij})
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$

An important result in likelihood theory is that the variance-covariance matrix
of the score vector is equal to the
expected information matrix *i.e.*
\[\text{Var}[U(\bm{\theta})]=\mathcal{I}(\bm \theta)\]
or
\[\text{Var}[U(\bm{\theta})]_{ij}= [\mathcal{I}(\bm \theta)]_{ij}, 
\quad i=1,\ldots ,p, \quad j=1,\ldots ,p\]
provided that

1. The variance exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

#### Proof  (continuous $\bm y$ -- in discrete case replace $\int$ by $\sum$) {-}

For each $i = 1,\ldots, p$ and $j = 1, \ldots, p$,
\begin{align*}
\text{Var}[U(\bm{\theta})]_{ij}&= E[U_i(\bm \theta)U_j(\bm \theta)]\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta)
{{\partial}\over{\partial\theta_j}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)}
{{{{\partial}\over{\partial\theta_j}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta)d\bm y\cr
&= \int \frac{1}{L(\theta)}{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)
 {{\partial}\over{\partial\theta_j}}  f_{\bm Y}(\bm y; \bm \theta)  d\bm y \cr
\end{align*}

Now
\begin{align*}
[\mathcal{I}(\bm \theta)]_{ij}&=E\left[-{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)\right]\cr
&=\int -{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta)  f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int -{{\partial}\over{\partial\theta_i}}\left[
{{{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}\right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int \left[
-{{{{\partial^2}\over{\partial\theta_i\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}
+ {{{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)^2} \right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= -{{\partial^2}\over{\partial\theta_i\partial\theta_j}}\int  f_{\bm Y}(\bm y; \bm \theta) d\bm y
+ \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)  d\bm y\cr
&= \text{Var}[U(\bm{\theta})]_{ij}
\end{align*}

#### Bernoulli example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\begin{align*}
u(p)&= {{n\bar y}\over{ p}}-{{n(1-\bar y)}\over {(1-p)}}\cr
-H(p)&= {{n\bar y}\over{ p^2}}+{{n(1-\bar y)}\over {(1-p)^2}}\cr
{\cal I}(p)&= {{n}\over{ p}}+{{n}\over {(1-p)}}={{n}\over {p(1-p)}}.
\end{align*}

#### Normal example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and,
\begin{align*}
u_1(\mu,\sigma^2) &=  \frac{n(\bar y-\mu)}{\sigma^2} \\
u_2(\mu,\sigma^2) &= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2
\end{align*}
Therefore
$$
-\bm{H}(\mu,\sigma^2) = \left( \begin{array}{cc}
\frac{n}{\sigma^2} & \frac{n(\bar y-\mu)}{(\sigma^2)^2} \cr
\frac{n(\bar y-\mu)}{(\sigma^2)^2}&
 \frac{1}{(\sigma^2)^3} \sum(y_i-\mu)^2- \frac{n}{2(\sigma^2)^2}
\end{array}
\right)
$$
$$
{\cal I}(\mu,\sigma^2)= \left( \begin{array}{cc}
\frac{n}{\sigma^2} & 0 \cr
0& \frac{n}{2(\sigma^2)^2}
\end{array} \right).
$$

### Asymptotic distribution of the MLE {#sn:asnmle}

Maximum likelihood estimation is an attractive method of estimation
for a number of reasons.
It is intuitively sensible (choosing $\bm \theta$ which makes the observed data
most probable) and usually reasonably straightforward to carry out.
Even when the simultaneous equations we obtain by
differentiating the log likelihood function are impossible to solve directly,
solution by numerical methods is usually feasible.

Perhaps the most compelling reason for considering maximum likelihood
estimation is the asymptotic behaviour of maximum likelihood estimators.

Suppose that $y_1, \ldots, y_n$ are observations of  independent random variables $Y_1, \ldots, Y_n$,
whose joint p.d.f.  $f_{\bm Y}(\bm y;\bm \theta)=\prod_{i=1}^n f_{Y_i}(y_i;\bm \theta)$ is
completely specified except for the values of an unknown parameter vector
$\bm \theta$, and that
$\hat{\bm \theta}$ is the maximum likelihood estimator of $\bm \theta$.

Then, as $n\to\infty$, the distribution of $\hat{\bm \theta}$ tends to a multivariate
normal distribution with mean vector $\bm \theta$ and variance covariance matrix
$\mathcal{I}(\bm \theta)^{-1}$.

Where $p=1$ and $\bm \theta=(\theta)$, the distribution of the MLE $\hat{\theta}$
tends to $N[\theta,1/{\cal I}(\theta)]$.

For 'large enough $n$', we can treat the asymptotic distribution of the MLE as
an approximation. The fact that $E(\hat{\bm \theta})\approx\bm \theta$ means that the maximum
likelihood estimator is approximately *unbiased*
for large samples. Furthermore, its variability, as measured by its
variance ${\cal I}(\bm \theta)^{-1}$ is the smallest possible amongst unbiased
estimators (the CramÃ©r-Rao lower bound), so the maximum likelihood has good precision.
Therefore the MLE is a desirable estimator in large samples (and therefore we hope also reasonable in small samples, though we should investigate this case by case).


The usefulness of an estimate is always enhanced
if some kind of measure of its precision can also be provided.
Usually, this will be a *standard error*, an estimate of
the standard deviation of the associated estimator.
For the maximum likelihood estimator $\hat{\theta}$, a standard error is
given by
$$
s.e.(\hat{\theta})={1\over{{\cal I}(\hat{\theta})^{{1\over 2}}}},
$$
and for a vector parameter $\bm \theta$
$$
s.e.(\hat{\theta}_i)=[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{{1\over 2}}
\qquad i=1,\ldots ,p.
$$

An alternative summary of the information provided by the observed data about the
location of a parameter $\theta$ and the associated precision is
an *interval estimate* or *confidence interval*.

The asymptotic distribution of the maximum likelihood estimator can be used to
provide approximate large sample confidence intervals. Asymptotically,
$\hat{\theta}_i$ has a
$N(\theta_i,[\mathcal{I}(\bm \theta)^{-1}]_{ii})$ distribution and we can find $z_{1-\frac{\alpha}{2}}$ such that
$$
P\left(z_{1-\frac{\alpha}{2}}\le {{\hat{\theta}_i-\theta_i}\over{[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) = 1- \alpha.
$$
Therefore
$$
P\left(\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}\le\theta_i
\le\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}
\right) = 1- \alpha.
$$
The endpoints of this interval cannot be evaluated
because they also depend on the unknown parameter vector $\bm \theta$.
However, if we replace $\mathcal{I}(\bm \theta)$ by its MLE ${\cal I}(\hat{\bm \theta})$
we obtain the approximate large sample 100$\alpha$\% confidence interval
$$
[\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2},
\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2}].
$$
For $\alpha=0.9,0.95,0.99$, $z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58$.

#### Bernoulli example {-}
If $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables then asymptotically $\hat{p}=\bar y$ has a  $N(p,{p(1-p)}/ n)$
distribution, and a large sample 95\% confidence interval
for $p$ is
\begin{align*}
& [\hat{p}- 1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2},
\hat{p}+1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2}]
\cr
&=
[\hat{p}-1.96[\hat{p}(1-\hat{p})/n]^{1\over 2},
\hat{p}+1.96[\hat{p}(1-\hat{p})/n]^{1\over 2}]\cr
&=
[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}].
\end{align*}


### Comparing statistical models

If we have a set of competing probability models
which might have generated the observed data, we may want to determine which of
the models is most appropriate.
In practice, we proceed by comparing models pairwise.
Suppose that we have two competing alternatives, $f^{(0)}_{\bm Y}$ (model
$H_0$) and $f^{(1)}_{\bm Y}$  (model $H_1$) for $f_{\bm Y}$,
the joint distribution of $Y_1, \ldots, Y_n$.
The most common situation is where $H_0$ and $H_1$
both take the same parametric form, $f_{\bm Y}(\bm{y};\bm \theta)$ but
with $\bm \theta\in\Theta^{(0)}$ for $H_0$ and $\bm \theta\in\Theta^{(1)}$ for $H_1$,
where $\Theta^{(0)}$ and $\Theta^{(1)}$ are alternative sets of possible values
for $\bm \theta$.

A hypothesis test provides a mechanism for comparing the two competing statistical
models, $H_0$ and $H_1$.
A hypothesis test does not treat the two hypotheses (models) symmetrically. One
hypothesis, $H_0$, is accorded special status, and referred to as the 
*null hypothesis*. The null hypothesis is the reference model, and will be assumed to
be appropriate unless the observed data strongly indicate that $H_0$ is
inappropriate, and that $H_1$ (the *alternative* hypothesis) should be
preferred.

Hence, the fact that a hypothesis test does not reject $H_0$ should not be taken
as evidence that $H_0$ is true and $H_1$ is not, or that $H_0$ is  better
supported by the data than $H_1$, merely that the data does not provide
sufficient evidence to reject $H_0$ in favour of $H_1$.


A hypothesis test is defined by its *critical region* or 
*rejection region*, which we shall denote by $C$. $C$ is a subset of $\mathbb{R}^n$ and is the set
of possible $\bm{y}$ which would lead to rejection
of $H_0$ in favour of $H_1$, *i.e.*

- If $\bm{y} \in C$, $H_0$ is rejected in favour of $H_1$;
- If $\bm{y} \not\in C$, $H_0$ is not rejected.

As $\bm Y$ is a random variable, there remains the possibility that a hypothesis
test will produce an erroneous result.
We define
\begin{align*}
\alpha &= \max_{\bm \theta\in\Theta^{(0)}}P(\bm Y\in C;\bm \theta) \\
\omega(\bm \theta)&= P(\bm Y\in C;\bm \theta)
\end{align*}
We call $\alpha$  the *size* (or *significance level*) of the
test; it is the maximum probability of erroneously rejecting $H_0$, over all
possible distributions for $\bm Y$ implied by $H_0$.
The function $\omega(\bm \theta)$ is  called the
*power function*. It represents the probability of rejecting $H_0$
for a particular value of $\bm \theta$.
Clearly we would like to find a test with where
$\omega(\bm \theta)$ is large for every $\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}$, 
while at the same time avoiding erroneous rejection of $H_0$.
In other words, a good test will
have small size, but large power.

The general hypothesis testing procedure is to fix $\alpha$ to be some small
value (often 0.05), so that the probability of erroneous rejection of $H_0$ is
limited. In doing this, we are giving $H_0$ precedence over $H_1$.
Given our specified $\alpha$, we try to choose a test, defined by its
rejection region $C$, to make $\omega(\bm \theta)$ as large  as possible for
$\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}$.


Suppose that $H_0$ and $H_1$ both take the same parametric form,
$f_{\bm Y}(\bm{y};\bm \theta)$ with $\bm \theta\in\Theta^{(0)}$ for $H_0$ and $\bm \theta\in\Theta^{(1)}$
for $H_1$, where $\Theta^{(0)}$ and $\Theta^{(1)}$ are alternative sets of
possible values for $\bm \theta$.
A *generalised likelihood ratio test* of $H_0$ against $H_1$ has a critical region
of the form
\[C=\left\{ \bm{y}: 
\frac{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)} 
>k\right\}\]
where $k$ is determined by $\alpha$, the size of the test, so
\[\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.\]
Therefore, we will only reject
$H_0$ if $H_1$ offers a distribution for $Y_1, \ldots, Y_n$ which makes the observed data
much more probable than any distribution under $H_0$.
This is intuitively appealing and  tends to produce good tests (large power)
across a wide range of examples.

#### Bernoulli example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Suppose that we require a size $\alpha$ test of the hypothesis
$H_0$: $p=p_0$ against the general alternative $H_1$: '$p$ is unrestricted' where
$\alpha$ and
$p_0$ are specified.

Here $\bm \theta=(p)$, $\Theta^{(0)}=\{p_0\}$ and $\Theta^{(1)}=(0,1)$ and the
generalised likelihood ratio test rejects $H_0$ when
\[{{\max_{p\in(0,1)}L(p)}\over{\max_{p=p_0}L(p)}} > k\]
or equivalently when
\[{{\bar y^{\sum_i y_i}(1-\bar y)^{n-\sum_i y_i}}\over {p_0^{\sum_i y_i}(1-p_0)^{n-\sum_i y_i}}} > k\]
or
\begin{equation}
\left({{\bar y}\over{p_0}}\right)^{n\bar y}
\left({{1-\bar y}\over{1-p_0}}\right)^{n(1-\bar y)} >k. 
  (\#eq:her1)
\end{equation}
Now the left hand side of \@ref(eq:her1) is minimised as a function of $\bar y$
at $\bar y=p_0$ and increases as
$\bar y$ moves away from $p_0$ in either direction.
Therefore, the rejection region \@ref(eq:her1) is equivalent to
$$
C=\left\{ \bm{y}:\bar y > k' \text{ or } \bar y < k''\right\}
$$
where $k'$ and $k''$ are chosen so that
$$
P(\bm{y}\in C;p_0)=\alpha.
$$
Therefore, we can use the binomial$(n,p_0)$ distribution to find a precise rejection
region for a test of specified size $\alpha$.

Alternatively, if $n$ is large, we can use the asymptotic distribution of
$\bar y$, $N(p_0,p_0[1-p_0]/n)$.


### The log-likelihood ratio statistic {#sn:lrt}

A *generalised likelihood ratio test* of $H_0$ against $H_1$ has a critical region
of the form
$$
C=\left\{ \bm{y}:{{\max_{\bm \theta\in \Theta^{(1)}} L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}} >k\right\}
$$
where $k$ is determined by $\alpha$, the size of the test, so
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
$$

Therefore, in order to determine $k$, we need
to know the distribution of the likelihood ratio, or an equivalent statistic,
under $H_0$.
In general, this will not be available to us.
However, we can make use of an important asymptotic result.


First we notice that, as $\log$ is a strictly increasing function, the rejection
region is equivalent to

$$
C=\left\{ \bm{y}: 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right) >k'\right\}
$$
where
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
$$
Now, provided that $H_0$ is *nested within* $H_1$, in other words
$\Theta^{(0)}\subset\Theta^{(1)}$ ($\Theta^{(0)}$ is a subspace of
$\Theta^{(1)}$) then under
$H_0$: $\bm \theta\in\Theta^{(0)}$, asymptotically as
$n\to\infty$
$$
L_{01}\equiv 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)
$$
has a chi-squared distribution with degrees of freedom equal to the difference in
the dimensions of $\Theta^{(1)}$ and $\Theta^{(0)}$.

#### Proof {-}
First we note that in the case where $\bm \theta$ is one-dimensional and $\bm \theta=(\theta)$,
a Taylor series expansion of $\ell(\theta)$  around the
MLE $\hat{\theta}$ gives
$$
\ell(\theta)=\ell(\hat{\theta})+(\theta-\hat{\theta})
U(\hat{\theta})+{1\over 2}(\theta-\hat{\theta})^2 U'(\hat{\theta}) +\;\ldots
$$
Now, $U(\hat{\theta})=0$, and if we approximate $U'(\hat{\theta})\equiv
H(\hat{\theta})$ by $E[H(\theta)]\equiv -{\cal I}(\theta)$, and also ignore higher order
terms, we obtain
$$
2[\ell (\hat{\theta})-\ell(\theta)]=
(\theta-\hat{\theta})^2 {\cal I}(\theta)
$$
As $\hat{\theta}$ is asymptotically
$N[\theta,{\cal I}(\theta)^{-1}]$, $(\theta-\hat{\theta})^2 {\cal I}(\theta)$ is
asymptotically $\chi^2_1$, and hence so is $2[\ell(\hat \theta)-\ell (\theta)]$.

Similarly it can be shown that when $\bm \theta\in\Theta$, a multidimensional space,
$2[\ell(\bm{\hat \theta})-\ell (\bm \theta)]$ is asymptotically
$\chi^2_p$, where $p$ is the dimension of $\Theta$.

Let the dimensions of $\Theta^{(0)}$ and $\Theta^{(1)}$ be $d_0$ and $d_1$ respectively. Now, suppose that $H_0$ is true and  $\bm \theta\in\Theta^{(0)}$ and therefore
$\bm \theta\in\Theta^{(1)}$. Furthermore, suppose that $\ell(\bm \theta)$
is maximised in $\Theta^{(0)}$ by $\hat{\bm \theta}^{(0)}$ and is maximised
in $\Theta^{(1)}$ by $\hat{\bm \theta}^{(1)}$. Then
\begin{align*}
L_{01}&\equiv  2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)\cr
&= 2\log L\hat{\bm \theta}^{(1)})-2\log L\hat{\bm \theta}^{(0)})\cr
&= 2[\log L(\hat{\bm \theta}^{(1)})-\log L(\bm \theta)]
 -2[\log L(\hat{\bm \theta}^{(0)})-\log L(\bm \theta)]\cr
&= L_1-L_0.
\end{align*}

Therefore $L_1=L_{01}+L_0$ and we know that, under $H_0$, $L_1$ has
a $\chi^2_{d_1}$ distribution and $L_0$ has
a $\chi^2_{d_0}$ distribution.
Furthermore, it is possible to show (although we will not do so here)
that under $H_0$, $L_{01}$ and $L_0$ are independent.
It can also be shown that under $H_0$  the difference $ L_1-L_0$
can be expressed as a quadratic form of normal random variables.
Therefore, it follows that under $H_0$, the log likelihood ratio
statistic $L_{01}$ has a $\chi^2_{d_1-d_0}$ distribution.

#### Bernoulli example {-}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Suppose that we require a size $\alpha$ test of the hypothesis
$H_0$: $p=p_0$ against the general alternative $H_1$: '$p$ is unrestricted' where
$\alpha$ and
$p_0$ are specified.

Here $\bm \theta=(p)$, $\Theta^{(0)}=\{p_0\}$ and $\Theta^{(1)}=(0,1)$ and the
log likelihood ratio statistic is
$$
L_{01}=2n\bar y\log\left({{\bar y}\over{p_0}}\right)
+2n(1-\bar y)\log\left({{1-\bar y}\over{1-p_0}}\right).
$$
As $d_1=1$ and $d_0=0$, under $H_0$, the log-likelihood ratio statistic
has an asymptotic $\chi^2_1$ distribution.
For a log likelihood ratio test, we only reject $H_0$ in favour of $H_1$ when the test
statistic is too large (observed data are much more probable under model $H_1$ than
under model $H_0$), so in this case we reject $H_0$ when the observed value
of the test statistic above is 'too large' to have come from a
$\chi^2_1$ distribution.
What we mean by 'too large' depends on the significance level $\alpha$ of the
test. For example, if $\alpha=0.05$, a common choice, then we should reject $H_0$
if the test statistic is greater than the 3.84, the 95\% point of the
$\chi^2_1$ distribution.

<!-- \begin{figure}[hbt] -->
<!-- \begin{center} -->
<!-- \includegraphics[height=3in]{chsq1} -->
<!-- \end{center} -->
<!-- \caption{\protect\small\baselineskip .4 true cm   -->
<!-- The $\chi^2_1$ distribution } -->
<!-- \label{fig:chsq1} -->
<!-- \end{figure} -->

<!-- \end{exampl} -->


## Linear Models (a brief revision) {#sn:lm}

### Introduction

In practical applications, we often distinguish between a *response* variable and a group of
*explanatory* variables.
The aim is to determine the pattern of dependence of the response variable on the explanatory
variables.
We denote the $n$ observations of the response variable by $\bm{y}=(y_1,y_2,\ldots ,y_n)^T$.
In a statistical model, these are assumed to be observations of *random variables*
$\bm Y=(Y_1,Y_2,\ldots ,Y_n)^T$.
Associated with each $y_i$ is a vector $\bm{X}_i=(x_{i1},x_{i2},\ldots ,x_{ip})^T$ of values of $p$
explanatory variables.

Linear models are those for which the relationship between the response and explanatory
variables is of the form
\begin{align}
E(Y_i)&= \beta_1 x_{i1} +\beta_2 x_{i2} +\ldots + \beta_p x_{ip}\cr
&= \sum_{j=1}^p x_{ij} \beta_j\cr
&=  \bm{X}_i^T\bm{\beta}\cr
&= [\bm{X}\bm{\beta}]_i,\qquad i=1,\ldots ,n \hspace{2cm}   (\#eq:lmNonMat)
\end{align}
where
$$
E(\bm Y)=\begin{pmatrix}E(Y_1)\cr\vdots\cr E(Y_n) \end{pmatrix} \qquad
\bm{X}=\left( \begin{array}{c}
\bm{X}_1^T \cr
\vdots
\cr \bm{X}_n^T \\
\end{array} \right)
=\left( \begin{array}{ccc}
x_{11} & \cdots &x_{1p}\cr
\vdots & \ddots &\vdots\cr
x_{n1} &\cdots &x_{np}
\end{array} \right)
$$
and
$\bm{\beta}=(\beta_1,\ldots ,\beta_p)^T$ is a vector of fixed but unknown parameters
describing the dependence of $Y_i$ on $\bm{X}_i$.
The four ways of describing the linear model in \@ref(eq:lmNonMat) are equivalent, but the most
economical is the matrix form
\begin{equation}
E(\bm Y)=\bm{X}\bm{\beta}. (\#eq:lmMat)
\end{equation}

The $n\times p$ matrix $\bm{X}$ consists of known (observed) constants
and is called the *design matrix*. The $i$th row of $\bm{X}$ is $\bm{X}_i^T$, the
explanatory data corresponding to the $i$th observation of the response.
The $j$th column of $\bm{X}$ contains the $n$ observations of the $j$th explanatory variable.

#### Example: The null model {-}
 The null model
$$
E(Y_i)=\beta_1\qquad i = 1, \ldots, n
$$
$$
\bm{X}=\left( \begin{array}{c}
1\cr
1\cr
\vdots
\cr 1
\end{array} \right)
\qquad
\bm{\beta}=(\beta_1).
$$
One (dummy) explanatory variable. In practice, this variable is present
in all models.

#### Example: simple linear regression {-}
 Simple linear regression
$$
E(Y_i)=\beta_1+\beta_2 x_i\qquad i = 1, \ldots, n
$$
$$
\bm{X}=\left( \begin{array}{cc} 1&x_1\cr 1&x_2\cr \vdots&\vdots\cr 1&x_n
\end{array} \right)
\qquad \bm{\beta}=\left( \begin{array}{c} \beta_1\cr\beta_2 \end{array} \right)
$$
Two explanatory variables; the dummy variable
and one 'real' variable.

#### Example: Polynomial regression {-}
$$
E(Y_i)=\beta_1+\beta_2 x_i+\beta_3 x_i^2 +\ldots +\beta_p x_i^{p-1}\qquad i = 1, \ldots, n
$$
$$
\bm{X}=\left( \begin{array}{ccccc} 1&x_1&x_1^2&\cdots&x_1^{p-1}\cr
1&x_2&x_2^2&\cdots&x_2^{p-1}\cr
\vdots&\vdots&\vdots&\ddots&\vdots\cr
1&x_n&x_n^2&\cdots&x_n^{p-1} \end{array} \right)
\qquad \bm{\beta}=\left( \begin{array}{c} \beta_1\cr\beta_2\cr\vdots\cr\beta_p \end{array} \right)
$$
$p$ explanatory variables; the dummy variable
and one 'real' variable, transformed to $p-1$ variables.

#### Example: Multiple regression {-}
$$
E(Y_i)=\beta_1+\beta_2 x_{i1}+\beta_3 x_{i2} +\ldots +\beta_p x_{i\,p-1}\qquad i = 1, \ldots, n
$$
$$
\bm{X}=\left( \begin{array}{ccccc} 1&x_{11}&x_{12}&\cdots&x_{1\,p-1}\cr
1&x_{21}&x_{22}&\cdots&x_{2\,p-1}\cr
\vdots&\vdots&\vdots&\ddots&\vdots\cr
1&x_{n1}&x_{n2}&\cdots&x_{n\,p-1} \end{array} \right)
\qquad \bm{\beta}=\left( \begin{array}{c} \beta_1\cr\beta_2\cr\vdots\cr\beta_p \end{array} \right)
$$
$p$ explanatory variables; the dummy variable
and $p-1$ 'real' variables.

Strictly, the only requirement for a model to be linear is that the relationship
between the response variables, $\bm Y$, and any explanatory variables
can be written in the form \@ref(eq:lmMat).
No further specification of the joint distribution of
$Y_1, \ldots, Y_n$ is required.
However, the linear model is more useful for statistical analysis
if we can make three further assumptions:

- $Y_1, \ldots, Y_n$ are independent random variables.
- $Y_1, \ldots, Y_n$ are normally distributed.
- $\text{Var}(Y_1)=\text{Var}(Y_2)=\cdots =\text{Var}(Y_n)$ ($Y_1, \ldots, Y_n$ are homoscedastic).

We denote this common variance by $\sigma^2$.

With these assumptions the linear model completely specifies the distribution
of $\bm Y$, in that $Y_1, \ldots, Y_n$ are independent and
$$
Y_i\sim N\left(\bm{X}_i^T\bm{\beta}\; ,\;\sigma^2\right)\qquad i = 1, \ldots, n .
$$
Another way of writing this is
$$
Y_i=\bm{X}_i^T\bm{\beta}\;+\;\epsilon_i\qquad i = 1, \ldots, n
$$
where $\epsilon_1, \ldots, \epsilon_n$ are i.i.d. $N(0,\sigma^2)$ random variables.

A linear model can now be expressed in matrix form as
\begin{equation}
\bm Y=\bm{X}\bm{\beta}+\bm{\epsilon} 
(\#eq:lmHom)
\end{equation}
where $\bm{\epsilon}=(\epsilon_1, \ldots, \epsilon_n)^T$ has a multivariate normal distribution
with mean vector ${\bf 0}$ and variance covariance matrix $\sigma^2\bm{I}$,
(because all $\text{Var}(\epsilon_i)=\sigma^2$ and $\epsilon_1, \ldots, \epsilon_n$ are independent
implies all $\text{Cov}(\epsilon_i,\epsilon_j)=0$).
It follows from \@ref(eq:lmHom) that the distribution of $\bm Y$ is multivariate normal
with mean vector $\bm{X}\bm{\beta}$ and variance covariance matrix $\sigma^2\bm{I}$,
*i.e.* $\bm Y\sim N(\bm{X}\bm{\beta},\sigma^2\bm{I})$.


### Least squares estimation

The regression coefficients $\beta_1, \ldots, \beta_p$ describe
the pattern by which the response depends on the explanatory variables.
We use the observed data $y_1, \ldots, y_n$ to *estimate* this pattern of dependence.

In least squares estimation, roughly speaking, we choose $\hat{\bm{\beta}}$, the estimates of
$\bm{\beta}$ to make the fitted values $\hat{\bm{Y}}=\bm{X}\hat{\bm{\beta}}$ *as close as possible* to the
observed values $\bm{y}$, *i.e.* $\hat{\bm{\beta}}$ minimises the sum of squares
\begin{align*}
\sum_{i=1}^n [y_i-E(Y_i)]^2&=\sum_{i=1}^n \left(y_i-\bm{X}_i^T\bm{\beta}\right)^2\cr
&=\sum_{i=1}^n \left(y_i-\sum_{j=1}^p x_{ij}\beta_j\right)^2
\end{align*}
as a function of $\beta_1, \ldots, \beta_p$.
The sum of squares may also be written as $(\bm{y}-\bm{X}\bm{\beta})^T(\bm{y}-\bm{X}\bm{\beta})$.

For each $k = 1,\ldots, p$, differentiating w.r.t. $\beta_k$ and setting equal to $0$ gives
\[ -2 \sum_{i=1}^n x_{ik}\left(y_i-\sum_{j=1}^p x_{ij}\hat{\beta}_j\right)=   0\]
or
\[\sum_{i=1}^n x_{ik}y_i = \sum_{i=1}^n\sum_{j=1}^p x_{ik}x_{ij}\hat{\beta}_j.\]
In matrix form,
\[[\bm{X}^T\bm{y}]_k = [\bm{X}^T\bm{X}\hat{\bm{\beta}}]_k\]
so
\[\bm{X}^T\bm{y} = \bm{X}^T\bm{X}\hat{\bm{\beta}}\]

The least squares estimates $\hat{\bm{\beta}}$ are the solutions to this
set of $p$ simultaneous linear equations, which are known as
the *normal equations*.
If $\bm{X}^T\bm{X}$ is invertible (as it usually is if we specify the model carefully) then the least squares
estimates are given by
$$
\hat{\bm{\beta}}=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}.
$$
The corresponding fitted values are
\begin{align*}
& & \hat{\bm{Y}}=\bm{X}\hat{\bm{\beta}}=\bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}\cr
\Rightarrow & & \qquad \hat{Y}_i=\bm{X}_i^T\hat{\bm{\beta}}\qquad i = 1, \ldots, n.
\end{align*}
Recalling that $\bm Y=\bm{X}\bm{\beta}+\bm{\epsilon}$, we notice that the least squares estimates for
$\bm{\epsilon}=(\epsilon_1, \ldots, \epsilon_n)^T$ are obtained as the difference between the observed
and fitted values
\begin{align*}
& & \qquad\hat{\bm{\epsilon}}=\bm{y}-\bm{X}\hat{\bm{\beta}}\cr
\Rightarrow & & \qquad\hat{\epsilon}_i=y_i-\bm{X}_i^T\hat{\bm{\beta}}\qquad i = 1, \ldots, n.
\end{align*}
${\hat{\epsilon}_1,\ldots ,\hat{\epsilon}_n}$ describe the variability in the observed responses $y_1, \ldots, y_n$ which
has not been explained by the linear model. It is residual variability, and we call
${\hat{\epsilon}_1,\ldots ,\hat{\epsilon}_n}$, the residuals.
We call
\begin{align*}
D&= \sum_{i=1}^n \hat{\epsilon}_i^2\cr
&= \sum_{i=1}^n \left(y_i-\bm{X}_i^T\hat{\bm{\beta}}\right)^2
\end{align*}
the *residual sum of squares* or *deviance* for the linear model.
It is the actual minimum value attained in the least squares estimation.


### Properties of the least squares estimator

We have
$E(\hat{\bm{\beta}})=\bm{\beta}$ and $\text{Var}(\hat{\bm{\beta}})=\sigma^2(\bm{X}^T\bm{X})^{-1}$. 
This follows by recalling that $\bm Y\sim N(\bm{X}\bm{\beta},\sigma^2\bm{I})$ and
$\hat{\bm{\beta}}=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}$  is a linear function of $\bm{y}$.
Therefore, from the properties of expectation and variance of a vector random
variable we have
\begin{align*}
E(\hat{\bm{\beta}})&=E[(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm Y]\cr
&=(\bm{X}^T\bm{X})^{-1}\bm{X}^TE[\bm Y]\cr
&=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{X}\bm{\beta}\cr
&=\bm{\beta}
\end{align*}
and
\begin{align*}
\text{Var}(\hat{\bm{\beta}})&=\text{Var}[(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm Y]\cr
&=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\text{Var}[\bm Y][(\bm{X}^T\bm{X})^{-1}\bm{X}^T]^T\cr
&=\sigma^2(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{I}\bm{X} (\bm{X}^T\bm{X})^{-1}\cr
&=\sigma^2(\bm{X}^T\bm{X})^{-1}.
\end{align*}

Assuming that $\epsilon_1, \ldots, \epsilon_n$ are i.i.d. $N(0,\sigma^2)$ the least squares estimate $\hat{\bm{\beta}}$
is also the maximum likelihood estimate.
This is obvious when one considers the likelihood for a linear model
\begin{equation}
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{X}_i^T\bm{\beta})^2\right).
(\#eq:lmLikelihood)
\end{equation}
This is maximised with respect to $(\bm{\beta},\sigma^2)$ at
$\bm{\beta}=\hat{\bm{\beta}}$ and $\sigma^2=\hat \sigma^2=D/n$.

We have
$\hat{\bm{\beta}}$ is multivariate normal (with mean and variance given above).
As $\bm Y$ is normally distributed, and $\hat{\bm{\beta}}$ is
a linear function of $\bm Y$, then $\hat{\bm{\beta}}$
 must also be normally distributed.


### Estimation of $\sigma^2$


In addition to the linear coefficients $\beta_1, \ldots, \beta_p$ estimated using
least squares, a linear model usually involves the unknown *residual variance*
$\sigma^2$, representing the variability of observations
about their mean.

We can estimate $\sigma^2$ using maximum likelihood. Maximising \@(eq:lmLikelihood)
with respect to $\bm{\beta}$ and $\sigma^2$ gives
$$
\hat \sigma^2={D\over n}={1\over n}\sum_{i=1}^n \hat{\epsilon}_i^2.
$$
It is possible to prove (although we shall not do so here)
that if the model is correct,
$$
{D\over\sigma^2}\sim\chi^2_{n-p}
$$
which implies that
$$
E(\hat \sigma^2)={{n-p}\over n}\sigma^2,
$$
so the maximum likelihood estimator is biased for $\sigma^2$ (although
still asymptotically unbiased as ${{n-p}\over n}\to 1$ as $n\to\infty$).
We usually prefer to use the unbiased estimator of $\sigma^2$
$$
\tilde \sigma^2={D\over {n-p}}={1\over {n-p}}\sum_{i=1}^n \hat{\epsilon}_i^2.
$$
The denominator $n-p$, the number of observations minus the number of
linear coefficients in the model is called the *degrees of freedom* of the model.
Therefore, we estimate the residual variance by the deviance
divided by the degrees of freedom.

### Comparing linear models

If we have a set of competing linear models
which might explain the dependence of the response
on the explanatory variables, we will want to determine which of
the models is most appropriate.
Recall that we have three main requirements of a statistical model;
plausibility, parsimony and goodness of fit, of which parsimony
and goodness of fit are statistical issues.

The goodness of fit of a linear model to the observed data is
encapsulated by its deviance or residual sum of squares.
Models which fit the data well have a low deviance, whereas those which
fit the data poorly have a high deviance.
However, the calibration of 'low' and 'high' is unclear, and depends
on the scale of measurement of the response.
We can calibrate the deviance by dividing it by
the *natural variation in the data*, $\sum_{i=1}^n(y_i-\bar y)^2$.
Then
$$
R^2=1-{D\over{\sum_{i=1}^n(y_i-\bar y)^2}}
$$
is the proportion of natural variation in the data which has been
accounted for, or explained, by the linear model.

$R^2$ is still not an entirely satisfactory measure by which to compare models
(although it is an easily interpretable summary of the goodness-of-fit
of a selected model) because adding terms to a model increases $R^2$ whether
or not the increased complexity is justified.

The $C_p$ statistic
combines the deviance (a measure of goodness of fit)
and the number of linear parameters of the model (a measure of complexity)
into an overall measure for a model.
The $C_p$ statistic calculated by `R` is
$$
C_p=D+2p\sigma^2
$$
which clearly penalises a model for lack of fit (through $D$)
and complexity (through $p$, the number of linear parameters).
Here $\sigma^2$ is replaced by the most reliable estimate of $\sigma^2$ available, in practice
the estimate based on the most complex model under consideration.

Although $C_p$ provides a mechanism for comparing any pair of
models, it is essentially an *ad hoc* summary measure.
In practice, we use it as a guide to direct a
model comparison strategy based on hypothesis tests.

As described previously, we proceed by comparing models pairwise
using a generalised likelihood ratio test.
For linear models this kind of comparison is restricted to situations where
one of the models, $H_0$, is *nested* in the other, $H_1$.
For linear models, this usually means that the explanatory variables
present in $H_0$ are a subset of those present in $H_1$.
In this case model $H_0$ is a special case of model $H_1$, where certain coefficients
are set equal to zero.
We let $\bm \theta$ represent the collection of linear parameters for model $H_1$,
together with the residual variance $\sigma^2$, and let $\Theta^{(1)}$ be the
unrestricted parameter space for $\bm \theta$.
Then $\Theta^{(0)}$ is the parameter space corresponding
to model $H_0$, *i.e.* with the appropriate coefficients constrained to
zero.

We will assume that model $H_1$ contains $p$ linear parameters and
model $H_0$ a subset of $q<p$ of these.
Without loss of generality, we can think of $H_1$ as the model
$$
E(Y_i)=\sum_{j=1}^p x_{ij} \beta_j \qquad i = 1, \ldots, n
$$
and $H_0$ being the same model with
$$\beta_{q+1}=\beta_{q+2}=\cdots=\beta_p=0.
$$


Now, a *generalised likelihood ratio test* of $H_0$ against $H_1$
has a critical region of the form
$$
C=\left\{ \bm{y}:{{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(1)}}L(\bm{\beta},\sigma^2)}\over
{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(0)}}L(\bm{\beta},\sigma^2)}} >k\right\}
$$
where $k$ is determined by $\alpha$, the size of the test, so
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm{\beta},\sigma^2)=\alpha.
$$
For a linear model,
$$
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{X}_i^T\bm{\beta})^2\right).
$$
This is maximised with respect to $(\bm{\beta},\sigma^2)$ at
$\bm{\beta}=\hat{\bm{\beta}}$ and $\sigma^2=\hat \sigma^2=D/n$.
Therefore
\begin{align*}
\max_{\bm{\beta},\sigma^2} L(\bm{\beta},\sigma^2)&=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over{2D}} \sum_{i=1}^n (y_i-\bm{X}_i^T\hat{\bm{\beta}})^2\right)\cr
&=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over2}\right).\cr
\end{align*}
This form applies for both $\bm \theta\in\Theta^{(0)}$ and $\bm \theta\in\Theta^{(1)}$, with only the model changing. Let the deviances under models $H_0$ and $H_1$ be denoted by $D_0$ and $D_1$
respectively.
Then the critical region for the generalised likelihood ratio test
is of the form
\begin{align*}
&  \qquad & {{(2\pi D_1/n)^{-{n\over 2}}}\over{(2\pi D_0/n)^{-{n\over 2}}}}>k\cr
\Rightarrow & &\qquad \left({{D_0}\over{D_1}}\right)^{n\over 2}>k\cr
\Rightarrow & &\qquad \left({{D_0}\over{D_1}}-1\right){{n-p}\over{p-q}}>k'\cr
\Rightarrow & &\qquad {{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}>k'.
\end{align*}
We refer to the left hand side of this inequality as the F-statistic.
We reject the simpler model $H_0$ in favour of the more complex model $H_1$ if
$F$ is 'too large'.

As we have required $H_0$ to be nested in $H_1$, $F$ has a known distribution
when $H_0$ is true. It is an F distribution with $p-q$ degrees of freedom
in the numerator and $n-p$ degrees of freedom in the denominator.
To see this, note that
$$
{{D_0}\over\sigma^2}={{D_0-D_1}\over\sigma^2}+{{D_1}\over\sigma^2}.
$$
Furthermore, we know that, under $H_0$,
$D_1/\sigma^2$ has a $\chi^2_{n-p}$ distribution and
$D_0/\sigma^2$ has a $\chi^2_{n-q}$ distribution.
It is possible to show (although we will not do so here)
that under $H_0$, $(D_0-D_1)/\sigma^2$ and $D_0/\sigma^2$ are independent.
Therefore, from the properties of the chi-squared distribution, it follows that
under $H_0$, $(D_0-D_1)/\sigma^2$ has a  $\chi^2_{p-q}$ distribution, and
$F$ has a $F_{p-q,\,n-p}$ distribution.


Therefore, the precise critical region can be evaluated given the
size, $\alpha$,  of the test.
We reject $H_0$ in favour of $H_1$ when
$$
{{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}>k
$$
where $k$ is the $100(1-\alpha)\%$ point of the $F_{p-q,\,n-p}$ distribution.


### Assessing a selected model

An easily interpreted overall measure of goodness of fit of a model
is provided by the $R^2$ coefficient.
If predictions are required, then confidence intervals for the predictions
will provide an appropriate measure for assessing the likely accuracy of
the model.
Remember that it is always risky to use a model to make predictions
for values of the explanatory variables which
are not in the range of those which were used to fit the model.

Confidence intervals and hypothesis tests for linear models
may be unreliable if all the model assumptions are not justified.
In particular, we have made three assumptions about the
distribution of $Y_1, \ldots, Y_n$.

1. $Y_1, \ldots, Y_n$ are independent random variables.
1. $Y_1, \ldots, Y_n$ are normally distributed.
1. $\text{Var}(Y_1)=\text{Var}(Y_2)=\cdots =\text{Var}(Y_n)$.

The validity of these assumptions can be checked using residual plots.

1. In general, independence is difficult to validate, but where
observations have been collected in serial order, serial correlation
may be detected by a plot of the residuals ${\hat{\epsilon}_1,\ldots ,\hat{\epsilon}_n}$ against the serial
order $i = 1, \ldots, n$.
1. A simple check for non-normality is obtained by plotting the
ordered residuals against the expected order statistics of a sample of size $n$
from a standard normal distribution. The plot should look like a straight line.
Beware of any obvious curves in the plot.
1. A simple check for non-constant variance is obtained by plotting the
residuals ${\hat{\epsilon}_1,\ldots ,\hat{\epsilon}_n}$ against the corresponding fitted values
$\bm{X}_i^T\hat{\bm{\beta}},\quad i = 1, \ldots, n$. The plot should look like a random scatter.
Beware of 'funnelling'.


Other residual plots may also be useful. For example, if a plot of the residuals
against the values of an explanatory variable reveals a pattern,
then this suggests that the explanatory variable, or some
function of it, should be included in the model.

Another place where residual diagnostics are useful is in
assessing *influence*. An observation is influential if deleting it
would lead to estimates of model parameters being
substantially changed. `R` calculates Cook's distance for each observation.
Cook's distance is a measure of the change in $\hat{\bm{\beta}}$ when the
observation is omitted from the data set.
$$
\text{Cook's distance}=
{{(\hat{\bm{\beta}}'-\hat{\bm{\beta}})^T\bm{X}^T\bm{X}(\hat{\bm{\beta}}'-\hat{\bm{\beta}})}\over{p\hat \sigma^2}},
$$
where $\hat{\bm{\beta}}'$ is the least squares estimate of $\bm{\beta}$ based on
the observed data with the $i$th observation omitted.
A rule of thumb is that a Cook's distance of 1 or more indicates
a potentially important change in $\hat{\bm{\beta}}$ and may be worthy of
further investigation.


