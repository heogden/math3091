# Inference about parameters of a model {#inference}

## Introduction

Probability distributions like the binomial,  Poisson  and normal,
enable us to calculate probabilities, and other quantities of interest 
(e.g. expectations) for a probability model of a random process.
Therefore, given the model, we can make statements about possible
outcomes of the process.

Statistical inference is concerned with the inverse problem.
Given outcomes of a random process (observed data), what conclusions
(inferences) can we draw about the process itself?

We assume that the $n$ observations of the response
$\bm y=(y_1,\ldots ,y_n)^T$ are observations of random variables
$\bm Y=(Y_1,\ldots ,Y_n)^T$, which have joint p.d.f. $f_{\bm Y}$ (joint
p.f. for discrete variables).
We use the observed data $\bm y$ to make inferences about $f_{\bm Y}$.

We usually make certain assumptions about $f_{\bm Y}$. In particular,
we often assume that $y_1, \ldots, y_n$ are observations of
*independent* random variables.
Hence
$$
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
$$


In parametric statistical inference, we specify a joint
distribution $f_{\bm Y}$, for $\bm Y$, which is known, except for the values of
parameters $\theta_1,\theta_2,\ldots ,\theta_p$ (sometimes denoted by $\bm \theta$).
Then we use the observed data $\bm y$ to make inferences about
$\theta_1,\theta_2,\ldots ,\theta_p$. In this case, we usually write $f_{\bm Y}$ as
$f_{\bm Y}(\bm y;\bm \theta)$, to make explicit the dependence on the unknown
$\bm \theta$.

## The likelihood function

We often think of the joint density $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm{y}$ for fixed $\bm \theta$, which describes the relative probabilities of different
possible values of $\bm y$, given a particular set of parameters $\bm \theta$.
However, in statistical inference, we have observed $y_1, \ldots, y_n$ (values of $Y_1, \ldots, Y_n$).
Knowledge of the probability of alternative possible realisations of $\bm Y$
is largely irrelevant.
What we want to know about is $\bm \theta$.

Our only link between the observed data $y_1, \ldots, y_n$ and
$\bm \theta$ is through the function $f_{\bm Y}(\bm y;\bm \theta)$. 
Therefore, it seems sensible
that parametric statistical inference should be based on this function.
We can think of $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm \theta$ for fixed $\bm{y}$, which describes the relative *likelihoods* of
different possible (sets of) $\bm \theta,$ given observed data $y_1, \ldots, y_n$.
We write 
\[L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)\]
for this *likelihood*,
which is a function of the unknown parameter $\bm \theta$. For convenience, we often
drop $\bm y$ from the notation, and write $L(\bm \theta)$.

The likelihood function is of central importance in parametric statistical
inference.
It provides a means for comparing different possible values of $\bm \theta$, based on
the probabilities (or probability densities) that they assign to the observed data $y_1, \ldots, y_n$.

#### Notes {-}

1. Frequently it is more convenient to consider the *log-likelihood* function
  $\ell(\bm \theta) = \log L(\bm \theta)$.
1. Nothing in the definition of the likelihood requires $y_1, \ldots, y_n$ to be
  observations of independent random variables, although we shall frequently
  make this assumption.
1. Any factors which depend on $y_1, \ldots, y_n$ alone (and not on $\bm \theta$) can be ignored
  when writing down the likelihood. Such factors give no information about the   relative likelihoods of different possible values of $\bm \theta$.

```{example, name = "Bernoulli"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, independent identically distributed
(i.i.d.) Bernoulli$(p)$ random
variables. Here $\theta=(p)$ and the likelihood is
$$
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}.
$$
The log-likelihood is
$$
\ell(p) = \log L(p) =n\bar y\log p+n(1-\bar y)\log(1-p).
$$
```

```{example, name = "Normal"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and the likelihood is
\begin{align*}
L(\mu,\sigma^2) &=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right).
\end{align*}
The log-likelihood is
\[\ell(\mu, \sigma^2) = \log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]
```

## Maximum  likelihood estimation 

One of the primary tasks of parametric statistical inference is
*estimation* of the unknown parameters $\theta_1, \ldots, \theta_p$.
Consider the value of $\bm \theta$ which maximises the
likelihood function. This is the 'most likely' value of $\bm \theta$, the one which makes
the observed data 'most probable'. When we are searching for an estimate of
$\bm \theta$, this would seem to be a good candidate.

We call the value of $\bm \theta$ which maximises the likelihood $L(\theta)$
the *maximum likelihood estimate* (MLE) of $\bm \theta$, denoted by
$\hat{\bm \theta}$.
$\hat{\bm \theta}$ depends on $\bm y$, as different observed data samples
lead to different likelihood functions.
The corresponding function of $\bm Y$ is called the
*maximum likelihood estimator*  and is also denoted by $\hat{\bm \theta}$.

Note that as $\bm \theta=(\theta_1, \ldots, \theta_p)$, the MLE for any component
of $\bm \theta$ is given by the corresponding component of
$\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T$.
Similarly, the MLE for any function of parameters $g(\bm \theta)$ is given
by $g(\hat{\bm \theta})$.

As $\log$ is a strictly increasing
function, the value of $\bm \theta$ which maximises 
$L(\bm \theta)$ also maximises $\ell(\bm \theta) = \log L (\bm \theta)$.
It is almost always easier to maximise $\ell(\bm \theta)$.
This is achieved in the usual way; finding a stationary point by differentiating
$\ell(\bm \theta)$ with respect to $\theta_1, \ldots, \theta_p$, and solving the resulting $p$
simultaneous equations. It should also be checked that the stationary point is a
maximum.

```{example, name = "Bernoulli"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
Bernoulli$(p)$ random variables. Here $\bm \theta=(p)$ and
the log-likelihood is
\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]
  Differentiating with respect to $p$,
  \[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]
    so the MLE $\hat p$ solves
    \[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]
  Solving this for $\hat{p}$ gives $\hat{p}=\bar y$.
  Note that
  \[\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}<0\]
   everywhere, so the stationary point is clearly a maximum.
```

```{example, name = "Normal"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
 and the log-likelihood is
\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]
   Differentiating with respect to $\mu$
\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]
  so $(\hat \mu, \hat \sigma^2)$ solve
\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  (\#eq:normalScoreMu)
\end{equation}
  Differentiating with respect to $\sigma^2$
\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]
so
\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  (\#eq:normalScoreSs)
\end{equation}
Solving \@ref(eq:normalScoreMu) and \@ref(eq:normalScoreSs), we obtain
$\hat{\mu}=  \bar y$ and
\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]

Strictly,
to show that this stationary point is a maximum, we need to show that the
Hessian matrix (the matrix of second derivatives with elements
$[\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)$)
is negative definite at $\bm \theta=\hat{\bm \theta}$, that is $\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}<0$ for
every
$\bm{a}\ne {\bf 0}$.
Here
$$
\bm{H}(\hat{\mu},\hat \sigma^2)= \begin{pmatrix}
- \frac{n}{\hat \sigma^2 } & 0 \cr
0   &-\frac{n}{2(\hat \sigma^2)^2} \end{pmatrix}
$$
which is clearly negative definite.
```

## Score {#score}

Let
$$
u_i(\bm \theta)\equiv{{\partial}\over{\partial\theta_i}} \ell(\theta), \quad i=1,\ldots ,p
$$
and $\bm{u}(\bm \theta)\equiv[u_1(\bm \theta),\ldots ,u_p(\bm \theta)]^T$. Then we call $\bm{u}(\bm \theta)$ the
*vector of scores* or *score vector*.
Where $p=1$ and $\bm \theta=(\theta)$, the *score* is the scalar defined as
$$
u(\theta)\equiv{{\partial}\over{\partial\theta}}\ell(\theta).
$$
The maximum likelihood estimate $\hat{\bm \theta}$ satisfies
\[u(\hat{\bm \theta})={\bm 0},\]
that is,
\[u_i(\hat{\bm \theta})=0, \quad i=1,\ldots ,p.\]
Note that $u(\bm{\theta})$ is a function of $\bm \theta$ for fixed (observed) $\bm y$.
However, if we replace $y_1, \ldots, y_n$ in $u(\bm{\theta})$, by the corresponding random variables
$Y_1, \ldots, Y_n$ then we obtain a vector of random variables $U(\bm{\theta})\equiv
[U_1(\bm \theta),\ldots ,U_p(\bm \theta)]^T$.

An important result in likelihood theory is that the expected score
at the true (but unknown) value of $\bm \theta$ is zero:

```{theorem}
We have $E[U(\bm{\theta})]={\bm 0}$, *i.e.*
$E[U_i(\bm \theta)]= 0,$ $i=1,\ldots ,p,$ provided that

1. The expectation exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

```

```{proof}
Our proof is for continuous $\bm y$ -- in the discrete case,
replace $\int$ by $\sum$.
For each $i=1, \ldots, n$
\begin{align*}
E[U_i(\bm \theta)]&=\int U_i(\bm \theta)f_{\bm Y}(\bm y, \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}}\int f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}} 1 =0,
\end{align*}
as required.
```

```{example, name = "Bernoulli"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\[u(p)=n\bar y/ p-n(1-\bar y)/(1-p).\]
Since $E[U(p)] = 0$, we must have $E[\bar Y]=p$ (which we already
know is correct).
```

```{example, name = "Normal"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
$N(\mu,\sigma^2)$ random variables. Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2)&= {{n(\bar y-\mu)}/{\sigma^2}}\cr
u_2(\mu,\sigma^2)&= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum_{i=1}^n{(y_i-\mu)^2}
\end{align*}
Since $E[\bm U(\mu,\sigma^2)] = {\bm 0}$, we must have
$E[\bar Y]=\mu$ and 
$E[{\textstyle{1\over n}}\sum_{i=1}^n{(Y_i-\mu)^2}]=\sigma^2.$
```

## Information {#info}

Suppose that $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, whose joint p.d.f.
$L(\theta)$ is completely specified except
for the values of $p$ unknown parameters $\bm \theta=(\theta_1, \ldots, \theta_p)^T$.
Previously, we defined the Hessian matrix $H(\bm{\theta})$ to be the matrix with
components
$$
[H(\bm{\theta})]_{ij}\equiv{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$
We call the matrix $-H(\bm{\theta})$ the *observed information matrix*.
Where $p=1$ and $\bm \theta=(\theta)$, the *observed information* is a
scalar defined as
$$
-H(\theta)\equiv-{{\partial}\over{\partial\theta^2}}\ell(\theta).
$$

<!-- Here, we are interpreting $\bm \theta$ as the true (but unknown) value of -->
<!-- the parameter. -->
As with the score, if we replace $y_1, \ldots, y_n$ in $H(\bm{\theta})$, by the
corresponding random variables
$Y_1, \ldots, Y_n$, we obtain a matrix of random variables.
Then, we define the *expected information matrix* or
*Fisher information matrix*
$$
[\mathcal{I}(\bm \theta)]_{ij}=E(-[H(\bm{\theta})]_{ij})
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$

An important result in likelihood theory is that the variance-covariance matrix
of the score vector is equal to the
expected information matrix:

```{theorem}
We have $\text{Var}[U(\bm{\theta})]=\mathcal{I}(\bm \theta)$,
*i.e.*
\[\text{Var}[U(\bm{\theta})]_{ij}= [\mathcal{I}(\bm \theta)]_{ij}, 
\quad i=1,\ldots ,p, \quad j=1,\ldots ,p\]
provided that

1. The variance exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

```

```{proof}
Our proof is for continuous $\bm y$ -- in the discrete case, 
replace $\int$ by $\sum$.

For each $i = 1,\ldots, p$ and $j = 1, \ldots, p$,
\begin{align*}
\text{Var}[U(\bm{\theta})]_{ij}&= E[U_i(\bm \theta)U_j(\bm \theta)]\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta)
{{\partial}\over{\partial\theta_j}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)}
{{{{\partial}\over{\partial\theta_j}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta)d\bm y\cr
&= \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)
 {{\partial}\over{\partial\theta_j}}  f_{\bm Y}(\bm y; \bm \theta)  d\bm y.
\end{align*}

Now
\begin{align*}
[\mathcal{I}(\bm \theta)]_{ij}&=E\left[-{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)\right]\cr
&=\int -{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta)  f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int -{{\partial}\over{\partial\theta_i}}\left[
{{{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}\right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int \left[
-{{{{\partial^2}\over{\partial\theta_i\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}
+ {{{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)^2} \right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= -{{\partial^2}\over{\partial\theta_i\partial\theta_j}}\int  f_{\bm Y}(\bm y; \bm \theta) d\bm y
+ \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)  d\bm y\cr
&= \text{Var}[U(\bm{\theta})]_{ij},
\end{align*}
as required.
```

```{example, name = "Bernoulli"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\begin{align*}
u(p)&= {{n\bar y}\over{ p}}-{{n(1-\bar y)}\over {(1-p)}}\cr
-H(p)&= {{n\bar y}\over{ p^2}}+{{n(1-\bar y)}\over {(1-p)^2}}\cr
{\cal I}(p)&= {{n}\over{ p}}+{{n}\over {(1-p)}}={{n}\over {p(1-p)}}.
\end{align*}
```

```{example, name = "Normal"}
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2) &=  \frac{n(\bar y-\mu)}{\sigma^2} \\
u_2(\mu,\sigma^2) &= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2.
\end{align*}
Therefore
$$
-\bm{H}(\mu,\sigma^2) = \begin{pmatrix}
\frac{n}{\sigma^2} & \frac{n(\bar y-\mu)}{(\sigma^2)^2} \cr
\frac{n(\bar y-\mu)}{(\sigma^2)^2}&
 \frac{1}{(\sigma^2)^3} \sum(y_i-\mu)^2- \frac{n}{2(\sigma^2)^2}
\end{pmatrix}
$$
$$
{\cal I}(\mu,\sigma^2)= \begin{pmatrix}
\frac{n}{\sigma^2} & 0 \cr
0& \frac{n}{2(\sigma^2)^2}
\end{pmatrix}.
$$
```

## Asymptotic distribution of the MLE {#sn:asnmle}

Maximum likelihood estimation is an attractive method of estimation
for a number of reasons.
It is intuitively sensible
and usually reasonably straightforward to carry out.
Even when the simultaneous equations we obtain by
differentiating the log-likelihood function are impossible to solve directly,
solution by numerical methods is usually feasible.

Perhaps the most compelling reason for considering maximum likelihood
estimation is the asymptotic behaviour of maximum likelihood estimators.

Suppose that $y_1, \ldots, y_n$ are observations of  independent random variables $Y_1, \ldots, Y_n$,
whose joint p.d.f.  $f_{\bm Y}(\bm y;\bm \theta)=\prod_{i=1}^n f_{Y_i}(y_i;\bm \theta)$ is
completely specified except for the values of an unknown parameter vector
$\bm \theta$, and that
$\hat{\bm \theta}$ is the maximum likelihood estimator of $\bm \theta$.

Then, as $n\to\infty$, the distribution of $\hat{\bm \theta}$ tends to a multivariate
normal distribution with mean vector $\bm \theta$ and variance covariance matrix
$\mathcal{I}(\bm \theta)^{-1}$.

Where $p=1$ and $\bm \theta=(\theta)$, the distribution of the MLE $\hat{\theta}$
tends to $N[\theta,1/{\cal I}(\theta)]$.

For 'large enough $n$', we can treat the asymptotic distribution of the MLE as
an approximation. The fact that $E(\hat{\bm \theta})\approx\bm \theta$ means that the maximum
likelihood estimator is *approximately unbiased*
for large samples. The variance of $\hat{\bm \theta}$ is approximately $\mathcal{I}(\bm \theta)^{-1}$. 
It is possible to show that this is the smallest possible variance of any unbiased estimator of 
$\bm \theta$ (this result is called the Cramér--Rao lower bound, which we do not prove here).
Therefore the MLE is the 'best possible' estimator in large samples
(and therefore we hope also reasonable in
small samples, though we should investigate this case by case).

## Quantifying uncertainty in parameter estimates

The usefulness of an estimate is always enhanced
if some kind of measure of its precision can also be provided.
Usually, this will be a *standard error*, an estimate of
the standard deviation of the associated estimator.
For the maximum likelihood estimator $\hat{\theta}$, a standard error is
given by
$$
s.e.(\hat{\theta})={1\over{{\cal I}(\hat{\theta})^{{1\over 2}}}},
$$
and for a vector parameter $\bm \theta$
$$
s.e.(\hat{\theta}_i)=[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{{1\over 2}},
\quad i=1,\ldots ,p.
$$

An alternative summary of the information provided by the observed data about the
location of a parameter $\theta$ and the associated precision is
a *confidence interval*.

The asymptotic distribution of the maximum likelihood estimator can be used to
provide approximate large sample confidence intervals. Asymptotically,
$\hat{\theta}_i$ has a
$N(\theta_i,[\mathcal{I}(\bm \theta)^{-1}]_{ii})$ distribution and we can find $z_{1-\frac{\alpha}{2}}$ such that
$$
P\left(- z_{1-\frac{\alpha}{2}}\le {{\hat{\theta}_i-\theta_i}\over{[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) = 1- \alpha.
$$
Therefore
$$
P\left(\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}\le\theta_i
\le\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}
\right) = 1- \alpha.
$$
The endpoints of this interval cannot be evaluated
because they also depend on the unknown parameter vector $\bm \theta$.
However, if we replace $\mathcal{I}(\bm \theta)$ by its MLE ${\cal I}(\hat{\bm \theta})$
we obtain the approximate large sample $100(1 - \alpha)\%$ confidence interval
$$
[\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2},
\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2}].
$$
For $\alpha=0.1,0.05,0.01$, $z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58$.

```{example, name = "Bernoulli"}
If $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables then asymptotically $\hat{p}=\bar y$ has a  $N(p,{p(1-p)}/ n)$
distribution, and a large sample 95\% confidence interval
for $p$ is
\begin{align*}
& [\hat{p}- 1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2},
\hat{p}+1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2}]
\cr
&=
[\hat{p}-1.96[\hat{p}(1-\hat{p})/n]^{1\over 2},
\hat{p}+1.96[\hat{p}(1-\hat{p})/n]^{1\over 2}]\cr
&=
[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}].
\end{align*}
```
