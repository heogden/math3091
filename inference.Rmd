# Likelihood-based statistical inference {#inference}

## The likelihood function

Probability distributions like the binomial,  Poisson  and normal,
enable us to calculate probabilities, and other quantities of interest 
(e.g. expectations) for a probability model of a random process.
Therefore, given the model, we can make statements about possible
outcomes of the process.

Statistical inference is concerned with the inverse problem.
Given outcomes of a random process (observed data), what conclusions
(inferences) can we draw about the process itself?

We assume that the $n$ observations of the response
$\bm y=(y_1,\ldots ,y_n)^T$ are observations of random variables
$\bm Y=(Y_1,\ldots ,Y_n)^T$, which have joint p.d.f. $f_{\bm Y}$ (joint
p.f. for discrete variables).
We use the observed data $\bm y$ to make inferences about $f_{\bm Y}$.

We usually make certain assumptions about $f_{\bm Y}$. In particular,
we often assume that $y_1, \ldots, y_n$ are observations of
*independent* random variables.
Hence
$$
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
$$


In parametric statistical inference, we specify a joint
distribution $f_{\bm Y}$, for $\bm Y$, which is known, except for the values of
parameters $\theta_1,\theta_2,\ldots ,\theta_p$ (sometimes denoted by $\bm \theta$).
Then we use the observed data $\bm y$ to make inferences about
$\theta_1,\theta_2,\ldots ,\theta_p$. In this case, we usually write $f_{\bm Y}$ as
$f_{\bm Y}(\bm y;\bm \theta)$, to make explicit the dependence on the unknown
$\bm \theta$.

Until now, we have thought of the joint density $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm{y}$ for fixed $\bm \theta$, which describes the relative probabilities of different
possible values of $\bm y$, given a particular set of parameters $\bm \theta$.
However, in statistical inference, we have observed $y_1, \ldots, y_n$ (values of $Y_1, \ldots, Y_n$).
Knowledge of the probability of alternative possible realisations of $\bm Y$
is largely irrelevant.
What we want to know about is $\bm \theta$.

Our only link between the observed data $y_1, \ldots, y_n$ and
$\bm \theta$ is through the function $f_{\bm Y}(\bm y;\bm \theta)$. 
Therefore, it seems sensible
that parametric statistical inference should be based on this function.
We can think of $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm \theta$ for fixed $\bm{y}$, which describes the relative *likelihoods* of
different possible (sets of) $\bm \theta,$ given observed data $y_1, \ldots, y_n$.
We write 
\[L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)\]
for this *likelihood*,
which is a function of the unknown parameter $\bm \theta$. For convenience, we often
drop $\bm y$ from the notation, and write $L(\bm \theta)$.

The likelihood function is of central importance in parametric statistical
inference.
It provides a means for comparing different possible values of $\bm \theta$, based on
the probabilities (or probability densities) that they assign to the observed data $y_1, \ldots, y_n$.

**Notes**

1. Frequently it is more convenient to consider the *log-likelihood* function
  $\ell(\bm \theta) = \log L(\bm \theta)$.
1. Nothing in the definition of the likelihood requires $y_1, \ldots, y_n$ to be
  observations of independent random variables, although we shall frequently
  make this assumption.
1. Any factors which depend on $y_1, \ldots, y_n$ alone (and not on $\bm \theta$) can be ignored
  when writing down the likelihood. Such factors give no information about the   relative likelihoods of different possible values of $\bm \theta$.

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, independent identically distributed
(i.i.d.) Bernoulli$(p)$ random
variables. Here $\theta=(p)$ and the likelihood is
$$
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}.
$$
The log-likelihood is
$$
\ell(p) = \log L(p) =n\bar y\log p+n(1-\bar y)\log(1-p).
$$


**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and the likelihood is
\begin{align*}
L(\mu,\sigma^2) &=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right).
\end{align*}
The log-likelihood is
\[\ell(\mu, \sigma^2) = \log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]


## Maximum  likelihood estimation 

One of the primary tasks of parametric statistical inference is
*estimation* of the unknown parameters $\theta_1, \ldots, \theta_p$.
Consider the value of $\bm \theta$ which maximises the
likelihood function. This is the 'most likely' value of $\bm \theta$, the one which makes
the observed data 'most probable'. When we are searching for an estimate of
$\bm \theta$, this would seem to be a good candidate.

We call the value of $\bm \theta$ which maximises the likelihood $L(\theta)$
the *maximum likelihood estimate* (MLE) of $\bm \theta$, denoted by
$\hat{\bm \theta}$.
$\hat{\bm \theta}$ depends on $\bm y$, as different observed data samples
lead to different likelihood functions.
The corresponding function of $\bm Y$ is called the
*maximum likelihood estimator*  and is also denoted by $\hat{\bm \theta}$.

Note that as $\bm \theta=(\theta_1, \ldots, \theta_p)$, the MLE for any component
of $\bm \theta$ is given by the corresponding component of
$\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T$.
Similarly, the MLE for any function of parameters $g(\bm \theta)$ is given
by $g(\hat{\bm \theta})$.

As $\log$ is a strictly increasing
function, the value of $\bm \theta$ which maximises 
$L(\bm \theta)$ also maximises $\ell(\bm \theta) = \log L (\bm \theta)$.
It is almost always easier to maximise $\ell(\bm \theta)$.
This is achieved in the usual way; finding a stationary point by differentiating
$\ell(\bm \theta)$ with respect to $\theta_1, \ldots, \theta_p$, and solving the resulting $p$
simultaneous equations. It should also be checked that the stationary point is a
maximum.

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
Bernoulli$(p)$ random variables. Here $\bm \theta=(p)$ and
the log-likelihood is
\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]
  Differentiating with respect to $p$,
  \[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]
    so the MLE $\hat p$ solves
    \[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]
  Solving this for $\hat{p}$ gives $\hat{p}=\bar y$.
  Note that
  \[\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}<0\]
   everywhere, so the stationary point is clearly a maximum.

**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
 and the log-likelihood is
\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]
   Differentiating with respect to $\mu$
\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]
  so $(\hat \mu, \hat \sigma^2)$ solve
\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  (\#eq:normalScoreMu)
\end{equation}
  Differentiating with respect to $\sigma^2$
\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]
so
\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  (\#eq:normalScoreSs)
\end{equation}
Solving \@ref(eq:normalScoreMu) and \@ref(eq:normalScoreSs), we obtain
$\hat{\mu}=  \bar y$ and
\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]


Strictly,
to show that this stationary point is a maximum, we need to show that the
Hessian matrix (the matrix of second derivatives with elements
$[\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)$)
is negative definite at $\bm \theta=\hat{\bm \theta}$, that is $\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}<0$ for
every
$\bm{a}\ne {\bf 0}$.
Here
$$
\bm{H}(\hat{\mu},\hat \sigma^2)= \begin{pmatrix}
- \frac{n}{\hat \sigma^2 } & 0 \cr
0   &-\frac{n}{2(\hat \sigma^2)^2} \end{pmatrix}
$$
which is clearly negative definite.

## Score {#score}

Let
$$
u_i(\bm \theta)\equiv{{\partial}\over{\partial\theta_i}} \ell(\theta), \quad i=1,\ldots ,p
$$
and $\bm{u}(\bm \theta)\equiv[u_1(\bm \theta),\ldots ,u_p(\bm \theta)]^T$. Then we call $\bm{u}(\bm \theta)$ the
*vector of scores* or *score vector*.
Where $p=1$ and $\bm \theta=(\theta)$, the *score* is the scalar defined as
$$
u(\theta)\equiv{{\partial}\over{\partial\theta}}\ell(\theta).
$$
The maximum likelihood estimate $\hat{\bm \theta}$ satisfies
\[u(\hat{\bm \theta})={\bm 0},\]
that is,
\[u_i(\hat{\bm \theta})=0, \quad i=1,\ldots ,p.\]
Note that $u(\bm{\theta})$ is a function of $\bm \theta$ for fixed (observed) $\bm y$.
However, if we replace $y_1, \ldots, y_n$ in $u(\bm{\theta})$, by the corresponding random variables
$Y_1, \ldots, Y_n$ then we obtain a vector of random variables $U(\bm{\theta})\equiv
[U_1(\bm \theta),\ldots ,U_p(\bm \theta)]^T$.

An important result in likelihood theory is that the expected score
at the true (but unknown) value of $\bm \theta$ is zero, *i.e.*
\[E[U(\bm{\theta})]={\bf 0}\]
or
\[E[U_i(\bm \theta)]= 0,
\quad i=1,\ldots ,p,\]
provided that

1. The expectation exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

\begin{proof}{(continuous $\bm y$ -- in discrete case replace $\int$ by $\sum$)}

For each $i=1, \ldots, n$
\begin{align*}
E[U_i(\bm \theta)]&=\int U_i(\bm \theta)f_{\bm Y}(\bm y, \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}}\int f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}} 1 =0.
\end{align*}
\end{proof}

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\[u(p)=n\bar y/ p-n(1-\bar y)/(1-p).\]
Since $E[U(p)] = 0$, we must have $E[\bar Y]=p$ (which we already
know is correct).

**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
$N(\mu,\sigma^2)$ random variables. Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2)&= {{n(\bar y-\mu)}/{\sigma^2}}\cr
u_2(\mu,\sigma^2)&= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum_{i=1}^n{(y_i-\mu)^2}
\end{align*}
Since $E[\bm U(\mu,\sigma^2)] = {\bm 0}$, we must have
$E[\bar Y]=\mu$ and 
$E[{\textstyle{1\over n}}\sum_{i=1}^n{(Y_i-\mu)^2}]=\sigma^2.$

## Information {#info}

Suppose that $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, whose joint p.d.f.
$L(\theta)$ is completely specified except
for the values of $p$ unknown parameters $\bm \theta=(\theta_1, \ldots, \theta_p)^T$.
Previously, we defined the Hessian matrix $H(\bm{\theta})$ to be the matrix with
components
$$
[H(\bm{\theta})]_{ij}\equiv{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$
We call the matrix $-H(\bm{\theta})$ the *observed information matrix*.
Where $p=1$ and $\bm \theta=(\theta)$, the *observed information* is a
scalar defined as
$$
-H(\theta)\equiv-{{\partial}\over{\partial\theta^2}}\ell(\theta).
$$

<!-- Here, we are interpreting $\bm \theta$ as the true (but unknown) value of -->
<!-- the parameter. -->
As with the score, if we replace $y_1, \ldots, y_n$ in $H(\bm{\theta})$, by the
corresponding random variables
$Y_1, \ldots, Y_n$, we obtain a matrix of random variables.
Then, we define the *expected information matrix* or
*Fisher information matrix*
$$
[\mathcal{I}(\bm \theta)]_{ij}=E(-[H(\bm{\theta})]_{ij})
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$

An important result in likelihood theory is that the variance-covariance matrix
of the score vector is equal to the
expected information matrix *i.e.*
\[\text{Var}[U(\bm{\theta})]=\mathcal{I}(\bm \theta)\]
or
\[\text{Var}[U(\bm{\theta})]_{ij}= [\mathcal{I}(\bm \theta)]_{ij}, 
\quad i=1,\ldots ,p, \quad j=1,\ldots ,p\]
provided that

1. The variance exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

\newpage
\begin{proof}{(continuous $\bm y$ -- in discrete case replace $\int$ by $\sum$)}

For each $i = 1,\ldots, p$ and $j = 1, \ldots, p$,
\begin{align*}
\text{Var}[U(\bm{\theta})]_{ij}&= E[U_i(\bm \theta)U_j(\bm \theta)]\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta)
{{\partial}\over{\partial\theta_j}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)}
{{{{\partial}\over{\partial\theta_j}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta)d\bm y\cr
&= \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)
 {{\partial}\over{\partial\theta_j}}  f_{\bm Y}(\bm y; \bm \theta)  d\bm y.
\end{align*}

Now
\begin{align*}
[\mathcal{I}(\bm \theta)]_{ij}&=E\left[-{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)\right]\cr
&=\int -{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta)  f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int -{{\partial}\over{\partial\theta_i}}\left[
{{{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}\right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int \left[
-{{{{\partial^2}\over{\partial\theta_i\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}
+ {{{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)^2} \right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= -{{\partial^2}\over{\partial\theta_i\partial\theta_j}}\int  f_{\bm Y}(\bm y; \bm \theta) d\bm y
+ \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)  d\bm y\cr
&= \text{Var}[U(\bm{\theta})]_{ij}
\end{align*}
\end{proof}

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\begin{align*}
u(p)&= {{n\bar y}\over{ p}}-{{n(1-\bar y)}\over {(1-p)}}\cr
-H(p)&= {{n\bar y}\over{ p^2}}+{{n(1-\bar y)}\over {(1-p)^2}}\cr
{\cal I}(p)&= {{n}\over{ p}}+{{n}\over {(1-p)}}={{n}\over {p(1-p)}}.
\end{align*}

\newpage

**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2) &=  \frac{n(\bar y-\mu)}{\sigma^2} \\
u_2(\mu,\sigma^2) &= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2.
\end{align*}
Therefore
$$
-\bm{H}(\mu,\sigma^2) = \begin{pmatrix}
\frac{n}{\sigma^2} & \frac{n(\bar y-\mu)}{(\sigma^2)^2} \cr
\frac{n(\bar y-\mu)}{(\sigma^2)^2}&
 \frac{1}{(\sigma^2)^3} \sum(y_i-\mu)^2- \frac{n}{2(\sigma^2)^2}
\end{pmatrix}
$$
$$
{\cal I}(\mu,\sigma^2)= \begin{pmatrix}
\frac{n}{\sigma^2} & 0 \cr
0& \frac{n}{2(\sigma^2)^2}
\end{pmatrix}.
$$

## Asymptotic distribution of the MLE {#sn:asnmle}

Maximum likelihood estimation is an attractive method of estimation
for a number of reasons.
It is intuitively sensible
and usually reasonably straightforward to carry out.
Even when the simultaneous equations we obtain by
differentiating the log-likelihood function are impossible to solve directly,
solution by numerical methods is usually feasible.

Perhaps the most compelling reason for considering maximum likelihood
estimation is the asymptotic behaviour of maximum likelihood estimators.

Suppose that $y_1, \ldots, y_n$ are observations of  independent random variables $Y_1, \ldots, Y_n$,
whose joint p.d.f.  $f_{\bm Y}(\bm y;\bm \theta)=\prod_{i=1}^n f_{Y_i}(y_i;\bm \theta)$ is
completely specified except for the values of an unknown parameter vector
$\bm \theta$, and that
$\hat{\bm \theta}$ is the maximum likelihood estimator of $\bm \theta$.

Then, as $n\to\infty$, the distribution of $\hat{\bm \theta}$ tends to a multivariate
normal distribution with mean vector $\bm \theta$ and variance covariance matrix
$\mathcal{I}(\bm \theta)^{-1}$.

Where $p=1$ and $\bm \theta=(\theta)$, the distribution of the MLE $\hat{\theta}$
tends to $N[\theta,1/{\cal I}(\theta)]$.

For 'large enough $n$', we can treat the asymptotic distribution of the MLE as
an approximation. The fact that $E(\hat{\bm \theta})\approx\bm \theta$ means that the maximum
likelihood estimator is *approximately unbiased*
for large samples. The variance of $\hat{\bm \theta}$ is approximately $\mathcal{I}(\bm \theta)^{-1}$. 
It is possible to show that this is the smallest possible variance of any unbiased estimator of 
$\bm \theta$ (this result is called the Cramér--Rao lower bound, which we do not prove here).
Therefore the MLE is the 'best possible' estimator in large samples
(and therefore we hope also reasonable in
small samples, though we should investigate this case by case).


The usefulness of an estimate is always enhanced
if some kind of measure of its precision can also be provided.
Usually, this will be a *standard error*, an estimate of
the standard deviation of the associated estimator.
For the maximum likelihood estimator $\hat{\theta}$, a standard error is
given by
$$
s.e.(\hat{\theta})={1\over{{\cal I}(\hat{\theta})^{{1\over 2}}}},
$$
and for a vector parameter $\bm \theta$
$$
s.e.(\hat{\theta}_i)=[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{{1\over 2}},
\quad i=1,\ldots ,p.
$$

An alternative summary of the information provided by the observed data about the
location of a parameter $\theta$ and the associated precision is
a *confidence interval*.

The asymptotic distribution of the maximum likelihood estimator can be used to
provide approximate large sample confidence intervals. Asymptotically,
$\hat{\theta}_i$ has a
$N(\theta_i,[\mathcal{I}(\bm \theta)^{-1}]_{ii})$ distribution and we can find $z_{1-\frac{\alpha}{2}}$ such that
$$
P\left(- z_{1-\frac{\alpha}{2}}\le {{\hat{\theta}_i-\theta_i}\over{[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) = 1- \alpha.
$$
Therefore
$$
P\left(\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}\le\theta_i
\le\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}
\right) = 1- \alpha.
$$
The endpoints of this interval cannot be evaluated
because they also depend on the unknown parameter vector $\bm \theta$.
However, if we replace $\mathcal{I}(\bm \theta)$ by its MLE ${\cal I}(\hat{\bm \theta})$
we obtain the approximate large sample $100(1 - \alpha)\%$ confidence interval
$$
[\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2},
\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2}].
$$
For $\alpha=0.1,0.05,0.01$, $z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58$.

**Example (Bernoulli)**.
If $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables then asymptotically $\hat{p}=\bar y$ has a  $N(p,{p(1-p)}/ n)$
distribution, and a large sample 95\% confidence interval
for $p$ is
\begin{align*}
& [\hat{p}- 1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2},
\hat{p}+1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2}]
\cr
&=
[\hat{p}-1.96[\hat{p}(1-\hat{p})/n]^{1\over 2},
\hat{p}+1.96[\hat{p}(1-\hat{p})/n]^{1\over 2}]\cr
&=
[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}].
\end{align*}


## Comparing statistical models

If we have a set of competing probability models
which might have generated the observed data, we may want to determine which of
the models is most appropriate.
In practice, we proceed by comparing models pairwise.
Suppose that we have two competing alternatives, $f^{(0)}_{\bm Y}$ (model
$H_0$) and $f^{(1)}_{\bm Y}$  (model $H_1$) for $f_{\bm Y}$,
the joint distribution of $Y_1, \ldots, Y_n$.
The most common situation is where $H_0$ and $H_1$
both take the same parametric form, $f_{\bm Y}(\bm{y};\bm \theta)$ but
with $\bm \theta\in\Theta^{(0)}$ for $H_0$ and $\bm \theta\in\Theta^{(1)}$ for $H_1$,
where $\Theta^{(0)}$ and $\Theta^{(1)}$ are alternative sets of possible values
for $\bm \theta$.

A hypothesis test provides a mechanism for comparing the two competing statistical
models, $H_0$ and $H_1$.
A hypothesis test does not treat the two hypotheses (models) symmetrically. One
hypothesis, $H_0$, is accorded special status, and referred to as the 
*null hypothesis*. The null hypothesis is the reference model, and will be assumed to
be appropriate unless the observed data strongly indicate that $H_0$ is
inappropriate, and that $H_1$ (the *alternative* hypothesis) should be
preferred.

Hence, the fact that a hypothesis test does not reject $H_0$ should not be taken
as evidence that $H_0$ is true and $H_1$ is not, or that $H_0$ is  better
supported by the data than $H_1$, merely that the data does not provide
sufficient evidence to reject $H_0$ in favour of $H_1$.


A hypothesis test is defined by its *critical region* or 
*rejection region*, which we shall denote by $C$. $C$ is a subset of $\mathbb{R}^n$ and is the set
of possible $\bm{y}$ which would lead to rejection
of $H_0$ in favour of $H_1$, *i.e.*

- If $\bm{y} \in C$, $H_0$ is rejected in favour of $H_1$;
- If $\bm{y} \not\in C$, $H_0$ is not rejected.

As $\bm Y$ is a random variable, there remains the possibility that a hypothesis
test will produce an erroneous result.
We define the *size* (or *significance level*) of the test
\[\alpha = \max_{\bm \theta\in\Theta^{(0)}}P(\bm Y\in C;\bm \theta)\]
This is the maximum probability of erroneously rejecting $H_0$, over all
possible distributions for $\bm Y$ implied by $H_0$.
We also define the power function 
\[\omega(\bm \theta)= P(\bm Y\in C;\bm \theta)\]
It represents the probability of rejecting $H_0$
for a particular value of $\bm \theta$.
Clearly we would like to find a test with where
$\omega(\bm \theta)$ is large for every $\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}$, 
while at the same time avoiding erroneous rejection of $H_0$.
In other words, a good test will
have small size, but large power.

The general hypothesis testing procedure is to fix $\alpha$ to be some small
value (often 0.05), so that the probability of erroneous rejection of $H_0$ is
limited. In doing this, we are giving $H_0$ precedence over $H_1$.
Given our specified $\alpha$, we try to choose a test, defined by its
rejection region $C$, to make $\omega(\bm \theta)$ as large  as possible for
$\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}$.


Suppose that $H_0$ and $H_1$ both take the same parametric form,
$f_{\bm Y}(\bm{y};\bm \theta)$ with $\bm \theta\in\Theta^{(0)}$ for $H_0$ and $\bm \theta\in\Theta^{(1)}$
for $H_1$, where $\Theta^{(0)}$ and $\Theta^{(1)}$ are alternative sets of
possible values for $\bm \theta$.
A *generalised likelihood ratio test* of $H_0$ against $H_1$ has a critical region
of the form
\[C=\left\{ \bm{y}: 
\frac{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)} 
>k\right\}\]
where $k$ is determined by $\alpha$, the size of the test, so
\[\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.\]
Therefore, we will only reject
$H_0$ if $H_1$ offers a distribution for $Y_1, \ldots, Y_n$ which makes the observed data
much more probable than any distribution under $H_0$.
This is intuitively appealing and  tends to produce good tests (large power)
across a wide range of examples.

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Suppose that we require a size $\alpha$ test of the hypothesis
$H_0$: $p=p_0$ against the general alternative $H_1$: '$p$ is unrestricted' where
$\alpha$ and
$p_0$ are specified.

Here $\bm \theta=(p)$, $\Theta^{(0)}=\{p_0\}$ and $\Theta^{(1)}=(0,1)$ and the
generalised likelihood ratio test rejects $H_0$ when
\[{{\max_{p\in(0,1)}L(p)}\over{\max_{p=p_0}L(p)}} > k\]
or equivalently when
\[{{\bar y^{\sum_i y_i}(1-\bar y)^{n-\sum_i y_i}}\over {p_0^{\sum_i y_i}(1-p_0)^{n-\sum_i y_i}}} > k\]
or
\begin{equation}
\left({{\bar y}\over{p_0}}\right)^{n\bar y}
\left({{1-\bar y}\over{1-p_0}}\right)^{n(1-\bar y)} >k. 
  (\#eq:her1)
\end{equation}
Now the left hand side of \@ref(eq:her1) is minimised as a function of $\bar y$
at $\bar y=p_0$ and increases as
$\bar y$ moves away from $p_0$ in either direction.
Therefore, the rejection region \@ref(eq:her1) is equivalent to
$$
C=\left\{ \bm{y}:\bar y > k' \text{ or } \bar y < k''\right\}
$$
where $k'$ and $k''$ are chosen so that
$$
P(\bm{y}\in C;p_0)=\alpha.
$$
Therefore, we can use the binomial$(n,p_0)$ distribution to find a precise rejection
region for a test of specified size $\alpha$.

Alternatively, if $n$ is large, we can use the asymptotic distribution of
$\bar y$, $N(p_0,p_0[1-p_0]/n)$.


## The log-likelihood ratio statistic {#sn:lrt}

A *generalised likelihood ratio test* of $H_0$ against $H_1$ has a critical region
of the form
$$
C=\left\{ \bm{y}:{{\max_{\bm \theta\in \Theta^{(1)}} L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}} >k\right\}
$$
where $k$ is determined by $\alpha$, the size of the test, so
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
$$

Therefore, in order to determine $k$, we need
to know the distribution of the likelihood ratio, or an equivalent statistic,
under $H_0$.
In general, this will not be available to us.
However, we can make use of an important asymptotic result.


First we notice that, as $\log$ is a strictly increasing function, the rejection
region is equivalent to

$$
C=\left\{ \bm{y}: 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right) >k'\right\}
$$
where
$$
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
$$
Now, provided that $H_0$ is *nested within* $H_1$, in other words
$\Theta^{(0)}\subset\Theta^{(1)}$ ($\Theta^{(0)}$ is a subspace of
$\Theta^{(1)}$) then under
$H_0$: $\bm \theta\in\Theta^{(0)}$, asymptotically as
$n\to\infty$
$$
L_{01}\equiv 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)
$$
has a chi-squared distribution with degrees of freedom equal to the difference in
the dimensions of $\Theta^{(1)}$ and $\Theta^{(0)}$.

\begin{proof}
First we note that in the case where $\bm \theta$ is one-dimensional and $\bm \theta=(\theta)$,
a Taylor series expansion of $\ell(\theta)$  around the
MLE $\hat{\theta}$ gives
$$
\ell(\theta)=\ell(\hat{\theta})+(\theta-\hat{\theta})
U(\hat{\theta})+{1\over 2}(\theta-\hat{\theta})^2 U'(\hat{\theta}) +\;\ldots
$$
Now, $U(\hat{\theta})=0$, and if we approximate $U'(\hat{\theta})\equiv
H(\hat{\theta})$ by $E[H(\theta)]\equiv -{\cal I}(\theta)$, and also ignore higher order
terms, we obtain
$$
2[\ell (\hat{\theta})-\ell(\theta)]=
(\theta-\hat{\theta})^2 {\cal I}(\theta)
$$
As $\hat{\theta}$ is asymptotically
$N[\theta,{\cal I}(\theta)^{-1}]$, $(\theta-\hat{\theta})^2 {\cal I}(\theta)$ is
asymptotically $\chi^2_1$, and hence so is $2[\ell(\hat \theta)-\ell (\theta)]$.

Similarly it can be shown that when $\bm \theta\in\Theta$, a multidimensional space,
$2[\ell(\bm{\hat \theta})-\ell (\bm \theta)]$ is asymptotically
$\chi^2_p$, where $p$ is the dimension of $\Theta$.

Let the dimensions of $\Theta^{(0)}$ and $\Theta^{(1)}$ be $d_0$ and $d_1$ respectively. Now, suppose that $H_0$ is true and  $\bm \theta\in\Theta^{(0)}$ and therefore
$\bm \theta\in\Theta^{(1)}$. Furthermore, suppose that $\ell(\bm \theta)$
is maximised in $\Theta^{(0)}$ by $\hat{\bm \theta}^{(0)}$ and is maximised
in $\Theta^{(1)}$ by $\hat{\bm \theta}^{(1)}$. Then
\begin{align*}
L_{01}&\equiv  2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)\cr
&= 2\log L(\hat{\bm \theta}^{(1)})-2\log L(\hat{\bm \theta}^{(0)})\cr
&= 2[\log L(\hat{\bm \theta}^{(1)})-\log L(\bm \theta)]
 -2[\log L(\hat{\bm \theta}^{(0)})-\log L(\bm \theta)]\cr
&= L_1-L_0.
\end{align*}

Therefore $L_1=L_{01}+L_0$ and we know that, under $H_0$, $L_1$ has
a $\chi^2_{d_1}$ distribution and $L_0$ has
a $\chi^2_{d_0}$ distribution.
Furthermore, it is possible to show (although we will not do so here)
that under $H_0$, $L_{01}$ and $L_0$ are independent.
It can also be shown that under $H_0$  the difference $ L_1-L_0$
can be expressed as a quadratic form of normal random variables.
Therefore, it follows that under $H_0$, the log likelihood ratio
statistic $L_{01}$ has a $\chi^2_{d_1-d_0}$ distribution.
\end{proof}

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Suppose that we require a size $\alpha$ test of the hypothesis
$H_0$: $p=p_0$ against the general alternative $H_1$: '$p$ is unrestricted' where
$\alpha$ and
$p_0$ are specified.

Here $\bm \theta=(p)$, $\Theta^{(0)}=\{p_0\}$ and $\Theta^{(1)}=(0,1)$ and the
log likelihood ratio statistic is
$$
L_{01}=2n\bar y\log\left({{\bar y}\over{p_0}}\right)
+2n(1-\bar y)\log\left({{1-\bar y}\over{1-p_0}}\right).
$$
As $d_1=1$ and $d_0=0$, under $H_0$, the log likelihood ratio statistic
has an asymptotic $\chi^2_1$ distribution.
For a log likelihood ratio test, we only reject $H_0$ in favour of $H_1$ when the test
statistic is too large (observed data are much more probable under model $H_1$ than
under model $H_0$), so in this case we reject $H_0$ when the observed value
of the test statistic above is 'too large' to have come from a
$\chi^2_1$ distribution.
What we mean by 'too large' depends on the significance level $\alpha$ of the
test. For example, if $\alpha=0.05$, a common choice, then we should reject $H_0$
if the test statistic is greater than the 3.84, the 95\% point of the
$\chi^2_1$ distribution.

<!-- \begin{figure}[hbt] -->
<!-- \begin{center} -->
<!-- \includegraphics[height=3in]{chsq1} -->
<!-- \end{center} --
## Likelihood-based statistical theory

## The likelihood function

Probability distributions like the binomial,  Poisson  and normal,
enable us to calculate probabilities, and other quantities of interest 
(e.g. expectations) for a probability model of a random process.
Therefore, given the model, we can make statements about possible
outcomes of the process.

Statistical inference is concerned with the inverse problem.
Given outcomes of a random process (observed data), what conclusions
(inferences) can we draw about the process itself?

We assume that the $n$ observations of the response
$\bm y=(y_1,\ldots ,y_n)^T$ are observations of random variables
$\bm Y=(Y_1,\ldots ,Y_n)^T$, which have joint p.d.f. $f_{\bm Y}$ (joint
p.f. for discrete variables).
We use the observed data $\bm y$ to make inferences about $f_{\bm Y}$.

We usually make certain assumptions about $f_{\bm Y}$. In particular,
we often assume that $y_1, \ldots, y_n$ are observations of
*independent* random variables.
Hence
$$
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
$$


In parametric statistical inference, we specify a joint
distribution $f_{\bm Y}$, for $\bm Y$, which is known, except for the values of
parameters $\theta_1,\theta_2,\ldots ,\theta_p$ (sometimes denoted by $\bm \theta$).
Then we use the observed data $\bm y$ to make inferences about
$\theta_1,\theta_2,\ldots ,\theta_p$. In this case, we usually write $f_{\bm Y}$ as
$f_{\bm Y}(\bm y;\bm \theta)$, to make explicit the dependence on the unknown
$\bm \theta$.

Until now, we have thought of the joint density $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm{y}$ for fixed $\bm \theta$, which describes the relative probabilities of different
possible values of $\bm y$, given a particular set of parameters $\bm \theta$.
However, in statistical inference, we have observed $y_1, \ldots, y_n$ (values of $Y_1, \ldots, Y_n$).
Knowledge of the probability of alternative possible realisations of $\bm Y$
is largely irrelevant.
What we want to know about is $\bm \theta$.

Our only link between the observed data $y_1, \ldots, y_n$ and
$\bm \theta$ is through the function $f_{\bm Y}(\bm y;\bm \theta)$. 
Therefore, it seems sensible
that parametric statistical inference should be based on this function.
We can think of $f_{\bm Y}(\bm y;\bm \theta)$ as a function
of $\bm \theta$ for fixed $\bm{y}$, which describes the relative *likelihoods* of
different possible (sets of) $\bm \theta,$ given observed data $y_1, \ldots, y_n$.
We write 
\[L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)\]
for this *likelihood*,
which is a function of the unknown parameter $\bm \theta$. For convenience, we often
drop $\bm y$ from the notation, and write $L(\bm \theta)$.

The likelihood function is of central importance in parametric statistical
inference.
It provides a means for comparing different possible values of $\bm \theta$, based on
the probabilities (or probability densities) that they assign to the observed data $y_1, \ldots, y_n$.

**Notes**

1. Frequently it is more convenient to consider the *log-likelihood* function
  $\ell(\bm \theta) = \log L(\bm \theta)$.
1. Nothing in the definition of the likelihood requires $y_1, \ldots, y_n$ to be
  observations of independent random variables, although we shall frequently
  make this assumption.
1. Any factors which depend on $y_1, \ldots, y_n$ alone (and not on $\bm \theta$) can be ignored
  when writing down the likelihood. Such factors give no information about the   relative likelihoods of different possible values of $\bm \theta$.

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, independent identically distributed
(i.i.d.) Bernoulli$(p)$ random
variables. Here $\theta=(p)$ and the likelihood is
$$
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}.
$$
The log-likelihood is
$$
\ell(p) = \log L(p) =n\bar y\log p+n(1-\bar y)\log(1-p).
$$


**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and the likelihood is
\begin{align*}
L(\mu,\sigma^2) &=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right).
\end{align*}
The log-likelihood is
\[\ell(\mu, \sigma^2) = \log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]


## Maximum  likelihood estimation 

One of the primary tasks of parametric statistical inference is
*estimation* of the unknown parameters $\theta_1, \ldots, \theta_p$.
Consider the value of $\bm \theta$ which maximises the
likelihood function. This is the 'most likely' value of $\bm \theta$, the one which makes
the observed data 'most probable'. When we are searching for an estimate of
$\bm \theta$, this would seem to be a good candidate.

We call the value of $\bm \theta$ which maximises the likelihood $L(\theta)$
the *maximum likelihood estimate* (MLE) of $\bm \theta$, denoted by
$\hat{\bm \theta}$.
$\hat{\bm \theta}$ depends on $\bm y$, as different observed data samples
lead to different likelihood functions.
The corresponding function of $\bm Y$ is called the
*maximum likelihood estimator*  and is also denoted by $\hat{\bm \theta}$.

Note that as $\bm \theta=(\theta_1, \ldots, \theta_p)$, the MLE for any component
of $\bm \theta$ is given by the corresponding component of
$\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T$.
Similarly, the MLE for any function of parameters $g(\bm \theta)$ is given
by $g(\hat{\bm \theta})$.

As $\log$ is a strictly increasing
function, the value of $\bm \theta$ which maximises 
$L(\bm \theta)$ also maximises $\ell(\bm \theta) = \log L (\bm \theta)$.
It is almost always easier to maximise $\ell(\bm \theta)$.
This is achieved in the usual way; finding a stationary point by differentiating
$\ell(\bm \theta)$ with respect to $\theta_1, \ldots, \theta_p$, and solving the resulting $p$
simultaneous equations. It should also be checked that the stationary point is a
maximum.

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
Bernoulli$(p)$ random variables. Here $\bm \theta=(p)$ and
the log-likelihood is
\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]
  Differentiating with respect to $p$,
  \[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]
    so the MLE $\hat p$ solves
    \[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]
  Solving this for $\hat{p}$ gives $\hat{p}=\bar y$.
  Note that
  \[\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}<0\]
   everywhere, so the stationary point is clearly a maximum.

**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
 and the log-likelihood is
\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]
   Differentiating with respect to $\mu$
\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]
  so $(\hat \mu, \hat \sigma^2)$ solve
\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  (\#eq:normalScoreMu)
\end{equation}
  Differentiating with respect to $\sigma^2$
\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]
so
\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  (\#eq:normalScoreSs)
\end{equation}
Solving \@ref(eq:normalScoreMu) and \@ref(eq:normalScoreSs), we obtain
$\hat{\mu}=  \bar y$ and
\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]


Strictly,
to show that this stationary point is a maximum, we need to show that the
Hessian matrix (the matrix of second derivatives with elements
$[\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)$)
is negative definite at $\bm \theta=\hat{\bm \theta}$, that is $\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}<0$ for
every
$\bm{a}\ne {\bf 0}$.
Here
$$
\bm{H}(\hat{\mu},\hat \sigma^2)= \begin{pmatrix}
- \frac{n}{\hat \sigma^2 } & 0 \cr
0   &-\frac{n}{2(\hat \sigma^2)^2} \end{pmatrix}
$$
which is clearly negative definite.

## Score {#score}

Let
$$
u_i(\bm \theta)\equiv{{\partial}\over{\partial\theta_i}} \ell(\theta), \quad i=1,\ldots ,p
$$
and $\bm{u}(\bm \theta)\equiv[u_1(\bm \theta),\ldots ,u_p(\bm \theta)]^T$. Then we call $\bm{u}(\bm \theta)$ the
*vector of scores* or *score vector*.
Where $p=1$ and $\bm \theta=(\theta)$, the *score* is the scalar defined as
$$
u(\theta)\equiv{{\partial}\over{\partial\theta}}\ell(\theta).
$$
The maximum likelihood estimate $\hat{\bm \theta}$ satisfies
\[u(\hat{\bm \theta})={\bm 0},\]
that is,
\[u_i(\hat{\bm \theta})=0, \quad i=1,\ldots ,p.\]
Note that $u(\bm{\theta})$ is a function of $\bm \theta$ for fixed (observed) $\bm y$.
However, if we replace $y_1, \ldots, y_n$ in $u(\bm{\theta})$, by the corresponding random variables
$Y_1, \ldots, Y_n$ then we obtain a vector of random variables $U(\bm{\theta})\equiv
[U_1(\bm \theta),\ldots ,U_p(\bm \theta)]^T$.

An important result in likelihood theory is that the expected score
at the true (but unknown) value of $\bm \theta$ is zero, *i.e.*
\[E[U(\bm{\theta})]={\bf 0}\]
or
\[E[U_i(\bm \theta)]= 0,
\quad i=1,\ldots ,p,\]
provided that

1. The expectation exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

\begin{proof}{(continuous $\bm y$ -- in discrete case replace $\int$ by $\sum$)}

For each $i=1, \ldots, n$
\begin{align*}
E[U_i(\bm \theta)]&=\int U_i(\bm \theta)f_{\bm Y}(\bm y, \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}}\int f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= {{\partial}\over{\partial\theta_i}} 1 =0.
\end{align*}
\end{proof}

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\[u(p)=n\bar y/ p-n(1-\bar y)/(1-p).\]
Since $E[U(p)] = 0$, we must have $E[\bar Y]=p$ (which we already
know is correct).

**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d.
$N(\mu,\sigma^2)$ random variables. Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2)&= {{n(\bar y-\mu)}/{\sigma^2}}\cr
u_2(\mu,\sigma^2)&= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum_{i=1}^n{(y_i-\mu)^2}
\end{align*}
Since $E[\bm U(\mu,\sigma^2)] = {\bm 0}$, we must have
$E[\bar Y]=\mu$ and 
$E[{\textstyle{1\over n}}\sum_{i=1}^n{(Y_i-\mu)^2}]=\sigma^2.$

## Information {#info}

Suppose that $y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, whose joint p.d.f.
$L(\theta)$ is completely specified except
for the values of $p$ unknown parameters $\bm \theta=(\theta_1, \ldots, \theta_p)^T$.
Previously, we defined the Hessian matrix $H(\bm{\theta})$ to be the matrix with
components
$$
[H(\bm{\theta})]_{ij}\equiv{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$
We call the matrix $-H(\bm{\theta})$ the *observed information matrix*.
Where $p=1$ and $\bm \theta=(\theta)$, the *observed information* is a
scalar defined as
$$
-H(\theta)\equiv-{{\partial}\over{\partial\theta^2}}\ell(\theta).
$$

<!-- Here, we are interpreting $\bm \theta$ as the true (but unknown) value of -->
<!-- the parameter. -->
As with the score, if we replace $y_1, \ldots, y_n$ in $H(\bm{\theta})$, by the
corresponding random variables
$Y_1, \ldots, Y_n$, we obtain a matrix of random variables.
Then, we define the *expected information matrix* or
*Fisher information matrix*
$$
[\mathcal{I}(\bm \theta)]_{ij}=E(-[H(\bm{\theta})]_{ij})
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
$$

An important result in likelihood theory is that the variance-covariance matrix
of the score vector is equal to the
expected information matrix *i.e.*
\[\text{Var}[U(\bm{\theta})]=\mathcal{I}(\bm \theta)\]
or
\[\text{Var}[U(\bm{\theta})]_{ij}= [\mathcal{I}(\bm \theta)]_{ij}, 
\quad i=1,\ldots ,p, \quad j=1,\ldots ,p\]
provided that

1. The variance exists.
1. The sample space for $\bm Y$ does not depend on $\bm \theta$.

\newpage
\begin{proof}{(continuous $\bm y$ -- in discrete case replace $\int$ by $\sum$)}

For each $i = 1,\ldots, p$ and $j = 1, \ldots, p$,
\begin{align*}
\text{Var}[U(\bm{\theta})]_{ij}&= E[U_i(\bm \theta)U_j(\bm \theta)]\cr
&= \int {{\partial}\over{\partial\theta_i}} \ell(\theta)
{{\partial}\over{\partial\theta_j}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)}
{{{{\partial}\over{\partial\theta_j}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta)d\bm y\cr
&= \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)
 {{\partial}\over{\partial\theta_j}}  f_{\bm Y}(\bm y; \bm \theta)  d\bm y.
\end{align*}

Now
\begin{align*}
[\mathcal{I}(\bm \theta)]_{ij}&=E\left[-{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)\right]\cr
&=\int -{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta)  f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int -{{\partial}\over{\partial\theta_i}}\left[
{{{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}\right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&=\int \left[
-{{{{\partial^2}\over{\partial\theta_i\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}
+ {{{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)^2} \right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&= -{{\partial^2}\over{\partial\theta_i\partial\theta_j}}\int  f_{\bm Y}(\bm y; \bm \theta) d\bm y
+ \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)  d\bm y\cr
&= \text{Var}[U(\bm{\theta})]_{ij}
\end{align*}
\end{proof}

**Example (Bernoulli)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. Bernoulli$(p)$ random
variables. Here $\bm \theta=(p)$ and
\begin{align*}
u(p)&= {{n\bar y}\over{ p}}-{{n(1-\bar y)}\over {(1-p)}}\cr
-H(p)&= {{n\bar y}\over{ p^2}}+{{n(1-\bar y)}\over {(1-p)^2}}\cr
{\cal I}(p)&= {{n}\over{ p}}+{{n}\over {(1-p)}}={{n}\over {p(1-p)}}.
\end{align*}

\newpage

**Example (Normal)**.
$y_1, \ldots, y_n$ are observations of $Y_1, \ldots, Y_n$, i.i.d. $N(\mu,\sigma^2)$ random variables.
Here $\bm \theta=(\mu,\sigma^2)$ and
\begin{align*}
u_1(\mu,\sigma^2) &=  \frac{n(\bar y-\mu)}{\sigma^2} \\
u_2(\mu,\sigma^2) &= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2.
\end{align*}
Therefore
$$
-\bm{H}(\mu,\sigma^2) = \begin{pmatrix}
\frac{n}{\sigma^2} & \frac{n(\bar y-\mu)}{(\sigma^2)^2} \cr
\frac{n(\bar y-\mu)}{(\sigma^2)^2}&
 \frac{1}{(\sigma^2)^3} \sum(y_i-\mu)^2- \frac{n}{2(\sigma^2)^2}
\end{pmatrix}
$$
$$
{\cal I}(\mu,\sigma^2)= \begin{pmatrix}
\frac{n}{\sigma^2} & 0 \cr
0& \frac{n}{2(\sigma^2)^2}
\end{pmatrix}.
$$
