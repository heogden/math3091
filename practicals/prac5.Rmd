---
title: 'MATH3012 worksheet 5'
author: ""
date: ""
output: pdf_document
fontsize: 12pt
header-includes:
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_knit$set(root.dir = file.path('..', 'datasets'))
```

## Exploring the data

Load the data in `beetle.csv` into a variable `beetle`.
```{r, include = FALSE}
# read in the data
beetle <- read.csv("beetle.csv")
```

This dataset represents the number of beetles exposed (`exposed`) and number 
killed (`killed`) in eight groups exposed to different doses (`dose`)
of a particular insecticide. We are interested in how the proportion
of beetles killed is related to dose.

We could add a column to the dataset for the proportion of 
beetles killed in each of the eight groups:
```{r, include = FALSE}

# add a column for the proportion of beetles killed:
```
```{r}
beetle$prop_killed <- beetle$killed / beetle$exposed
```
Plot `prop_killed` against `dose`.
```{r, include = FALSE}

# plot prop_killed against dose:
plot(prop_killed ~ dose, data = beetle)
```
We might naively think about using a linear model to
model the dependence of `prop_killed` on `dose`:
```{r, include = FALSE}

# fit a linear model of prop_killed on dose
```
```{r}
beetle_lm <- lm(prop_killed ~ dose , data = beetle, weights = exposed)
```
```{r, include = FALSE}
# include "weights = exposed" to weight by number of beetles in each group
```
We use `weights = exposed` to weight
the response for each group by the number of beetles in that group.
Do you think this linear model is appropriate here?

You can add a fitted regression line to your plot of `prop_killed` against
`dose` with
```{r, include = FALSE}

# add a fitted regression line to plot:
```
```{r}
abline(beetle_lm)
```
Does the fitted regression line look reasonable here?
What is the predicted proportion of beetles who would be killed,
in a new group given `dose = 2`? Do you think this a good prediction?
```{r, include = FALSE}
# The fitted regression line does not look reasonable, because
# for large or small doses, it will give a predicted proportion
# of beetles killed which is greater than 1, or less than 0.

# Predict for dose = 2:
newdata <- data.frame(dose = 2)
predict(beetle_lm, newdata = newdata)
# The predicted proportion of beetles killed is 1.7, which is greater 
# than 1. This is not a sensible prediction.
```

## Fitting a logistic regression model

A better model might be to model the number of beetles killed in each
group as a binomial random variable with probability of death
depending on dose. Let $p_i$ denote the probability of death in the $i$th group, 
$i=1, \ldots, 8.$

A *logistic regression model* is
$$
Y_i | n_i, p_i \sim \text{Binomial}(n_i,p_i),\quad
\text{logit}(p_i) \equiv
\log\left({{p_i}\over{1-p_i}}\right)=\beta_1+\beta_2 x_i, \quad i=1,\ldots ,8,
$$
where $Y_i$ is the number of beetles `killed`, $n_i$ is the number of beetles
`exposed` and $x_i$ is the dose, for the $i$th row of the dataset.

This is a generalised linear model with a binomial distribution for
the response and logistic link function.

The quantity $\frac{p_i}{1-p_i}$ is called the **odds** of the event whose probability
is $p_i$: in this case, the odds of death. 
The interpretation of the above model is that the
logarithm of the odds of death is linear in $x$, the explanatory variable `dose`.

Fitting generalised linear models in `R` is very similar to fitting
linear models. You replace the command `lm` with the command
`glm`. The distribution and link function which you require for your model
is specified by the argument `family` which is supplied to the model.
For example `family = binomial(link = "logit")` fits a generalised linear
model with binomial distribution and logistic link.
In fact `family = binomial` does the same, as the default
is the canonical link.
The form of the linear predictor is specified in the model formula
for `glm` in the same way as for `lm`.

There are several ways to fit a logistic regression model in `R`.
One method, following our approach with linear models, is 
to let the response in the `formula` be the proportion of "successes" 
(proportion of beetles killed, `prop_killed`), and weight by the number 
of trials (number of beetles in the group, `exposed`):
```{r, include = FALSE}

# Fit a logistic regression model, using prop_killed as response:
```
```{r}
beetle_glm <- glm(prop_killed ~ dose, data = beetle, family = binomial,
                  weights = exposed)
```

Another method is to write the response as a matrix with two columns, 
with first column the number of "successes" (the number of
beetles killed, `killed`), and second
column the number of "failures" (the number of beetles not
killed, `exposed - killed`). We don't need to add weights with this
approach:
```{r, include = FALSE}

# Fit a logistic regression model, with a two-column matrix, with columns
# (no. of successes, no. of failures) as the response:
```
```{r}
beetle_glm_2 <- glm(cbind(killed, exposed - killed) ~ dose, data = beetle,
                    family = binomial)
```

The result of the `glm` command is a generalised linear model object
which can be used within many of the same `R` commands as a linear model
object.
Some useful ones are `resid`, `coef`, `deviance`, `fitted`,
`plot`, `summary`, `update` and `anova`.

## Interpreting the output

As with linear models, we can get parameter estimates and standard errors,
and conduct simple hypothesis tests, by using the `summary` command:
```{r, include = FALSE}

# summarise the fitted logistic regression:
```
```{r}
summary(beetle_glm)
```
```{r, include = FALSE}
# dose is highly significant here.
```
The `Coefficients` section gives the maximum likelihood estimates of the parameters,
together with their standard errors. The column `z value` is a statistic for
testing the hypothesis that the corresponding parameter is equal to zero,
which should be compared with a standard normal distribution. The
`Pr(>|z|)` column gives a $p$-value for this hypothesis test.

Use the `summary` command to check that the model
`beetle_glm_2` is equivalent to model `beetle_glm`.

Our fitted model is that
$$
\log \left( \frac{ p(x_i)}{1-p(x_i)} \right)
= \beta_1 + \beta_2 \, x_i = -60.72 + 34.27 x_i
$$
The MLE of $\beta_2$ is $\hat{\beta_2} = 34.27$ with s.e.
(asymptotic standard error) $2.912$.  From this we can test $H_0: \beta_2 =0$
by noting that under $H_0$, approximately $\hat{\beta_2}/\text{s.e.} \sim N(0, 1)$. 
So we will reject $H_0$ if
$|z| = |\hat{\beta_2}/\text{s.e.}| > 1.96$. In this case, $z = 11.77$, so
we reject $H_0$ at $5\%$ significance level. 
In fact, we would reject $H_0$ at any reasonable 
significance level: from the `summary` we see the $p$-value
for this test is $<2 \times 10^{-16}$.

We further interpret the estimate as follows. We first examine the model for two
values of dose $x$: $x_1$ and $x_2$.
\begin{align*}
\log \left( \frac{ p(x_2)}{1-p(x_2)} \right) &= \beta_1 + \beta_2 \, x_2 \\
&= \beta_1 + \beta_2 \, x_1 + \beta_2 (x_2 - x_1) \\
&=  \log \left( \frac{ p(x_1)}{1-p(x_1)} \right) +   \beta_2 (x_2 - x_1).
\end{align*}
By exponentiating both sides,
\[\frac{p(x_2)}{1-p(x_2)} =   \frac{ p(x_1)}{1-p(x_1) } e^{ \beta_2 (x_2 - x_1)}.\]

If we have $x_2 - x_1 = 1$, so $x_1 = x_2 -1$, we find that
\vspace{-0.5cm}
\begin{align*}
\frac{p(x_2)}{1-p(x_2)}  &=   \frac{ p(x_1)}{1-p(x_1) } \, e^{ \beta_2} \\
\text{Odds of death for $x_2$} &= \text{Odds of death for $(x_2-1)$} \times  e^{\beta_2}.
\end{align*}
The odds of death gets multiplied by $\exp(\beta_2)$ for each
unit increase in $x$.

Returning to this example, for a unit increase in $x$ (`dose`),
the estimated odds of death of a beetle are multiplied by `exp(34.27)`.  For example,
consider two beetles  with $x= 1.7$ and $1.8$ respectively.  
We estimate that the odds of death of the second beetle
is `exp(34.27 * 0.1) = 30.8` times the odds of death for the first beetle.

## Making predictions
We can make predictions for new doses by using `predict`:
```{r, include = FALSE}

# make predictions at dose = 1.7 and 1.8
```
```{r}
newdata <- data.frame(dose = c(1.7, 1.8))
predict(beetle_glm, newdata = newdata)
```
```{r, include = FALSE}
# by default, in predict, type = "link", which means we predict logit(p)
```
Are these predictions what you expected? Can you see
what is being predicted here?
You can check the help on this predict method with
```{r, purl = FALSE}
?predict.glm
```
The default for the `type` argument is `type = "link"`, which means 
these values are predictions of $\text{logit}(p(x))$, for each of the 
new doses. If we need the predicted probability of death $p(x)$,
we use `type = "response"` instead:
```{r, include = FALSE}

# To predict probabilities p, use type = "response":
```
```{r}
predict(beetle_glm, newdata = newdata, type = "response")
```
What is the predicted probability of death at `dose = 2`? How does this compare with
the prediction you found earlier with the linear model?
```{r, include = FALSE}

# To predict at dose = 2:
newdata <- data.frame(dose = 2)
predict(beetle_glm, newdata = newdata, type = "response")
# this is very close to (but less than) 1, which is much more sensible 
# than the predicted probability from the linear model (1.7).
```

We can add a curve of the predicted probabilities
from the logistic regression model
to our plot of `prop_killed` against dose, alongside the fitted regression
line from the linear model:
```{r, include = FALSE}

# Add fitted regression curves from linear model and logistic GLM 
# to a scatterplot of prop_killed against dose:
```
```{r}
newdata <- data.frame(dose = seq(1.6, 2, length = 30))
plot(prop_killed ~ dose, data = beetle, ylim = c(-0.5, 1.5), xlim = c(1.6, 2))
abline(beetle_lm)
lines(newdata$dose, predict(beetle_glm, newdata = newdata, type = "response"), 
      col = 2)
abline(h=c(0,1),lty=2)
legend("topleft", c("Linear model", "Logistic GLM"), lty = 1, col = c(1, 2))
```
Make sure that you understand what all of these commands are doing. 
Which of the models do you prefer after looking at these fitted regression curves?