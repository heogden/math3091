---
title: 'MATH3012, Worksheet 5: Logistic regression'
author: "Helen Ogden"
date: ""
output: pdf_document
header-includes:
- \newcommand{\bitem}{\begin{itemize}}
- \newcommand{\eitem}{\end{itemize}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
knitr::opts_knit$set(root.dir = file.path('..', 'datasets'))
```


Fitting generalised linear models in `R` is very similar to fitting
linear models. You replace the command `lm` with the command
`glm`. The distribution and link function which you require for your model
is specified by the argument `family` which is supplied to the model.
For example `family = binomial(logit)` fits a generalised linear
model with binomial distribution and logistic link.
In fact `family = binomial` does the same, as the default
is the canonical link.
The form of the linear predictor is specified in the model formula
for `glm` in the same way as for `lm`.

The result of the `glm` command is a generalised linear model object
which can be used within many of the same `R` commands as a linear model
object.
Useful ones are `resid`, `coef`, `deviance`, `fitted`,
`plot`, `summary`, `update` and `anova`.

Load the data in `beetle.csv` into a variable `beetle`.
```{r, include = FALSE}
beetle <- read.csv("beetle.csv")
```

This dataset represents the number of beetles exposed (`exposed`) and number 
killed (`killed`) in eight groups exposed to different doses (`dose`)
of a particular insecticide. Interest is focussed on how mortality is 
related to dose.

It seems sensible to model the proportion of beetles killed in each
group as a binomial random variable with probability of death
depending on dose. Let $p_i$ denote the probability of death in the $i$th group, 
$i=1, \ldots, 8.$

Pretend  that we  do not know the GLM theory and are just going to
use the normal linear models. We issue the following  commands.

```{r}
beet_lm <- lm(killed / exposed ~ dose , data = beetle, weights = exposed)
summary(beet_lm)
predict(beet_lm)
```

The predicted  probability for dose 1.88 is **1.085304**. This is not the work
of a good statistician! A probability cannot be bigger than 1.

We should fit  the logistic regression model.
A *logistic regression model* is
$$
Y_i | n_i, p_i \sim \text{Binomial}(n_i,p_i),\quad
\text{logit}(p_i) \equiv
\log\left({{p_i}\over{1-p_i}}\right)=\beta_1+\beta_2 x_i, \quad i=1,\ldots ,8,
$$
where $Y_i$ is the number of beetles `killed`, $n_i$ is the number of beetles
`exposed` and $x_i$ is the dose, for the $i$th row of the dataset.

This is a generalised linear model with a binomial distribution for
the response and logistic link function.

The quantity $\frac{p_i}{1-p_i}$ is called the odds of the event whose probability
is $p_i$, i.e. odds of death. The interpretation of the above model is that the
logarithm of the odds of death is linear in $x$, the covariate `dose`.

To illustrate the effect the logit link function has, it is useful to plot
$y/n$ against $x$ and compare with a plot of the *empirical logit*,
$\text{logit}(y/n)$ against $x$.
In fact, because there are extreme values $y/n=1$ in the data, we
use the *modified* empirical logit, logit$([y+{1\over 2}]/[n+1])$ in the
second plot. Does it look as if a linear logistic model will fit well?
The commands are:
```{r}
plot(beetle$dose, beetle$killed / beetle$exposed)
p <- (beetle$dose + 0.5) / (beetle$exposed + 1)
plot(beetle$dose, log(p/(1-p)))
```


To fit the linear logistic regression model in `R`, we use the command
```{r}
beet_glm <- glm(killed/exposed ~ dose, data=beetle, family=binomial, weights=exposed)
```

The response variable for a binomial GLM can be given in one of several formats.
It can just be a factor with two levels (one for success and the other for failure) when the
weights need not be satisfied. Here we have `killed` out of total `exposed`, hence we have used this
format, i.e. the ratio `killed / exposed` with the weights `exposed`.
See `?glm`.


## Interpreting the output


Issue the command 
```{r}
summary(beet_glm)
```
It provides
similar information to the equivalent command for a linear model.

\bitem
\item `Call` confirms the model which was fitted.
\item `Deviance Residuals` gives a summary of the distribution of the
*deviance* residuals. We will discuss these later.
\item `Coefficients` provide the maximum likelihood estimates,
together with their standard errors. The column `z value` should be
 compared to the cut-off point from the standard normal distribution,
e.g. 1.96 at the $5\%$ level of significance.  Using the asymptotic
normality of the MLE, we can obtain confidence intervals as well.
\eitem

We interpret the coefficient for $x_i$ (`dose`) as follows.
We fitted  the model:
$$
\log \left( \frac{ p(x_i)}{1-p(x_i)} \right)
= \beta_1 + \beta_2 \, x_i = -60.72 + 34.27 x_i
$$
The maximum likelihood (ML) estimate of $\beta_1$ is $\hat{\beta_1} = -60.72$ with s.e.
(asymptotic standard error) 5.18.  From this we can test $H_0: \beta_1 =0$
by performing a normal test.  We will reject $H_0$ if
$|\hat{\beta_1}/\text{s.e.}| > 1.96$. In this case we do reject $H_0$ at $5\%$
level of significance.

The ML estimate of $\beta_2$ is $\hat{\beta}_2 = 34.27$ with s.e.
2.91.  Using the asymptotic theory the 95\% CI for $\beta_2$ is
$(34.27 \pm 1.96 \times 2.91) = (28.57, 39.97)$ which shows that $\beta_2$ is significant.
Thus the dose significantly affects the probability of death.

We further interpret the estimate  as follows. We first examine the model for two
values of dose $x$, 
$x_1$ and $x_2$:
\begin{align*}
\log \left( \frac{ p(x_2)}{1-p(x_2)} \right) &= \beta_1 + \beta_2 \, x_2 \\
&= \beta_1 + \beta_2 \, x_1 + \beta_2 (x_2 - x_1) \\
&=  \log \left( \frac{ p(x_1)}{1-p(x_1)} \right) +   \beta_2 (x_2 - x_1).
\end{align*}
By exponentiating both sides,
\begin{align*}
\frac{p(x_2)}{1-p(x_2)} &=   \frac{ p(x_1)}{1-p(x_1) } e^{ \beta_2 (x_2 - x_1)} \\
   &=   \frac{ p(x_1)}{1-p(x_1) } \, e^{ \beta_2}. \\
\text{Odds of death for $x_2$}  &= \text{Odds of death for $(x_2-1)$} \times  e^{\beta_2}.
\end{align*}
This is summarised by saying that the odds of death gets multiplied by $\exp(\beta_2)$ for each
unit increase in $x$.

Returning to this example, for a unit increase in $x$ (dose),
the estimated odds of death of a beetle are multiplied by `exp(34.27)`.  For example,
consider two beetles  with $x= 1.7$ and 1.8 respectively.  The odds of death of the second beetle
is `exp(34.27 * 0.1)` times the odds of death for the first beetle.
This means that the second beetle with a higher dose is *significantly*
more likely to be killed than the first beetle with a lower dose.

We can write the word significant here since $\beta_2$ is significant. A negatively significant
$\beta_2$ would have reduced the odds and the probability of the event for a unit increase in the
covariate.

`Number of Fisher Scoring Iterations` tells us how quickly the
Fisher scoring algorithm converged to the maximum likelihood estimate.

Issue the command 
```{r}
anova(beet_glm, test="Chisq")
```
It provides
similar information to the equivalent command for a linear model. It tests
whether it is worth including `dose` in the model.

Let us see  the fitted probabilities.  We issue
```{r}
predict(beet_glm, type="response")
```
All probabilities are between 0 and 1.

## Other link functions
`R` allows us to use link functions other than the canonical link.
For example, for binomial data, we can use
$$
g(\mu)=\Phi^{-1}(\mu)
$$
where $\Phi$ is the standard normal distribution function, so
$\Phi(z)=P(Z\le z)$ where $Z$ is a standard normal random variable.
This is the *probit* link function.
Alternatively
$$
g(\mu)=\log[-\log(1-\mu)]
$$
is called the *complementary log-log link function*.
Note that both of these links map $(0,1)$ on to the real line.

Create the following function:

```{r}
comparelinks <- function()
{
   x <- seq(from = -5., to = 5., length = 200.)
   y <- pnorm(x)
   z <- 1./(1. + exp( - x))
   compl <- 1- exp(-exp(x))
   plot(x, y, type = "l", main = "Comparison of Probit, Logit and Clog", col=2)
   lines(x, z, lty = 2., col=4)
   lines(x, compl, lty=3, col=6)
   legend(-4., 0.8, legend = c("Probit", "Logit", "cloglog"),
               lty = c(1., 2., 3), col=c(2, 4, 6))
}
```

Use the function `comparelinks()` to see the differences between these
link functions.

Try fitting the  models using these alternative link functions.
For example,

```{r}
beet_prlink <- glm(killed/exposed ~ dose, data = beetle, family = binomial(probit), 
                   weights = exposed)
beet_cloglinklink <- glm(killed/exposed ~ dose, data = beetle, 
                         family = binomial(cloglog), weights = exposed)
summary(beet_prlink)
summary(beet_cloglinklink)
summary(beet_glm)
```



