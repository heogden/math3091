<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Comparing statistical models | MATH3091: Statistical Modelling II</title>
  <meta name="description" content="The course notes for MATH3091: Statistical Modelling II" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Comparing statistical models | MATH3091: Statistical Modelling II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Comparing statistical models | MATH3091: Statistical Modelling II" />
  
  <meta name="twitter:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference.html">
<link rel="next" href="lm.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH3012: Statistical Modelling II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#elements-of-statistical-modelling"><i class="fa fa-check"></i><b>1.1</b> Elements of statistical modelling</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#regression-models"><i class="fa fa-check"></i><b>1.2</b> Regression models</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#example-data-to-be-analysed"><i class="fa fa-check"></i><b>1.3</b> Example data to be analysed</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#nitric-nitric-acid"><i class="fa fa-check"></i><b>1.3.1</b> <code>nitric</code>: Nitric acid</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#birth-weight-of-newborn-babies"><i class="fa fa-check"></i><b>1.3.2</b> <code>birth</code>: Weight of newborn babies</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#survival-time-to-death"><i class="fa fa-check"></i><b>1.3.3</b> <code>survival</code>: Time to death</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#beetle-mortality-from-carbon-disulphide"><i class="fa fa-check"></i><b>1.3.4</b> <code>beetle</code>: Mortality from carbon disulphide</a></li>
<li class="chapter" data-level="1.3.5" data-path="intro.html"><a href="intro.html#shuttle-challenger-disaster"><i class="fa fa-check"></i><b>1.3.5</b> <code>shuttle</code>: Challenger disaster</a></li>
<li class="chapter" data-level="1.3.6" data-path="intro.html"><a href="intro.html#heart-treatment-for-heart-attack"><i class="fa fa-check"></i><b>1.3.6</b> <code>heart</code>: Treatment for heart attack</a></li>
<li class="chapter" data-level="1.3.7" data-path="intro.html"><a href="intro.html#accident-road-traffic-accidents"><i class="fa fa-check"></i><b>1.3.7</b> <code>accident</code>: Road traffic accidents</a></li>
<li class="chapter" data-level="1.3.8" data-path="intro.html"><a href="intro.html#lymphoma-lymphoma-patients"><i class="fa fa-check"></i><b>1.3.8</b> <code>lymphoma</code>: Lymphoma patients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Parametric statistical inference</a><ul>
<li class="chapter" data-level="2.1" data-path="inference.html"><a href="inference.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="inference.html"><a href="inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>2.2</b> The likelihood function</a></li>
<li class="chapter" data-level="2.3" data-path="inference.html"><a href="inference.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4" data-path="inference.html"><a href="inference.html#score"><i class="fa fa-check"></i><b>2.4</b> Score</a></li>
<li class="chapter" data-level="2.5" data-path="inference.html"><a href="inference.html#info"><i class="fa fa-check"></i><b>2.5</b> Information</a></li>
<li class="chapter" data-level="2.6" data-path="inference.html"><a href="inference.html#sn:asnmle"><i class="fa fa-check"></i><b>2.6</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="2.7" data-path="inference.html"><a href="inference.html#quantifying-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>2.7</b> Quantifying uncertainty in parameter estimates</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html"><i class="fa fa-check"></i><b>3</b> Comparing statistical models</a><ul>
<li class="chapter" data-level="3.1" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.3" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#sn:lrt"><i class="fa fa-check"></i><b>3.3</b> Likelihood ratio tests for nested hypotheses</a></li>
<li class="chapter" data-level="3.4" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#information-criteria-for-model-comparison"><i class="fa fa-check"></i><b>3.4</b> Information criteria for model comparison</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#the-linear-model"><i class="fa fa-check"></i><b>4.1</b> The linear model</a></li>
<li class="chapter" data-level="4.2" data-path="lm.html"><a href="lm.html#eglm"><i class="fa fa-check"></i><b>4.2</b> Examples of linear model structure</a></li>
<li class="chapter" data-level="4.3" data-path="lm.html"><a href="lm.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>4.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="4.4" data-path="lm.html"><a href="lm.html#properties-of-the-mle"><i class="fa fa-check"></i><b>4.4</b> Properties of the MLE</a></li>
<li class="chapter" data-level="4.5" data-path="lm.html"><a href="lm.html#comparing-linear-models"><i class="fa fa-check"></i><b>4.5</b> Comparing linear models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>5</b> Generalised Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="glm.html"><a href="glm.html#regression-models-for-non-normal-data"><i class="fa fa-check"></i><b>5.1</b> Regression models for non-normal data</a></li>
<li class="chapter" data-level="5.2" data-path="glm.html"><a href="glm.html#sn:ef"><i class="fa fa-check"></i><b>5.2</b> The exponential family</a></li>
<li class="chapter" data-level="5.3" data-path="glm.html"><a href="glm.html#components-of-a-generalised-linear-model"><i class="fa fa-check"></i><b>5.3</b> Components of a generalised linear model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="glm.html"><a href="glm.html#the-random-component"><i class="fa fa-check"></i><b>5.3.1</b> The random component</a></li>
<li class="chapter" data-level="5.3.2" data-path="glm.html"><a href="glm.html#the-systematic-or-structural-component"><i class="fa fa-check"></i><b>5.3.2</b> The systematic (or structural) component</a></li>
<li class="chapter" data-level="5.3.3" data-path="glm.html"><a href="glm.html#the-link-function"><i class="fa fa-check"></i><b>5.3.3</b> The link function</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="glm.html"><a href="glm.html#examples-of-generalised-linear-models"><i class="fa fa-check"></i><b>5.4</b> Examples of generalised linear models</a><ul>
<li class="chapter" data-level="5.4.1" data-path="glm.html"><a href="glm.html#the-linear-model-1"><i class="fa fa-check"></i><b>5.4.1</b> The linear model</a></li>
<li class="chapter" data-level="5.4.2" data-path="glm.html"><a href="glm.html#models-for-binary-data"><i class="fa fa-check"></i><b>5.4.2</b> Models for binary data</a></li>
<li class="chapter" data-level="5.4.3" data-path="glm.html"><a href="glm.html#sn:count"><i class="fa fa-check"></i><b>5.4.3</b> Models for count data</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="glm.html"><a href="glm.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>5.5</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="5.6" data-path="glm.html"><a href="glm.html#sn:glminfer"><i class="fa fa-check"></i><b>5.6</b> Confidence intervals</a></li>
<li class="chapter" data-level="5.7" data-path="glm.html"><a href="glm.html#sn:compglm"><i class="fa fa-check"></i><b>5.7</b> Comparing generalised linear models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="glm.html"><a href="glm.html#sn:glmlrt"><i class="fa fa-check"></i><b>5.7.1</b> The likelihood ratio test</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="glm.html"><a href="glm.html#scaled-deviance-and-the-saturated-model"><i class="fa fa-check"></i><b>5.8</b> Scaled deviance and the saturated model</a></li>
<li class="chapter" data-level="5.9" data-path="glm.html"><a href="glm.html#sn:unknowndisp"><i class="fa fa-check"></i><b>5.9</b> Models with unknown <span class="math inline">\(a(\phi)\)</span></a></li>
<li class="chapter" data-level="5.10" data-path="glm.html"><a href="glm.html#residuals"><i class="fa fa-check"></i><b>5.10</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-categorical.html"><a href="chap-categorical.html"><i class="fa fa-check"></i><b>6</b> Models for categorical data</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-categorical.html"><a href="chap-categorical.html#contingency-tables"><i class="fa fa-check"></i><b>6.1</b> Contingency tables</a></li>
<li class="chapter" data-level="6.2" data-path="chap-categorical.html"><a href="chap-categorical.html#log-linear-models"><i class="fa fa-check"></i><b>6.2</b> Log-linear models</a></li>
<li class="chapter" data-level="6.3" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:multinomial"><i class="fa fa-check"></i><b>6.3</b> Multinomial sampling</a></li>
<li class="chapter" data-level="6.4" data-path="chap-categorical.html"><a href="chap-categorical.html#product-multinomial-sampling"><i class="fa fa-check"></i><b>6.4</b> Product multinomial sampling</a></li>
<li class="chapter" data-level="6.5" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:loginter"><i class="fa fa-check"></i><b>6.5</b> Interpreting log-linear models for two-way tables</a></li>
<li class="chapter" data-level="6.6" data-path="chap-categorical.html"><a href="chap-categorical.html#interpreting-log-linear-models-for-multiway-tables"><i class="fa fa-check"></i><b>6.6</b> Interpreting log-linear models for multiway tables</a></li>
<li class="chapter" data-level="6.7" data-path="chap-categorical.html"><a href="chap-categorical.html#simpsons-paradox"><i class="fa fa-check"></i><b>6.7</b> Simpson’s paradox</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3091: Statistical Modelling II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="comparing-statistical-models" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Comparing statistical models</h1>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>If we have a set of competing probability models which might have generated the observed data, we may want to determine which of the models is most appropriate. In practice, we proceed by comparing models pairwise. Suppose that we have two competing alternatives, <span class="math inline">\(f^{(0)}_{\bm Y}\)</span> (model <span class="math inline">\(M_0\)</span>) and <span class="math inline">\(f^{(1)}_{\bm Y}\)</span> (model <span class="math inline">\(M_1\)</span>) for <span class="math inline">\(f_{\bm Y}\)</span>, the joint distribution of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>. Often <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> both take the same parametric form, <span class="math inline">\(f_{\bm Y}(\bm{y};\bm \theta)\)</span> but with <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> for <span class="math inline">\(H_0\)</span> and <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span> for <span class="math inline">\(H_1\)</span>, where <span class="math inline">\(\Theta^{(0)}\)</span> and <span class="math inline">\(\Theta^{(1)}\)</span> are alternative sets of possible values for <span class="math inline">\(\bm \theta\)</span>. In the regression setting, we are often interested in determining which of a set of explanatory variables have an impact on the distribution of the response.</p>
</div>
<div id="hypothesis-testing" class="section level2">
<h2><span class="header-section-number">3.2</span> Hypothesis testing</h2>
<p>A hypothesis test provides one mechanism for comparing two competing statistical models. A hypothesis test does not treat the two hypotheses (models) symmetrically. One hypothesis, <span class="math display">\[\text{$H_0$: the data were generated from model $M_0$},\]</span> is accorded special status, and referred to as the <em>null hypothesis</em>. The null hypothesis is the reference model, and will be assumed to be appropriate unless the observed data strongly indicate that <span class="math inline">\(H_0\)</span> is inappropriate, and that <span class="math display">\[\text{$H_1$: the data were generated from model $M_1$},\]</span> (the <em>alternative</em> hypothesis) should be preferred. The fact that a hypothesis test does not reject <span class="math inline">\(H_0\)</span> should not be taken as evidence that <span class="math inline">\(H_0\)</span> is true and <span class="math inline">\(H_1\)</span> is not, or that <span class="math inline">\(H_0\)</span> is better supported by the data than <span class="math inline">\(H_1\)</span>, merely that the data does not provide sufficient evidence to reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>.</p>
<p>A hypothesis test is defined by its <em>critical region</em> or <em>rejection region</em>, which we shall denote by <span class="math inline">\(C\)</span>. <span class="math inline">\(C\)</span> is a subset of <span class="math inline">\(\mathbb{R}^n\)</span> and is the set of possible <span class="math inline">\(\bm{y}\)</span> which would lead to rejection of <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>, <em>i.e.</em></p>
<ul>
<li>If <span class="math inline">\(\bm{y} \in C\)</span>, <span class="math inline">\(H_0\)</span> is rejected in favour of <span class="math inline">\(H_1\)</span>;</li>
<li>If <span class="math inline">\(\bm{y} \not\in C\)</span>, <span class="math inline">\(H_0\)</span> is not rejected.</li>
</ul>
<p>As <span class="math inline">\(\bm Y\)</span> is a random variable, there remains the possibility that a hypothesis test will produce an erroneous result. We define the <em>size</em> (or <em>significance level</em>) of the test <span class="math display">\[\alpha = \max_{\bm \theta\in\Theta^{(0)}}P(\bm Y\in C;\bm \theta)\]</span> This is the maximum probability of erroneously rejecting <span class="math inline">\(H_0\)</span>, over all possible distributions for <span class="math inline">\(\bm Y\)</span> implied by <span class="math inline">\(H_0\)</span>. We also define the power function <span class="math display">\[\omega(\bm \theta)= P(\bm Y\in C;\bm \theta)\]</span> It represents the probability of rejecting <span class="math inline">\(H_0\)</span> for a particular value of <span class="math inline">\(\bm \theta\)</span>. Clearly we would like to find a test with where <span class="math inline">\(\omega(\bm \theta)\)</span> is large for every <span class="math inline">\(\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}\)</span>, while at the same time avoiding erroneous rejection of <span class="math inline">\(H_0\)</span>. In other words, a good test will have small size, but large power.</p>
<p>The general hypothesis testing procedure is to fix <span class="math inline">\(\alpha\)</span> to be some small value (often 0.05), so that the probability of erroneous rejection of <span class="math inline">\(H_0\)</span> is limited. In doing this, we are giving <span class="math inline">\(H_0\)</span> precedence over <span class="math inline">\(H_1\)</span>. Given our specified <span class="math inline">\(\alpha\)</span>, we try to choose a test, defined by its rejection region <span class="math inline">\(C\)</span>, to make <span class="math inline">\(\omega(\bm \theta)\)</span> as large as possible for <span class="math inline">\(\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}\)</span>.</p>
</div>
<div id="sn:lrt" class="section level2">
<h2><span class="header-section-number">3.3</span> Likelihood ratio tests for nested hypotheses</h2>
Suppose that <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> both take the same parametric form, <span class="math inline">\(f_{\bm Y}(\bm{y};\bm \theta)\)</span> with <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> for <span class="math inline">\(H_0\)</span> and <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span> for <span class="math inline">\(H_1\)</span>, where <span class="math inline">\(\Theta^{(0)}\)</span> and <span class="math inline">\(\Theta^{(1)}\)</span> are alternative sets of possible values for <span class="math inline">\(\bm \theta\)</span>. A <em>likelihood ratio test</em> of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span> has a critical region of the form
<span class="math display" id="eq:lrtCritical">\[\begin{equation}
C=\left\{ \bm{y}: 
\frac{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)} 
&gt;k\right\}
\tag{3.1}
\end{equation}\]</span>
<p>where <span class="math inline">\(k\)</span> is determined by <span class="math inline">\(\alpha\)</span>, the size of the test, so <span class="math display">\[\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.\]</span> Therefore, we will only reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(H_1\)</span> offers a distribution for <span class="math inline">\(Y_1, \ldots, Y_n\)</span> which makes the observed data much more probable than any distribution under <span class="math inline">\(H_0\)</span>. This is intuitively appealing and tends to produce good tests (large power) across a wide range of examples.</p>
<p>In order to determine <span class="math inline">\(k\)</span> in <a href="comparing-statistical-models.html#eq:lrtCritical">(3.1)</a>, we need to know the distribution of the likelihood ratio, or an equivalent statistic, under <span class="math inline">\(H_0\)</span>. In general, this will not be available to us. However, we can make use of an important asymptotic result.</p>
<p>First we notice that, as <span class="math inline">\(\log\)</span> is a strictly increasing function, the rejection region is equivalent to <span class="math display">\[
C=\left\{ \bm{y}: 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right) &gt;k&#39;\right\}
\]</span> where <span class="math display">\[
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
\]</span> Write <span class="math display">\[L_{01}\equiv 2\log \left(\frac{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}
  {\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}\right)\]</span> for the <em>log-likelihood ratio</em> test statistic. Provided that <span class="math inline">\(H_0\)</span> is <em>nested within</em> <span class="math inline">\(H_1\)</span>, the following result provides a useful large-<span class="math inline">\(n\)</span> approximation to the distribution of <span class="math inline">\(L_{01}\)</span>.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-21" class="theorem"><strong>Theorem 3.1  </strong></span>Suppose that <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> and <span class="math inline">\(H_1\)</span>: <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span>, where <span class="math inline">\(\Theta^{(0)}\subset\Theta^{(1)}\)</span>. Let <span class="math inline">\(d_0 = \dim(\Theta^{(0)})\)</span> and <span class="math inline">\(d_1 = \dim(\Theta^{(1)})\)</span>. Under <span class="math inline">\(H_0\)</span>, the distribution of <span class="math inline">\(L_{01}\)</span> tends towards <span class="math inline">\(\chi^2_{d_1 - d_0}\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> First we note that in the case where <span class="math inline">\(\bm \theta\)</span> is one-dimensional and <span class="math inline">\(\bm \theta=(\theta)\)</span>, a Taylor series expansion of <span class="math inline">\(\ell(\theta)\)</span> around the MLE <span class="math inline">\(\hat{\theta}\)</span> gives <span class="math display">\[
\ell(\theta)=\ell(\hat{\theta})+(\theta-\hat{\theta})
U(\hat{\theta})+{1\over 2}(\theta-\hat{\theta})^2 U&#39;(\hat{\theta}) +\;\ldots
\]</span> Now, <span class="math inline">\(U(\hat{\theta})=0\)</span>, and if we approximate <span class="math inline">\(U&#39;(\hat{\theta})\equiv H(\hat{\theta})\)</span> by <span class="math inline">\(E[H(\theta)]\equiv -{\cal I}(\theta)\)</span>, and also ignore higher order terms, we obtain <span class="math display">\[
2[\ell (\hat{\theta})-\ell(\theta)]=
(\theta-\hat{\theta})^2 {\cal I}(\theta)
\]</span> As <span class="math inline">\(\hat{\theta}\)</span> is asymptotically <span class="math inline">\(N[\theta,{\cal I}(\theta)^{-1}]\)</span>, <span class="math inline">\((\theta-\hat{\theta})^2 {\cal I}(\theta)\)</span> is asymptotically <span class="math inline">\(\chi^2_1\)</span>, and hence so is <span class="math inline">\(2[\ell(\hat \theta)-\ell (\theta)]\)</span>.</p>
<p>Similarly it can be shown that when <span class="math inline">\(\bm \theta\in\Theta\)</span>, a multidimensional space, <span class="math inline">\(2[\ell(\bm{\hat \theta})-\ell (\bm \theta)]\)</span> is asymptotically <span class="math inline">\(\chi^2_p\)</span>, where <span class="math inline">\(p\)</span> is the dimension of <span class="math inline">\(\Theta\)</span>.</p>
Now, suppose that <span class="math inline">\(H_0\)</span> is true and <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> and therefore <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span>. Furthermore, suppose that <span class="math inline">\(\ell(\bm \theta)\)</span> is maximised in <span class="math inline">\(\Theta^{(0)}\)</span> by <span class="math inline">\(\hat{\bm \theta}^{(0)}\)</span> and is maximised in <span class="math inline">\(\Theta^{(1)}\)</span> by <span class="math inline">\(\hat{\bm \theta}^{(1)}\)</span>. Then
<span class="math display">\[\begin{align*}
L_{01}&amp;\equiv  2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)\cr
&amp;= 2\log L(\hat{\bm \theta}^{(1)})-2\log L(\hat{\bm \theta}^{(0)})\cr
&amp;= 2[\log L(\hat{\bm \theta}^{(1)})-\log L(\bm \theta)]
 -2[\log L(\hat{\bm \theta}^{(0)})-\log L(\bm \theta)]\cr
&amp;= L_1-L_0.
\end{align*}\]</span>
Therefore <span class="math inline">\(L_1=L_{01}+L_0\)</span> and we know that, under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(L_1\)</span> has a <span class="math inline">\(\chi^2_{d_1}\)</span> distribution and <span class="math inline">\(L_0\)</span> has a <span class="math inline">\(\chi^2_{d_0}\)</span> distribution. Furthermore, it is possible to show (although we will not do so here) that under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(L_{01}\)</span> and <span class="math inline">\(L_0\)</span> are independent. It can also be shown that under <span class="math inline">\(H_0\)</span> the difference <span class="math inline">\(L_1-L_0\)</span> can be expressed as a quadratic form of normal random variables. Therefore, it follows that under <span class="math inline">\(H_0\)</span>, the log likelihood ratio statistic <span class="math inline">\(L_{01}\)</span> has a <span class="math inline">\(\chi^2_{d_1-d_0}\)</span> distribution.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-23" class="example"><strong>Example 3.1  (Bernoulli)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Suppose that we require a size <span class="math inline">\(\alpha\)</span> test of the hypothesis <span class="math inline">\(H_0\)</span>: <span class="math inline">\(p=p_0\)</span> against the general alternative <span class="math inline">\(H_1\)</span>: ‘<span class="math inline">\(p\)</span> is unrestricted’ where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(p_0\)</span> are specified.</p>
Here <span class="math inline">\(\bm \theta=(p)\)</span>, <span class="math inline">\(\Theta^{(0)}=\{p_0\}\)</span> and <span class="math inline">\(\Theta^{(1)}=(0,1)\)</span> and the log likelihood ratio statistic is <span class="math display">\[
L_{01}=2n\bar y\log\left({{\bar y}\over{p_0}}\right)
+2n(1-\bar y)\log\left({{1-\bar y}\over{1-p_0}}\right).
\]</span> As <span class="math inline">\(d_1=1\)</span> and <span class="math inline">\(d_0=0\)</span>, under <span class="math inline">\(H_0\)</span>, the log likelihood ratio statistic has an asymptotic <span class="math inline">\(\chi^2_1\)</span> distribution. For a log likelihood ratio test, we only reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> when the test statistic is too large (observed data are much more probable under model <span class="math inline">\(H_1\)</span> than under model <span class="math inline">\(H_0\)</span>), so in this case we reject <span class="math inline">\(H_0\)</span> when the observed value of the test statistic above is ‘too large’ to have come from a <span class="math inline">\(\chi^2_1\)</span> distribution. What we mean by ‘too large’ depends on the significance level <span class="math inline">\(\alpha\)</span> of the test. For example, if <span class="math inline">\(\alpha=0.05\)</span>, a common choice, then we should reject <span class="math inline">\(H_0\)</span> if the test statistic is greater than the 3.84, the 95% point of the <span class="math inline">\(\chi^2_1\)</span> distribution.
</div>

</div>
<div id="information-criteria-for-model-comparison" class="section level2">
<h2><span class="header-section-number">3.4</span> Information criteria for model comparison</h2>
<p>It is more difficult to use the likelihood ratio test of Section <a href="comparing-statistical-models.html#sn:lrt">3.3</a> to compare two models if those models are not nested. An alternative approach is to record some criterion measuring the quality of the model for each of a candidate set of models, then choose the model which is the best according to this criterion.</p>
<p>When we were estimating the unknown parameters <span class="math inline">\(\theta\)</span> of a model, we chose the value which maximised the likelihood: that is, the value of <span class="math inline">\(\theta\)</span> that maximises the probability of observing the data we actually saw. It is tempting to use a similar system for choosing between two models, and to choose the model which has the greater likelihood, under which the probability of seeing the data we actually observed is maximised. However, if we do this we will always end up choosing complicated models, which fit the observed data very closely, but do not meet our requirement of parsimony.</p>
<p>For a given model depending on parameters <span class="math inline">\(\theta \in \mathbb{R}^p\)</span>, let <span class="math inline">\(\hat L\)</span> be the likelihood function for that model evaluated at the MLE <span class="math inline">\(\hat \theta\)</span>. It is not sensible to choose between models by maximising <span class="math inline">\(\hat L\)</span> directly, and instead it is common to choose a model to maximise a criteria of the form <span class="math display">\[\hat L - \text{penalty},\]</span> where the penalty term will be large for complex models, and small for simple models.</p>
<p>Equivalently, we may choose between models by minimising a criteria of the form <span class="math display">\[ - 2 \hat L + \text{penalty}.\]</span> By convention, many commonly-used criteria for model comparison take this form. For instance, the Akaike information criterion (AIC) is <span class="math display">\[\text{AIC} = - 2 \hat L + 2 p,\]</span> where <span class="math inline">\(p\)</span> is the dimension of the unknown parameter in the candidate model, and the Bayesian information criterion (BIC) is <span class="math display">\[\text{BIC} = - 2 \hat L + \log(n) p,\]</span> where <span class="math inline">\(n\)</span> is the number of observations.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/heogden/math3091/edit/master/model_comparison.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH3091.pdf", "MATH3091.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
