<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Parametric statistical inference | MATH3091: Statistical Modelling II</title>
  <meta name="description" content="The course notes for MATH3091: Statistical Modelling II" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Parametric statistical inference | MATH3091: Statistical Modelling II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Parametric statistical inference | MATH3091: Statistical Modelling II" />
  
  <meta name="twitter:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html">
<link rel="next" href="comparing-statistical-models.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH3012: Statistical Modelling II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#elements-of-statistical-modelling"><i class="fa fa-check"></i><b>1.1</b> Elements of statistical modelling</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#regression-models"><i class="fa fa-check"></i><b>1.2</b> Regression models</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#example-data-to-be-analysed"><i class="fa fa-check"></i><b>1.3</b> Example data to be analysed</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#nitric-nitric-acid"><i class="fa fa-check"></i><b>1.3.1</b> <code>nitric</code>: Nitric acid</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#birth-weight-of-newborn-babies"><i class="fa fa-check"></i><b>1.3.2</b> <code>birth</code>: Weight of newborn babies</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#survival-time-to-death"><i class="fa fa-check"></i><b>1.3.3</b> <code>survival</code>: Time to death</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#beetle-mortality-from-carbon-disulphide"><i class="fa fa-check"></i><b>1.3.4</b> <code>beetle</code>: Mortality from carbon disulphide</a></li>
<li class="chapter" data-level="1.3.5" data-path="intro.html"><a href="intro.html#shuttle-challenger-disaster"><i class="fa fa-check"></i><b>1.3.5</b> <code>shuttle</code>: Challenger disaster</a></li>
<li class="chapter" data-level="1.3.6" data-path="intro.html"><a href="intro.html#heart-treatment-for-heart-attack"><i class="fa fa-check"></i><b>1.3.6</b> <code>heart</code>: Treatment for heart attack</a></li>
<li class="chapter" data-level="1.3.7" data-path="intro.html"><a href="intro.html#accident-road-traffic-accidents"><i class="fa fa-check"></i><b>1.3.7</b> <code>accident</code>: Road traffic accidents</a></li>
<li class="chapter" data-level="1.3.8" data-path="intro.html"><a href="intro.html#lymphoma-lymphoma-patients"><i class="fa fa-check"></i><b>1.3.8</b> <code>lymphoma</code>: Lymphoma patients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Parametric statistical inference</a><ul>
<li class="chapter" data-level="2.1" data-path="inference.html"><a href="inference.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="inference.html"><a href="inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>2.2</b> The likelihood function</a></li>
<li class="chapter" data-level="2.3" data-path="inference.html"><a href="inference.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4" data-path="inference.html"><a href="inference.html#score"><i class="fa fa-check"></i><b>2.4</b> Score</a></li>
<li class="chapter" data-level="2.5" data-path="inference.html"><a href="inference.html#info"><i class="fa fa-check"></i><b>2.5</b> Information</a></li>
<li class="chapter" data-level="2.6" data-path="inference.html"><a href="inference.html#sn:asnmle"><i class="fa fa-check"></i><b>2.6</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="2.7" data-path="inference.html"><a href="inference.html#quantifying-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>2.7</b> Quantifying uncertainty in parameter estimates</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html"><i class="fa fa-check"></i><b>3</b> Comparing statistical models</a><ul>
<li class="chapter" data-level="3.1" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.3" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#sn:lrt"><i class="fa fa-check"></i><b>3.3</b> Likelihood ratio tests for nested hypotheses</a></li>
<li class="chapter" data-level="3.4" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#information-criteria-for-model-comparison"><i class="fa fa-check"></i><b>3.4</b> Information criteria for model comparison</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#the-linear-model"><i class="fa fa-check"></i><b>4.1</b> The linear model</a></li>
<li class="chapter" data-level="4.2" data-path="lm.html"><a href="lm.html#eglm"><i class="fa fa-check"></i><b>4.2</b> Examples of linear model structure</a></li>
<li class="chapter" data-level="4.3" data-path="lm.html"><a href="lm.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>4.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="4.4" data-path="lm.html"><a href="lm.html#properties-of-the-mle"><i class="fa fa-check"></i><b>4.4</b> Properties of the MLE</a></li>
<li class="chapter" data-level="4.5" data-path="lm.html"><a href="lm.html#comparing-linear-models"><i class="fa fa-check"></i><b>4.5</b> Comparing linear models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>5</b> Generalised Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="glm.html"><a href="glm.html#regression-models-for-non-normal-data"><i class="fa fa-check"></i><b>5.1</b> Regression models for non-normal data</a></li>
<li class="chapter" data-level="5.2" data-path="glm.html"><a href="glm.html#sn:ef"><i class="fa fa-check"></i><b>5.2</b> The exponential family</a></li>
<li class="chapter" data-level="5.3" data-path="glm.html"><a href="glm.html#components-of-a-generalised-linear-model"><i class="fa fa-check"></i><b>5.3</b> Components of a generalised linear model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="glm.html"><a href="glm.html#the-random-component"><i class="fa fa-check"></i><b>5.3.1</b> The random component</a></li>
<li class="chapter" data-level="5.3.2" data-path="glm.html"><a href="glm.html#the-systematic-or-structural-component"><i class="fa fa-check"></i><b>5.3.2</b> The systematic (or structural) component</a></li>
<li class="chapter" data-level="5.3.3" data-path="glm.html"><a href="glm.html#the-link-function"><i class="fa fa-check"></i><b>5.3.3</b> The link function</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="glm.html"><a href="glm.html#examples-of-generalised-linear-models"><i class="fa fa-check"></i><b>5.4</b> Examples of generalised linear models</a><ul>
<li class="chapter" data-level="5.4.1" data-path="glm.html"><a href="glm.html#the-linear-model-1"><i class="fa fa-check"></i><b>5.4.1</b> The linear model</a></li>
<li class="chapter" data-level="5.4.2" data-path="glm.html"><a href="glm.html#models-for-binary-data"><i class="fa fa-check"></i><b>5.4.2</b> Models for binary data</a></li>
<li class="chapter" data-level="5.4.3" data-path="glm.html"><a href="glm.html#sn:count"><i class="fa fa-check"></i><b>5.4.3</b> Models for count data</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="glm.html"><a href="glm.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>5.5</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="5.6" data-path="glm.html"><a href="glm.html#sn:glminfer"><i class="fa fa-check"></i><b>5.6</b> Confidence intervals</a></li>
<li class="chapter" data-level="5.7" data-path="glm.html"><a href="glm.html#sn:compglm"><i class="fa fa-check"></i><b>5.7</b> Comparing generalised linear models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="glm.html"><a href="glm.html#sn:glmlrt"><i class="fa fa-check"></i><b>5.7.1</b> The likelihood ratio test</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="glm.html"><a href="glm.html#scaled-deviance-and-the-saturated-model"><i class="fa fa-check"></i><b>5.8</b> Scaled deviance and the saturated model</a></li>
<li class="chapter" data-level="5.9" data-path="glm.html"><a href="glm.html#sn:unknowndisp"><i class="fa fa-check"></i><b>5.9</b> Models with unknown <span class="math inline">\(a(\phi)\)</span></a></li>
<li class="chapter" data-level="5.10" data-path="glm.html"><a href="glm.html#residuals"><i class="fa fa-check"></i><b>5.10</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-categorical.html"><a href="chap-categorical.html"><i class="fa fa-check"></i><b>6</b> Models for categorical data</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-categorical.html"><a href="chap-categorical.html#contingency-tables"><i class="fa fa-check"></i><b>6.1</b> Contingency tables</a></li>
<li class="chapter" data-level="6.2" data-path="chap-categorical.html"><a href="chap-categorical.html#log-linear-models"><i class="fa fa-check"></i><b>6.2</b> Log-linear models</a></li>
<li class="chapter" data-level="6.3" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:multinomial"><i class="fa fa-check"></i><b>6.3</b> Multinomial sampling</a></li>
<li class="chapter" data-level="6.4" data-path="chap-categorical.html"><a href="chap-categorical.html#product-multinomial-sampling"><i class="fa fa-check"></i><b>6.4</b> Product multinomial sampling</a></li>
<li class="chapter" data-level="6.5" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:loginter"><i class="fa fa-check"></i><b>6.5</b> Interpreting log-linear models for two-way tables</a></li>
<li class="chapter" data-level="6.6" data-path="chap-categorical.html"><a href="chap-categorical.html#interpreting-log-linear-models-for-multiway-tables"><i class="fa fa-check"></i><b>6.6</b> Interpreting log-linear models for multiway tables</a></li>
<li class="chapter" data-level="6.7" data-path="chap-categorical.html"><a href="chap-categorical.html#simpsons-paradox"><i class="fa fa-check"></i><b>6.7</b> Simpson’s paradox</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3091: Statistical Modelling II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="inference" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Parametric statistical inference</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>Probability distributions like the binomial, Poisson and normal, enable us to calculate probabilities, and other quantities of interest (e.g. expectations) for a probability model of a random process. Therefore, given the model, we can make statements about possible outcomes of the process.</p>
<p>Statistical inference is concerned with the inverse problem. Given outcomes of a random process (observed data), what conclusions (inferences) can we draw about the process itself?</p>
<p>We assume that the <span class="math inline">\(n\)</span> observations of the response <span class="math inline">\(\bm y=(y_1,\ldots ,y_n)^T\)</span> are observations of random variables <span class="math inline">\(\bm Y=(Y_1,\ldots ,Y_n)^T\)</span>, which have joint p.d.f. <span class="math inline">\(f_{\bm Y}\)</span> (joint p.f. for discrete variables). We use the observed data <span class="math inline">\(\bm y\)</span> to make inferences about <span class="math inline">\(f_{\bm Y}\)</span>.</p>
<p>We usually make certain assumptions about <span class="math inline">\(f_{\bm Y}\)</span>. In particular, we often assume that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <em>independent</em> random variables. Hence <span class="math display">\[
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
\]</span></p>
<p>In parametric statistical inference, we specify a joint distribution <span class="math inline">\(f_{\bm Y}\)</span>, for <span class="math inline">\(\bm Y\)</span>, which is known, except for the values of parameters <span class="math inline">\(\theta_1,\theta_2,\ldots ,\theta_p\)</span> (sometimes denoted by <span class="math inline">\(\bm \theta\)</span>). Then we use the observed data <span class="math inline">\(\bm y\)</span> to make inferences about <span class="math inline">\(\theta_1,\theta_2,\ldots ,\theta_p\)</span>. In this case, we usually write <span class="math inline">\(f_{\bm Y}\)</span> as <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span>, to make explicit the dependence on the unknown <span class="math inline">\(\bm \theta\)</span>.</p>
</div>
<div id="the-likelihood-function" class="section level2">
<h2><span class="header-section-number">2.2</span> The likelihood function</h2>
<p>We often think of the joint density <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span> as a function of <span class="math inline">\(\bm{y}\)</span> for fixed <span class="math inline">\(\bm \theta\)</span>, which describes the relative probabilities of different possible values of <span class="math inline">\(\bm y\)</span>, given a particular set of parameters <span class="math inline">\(\bm \theta\)</span>. However, in statistical inference, we have observed <span class="math inline">\(y_1, \ldots, y_n\)</span> (values of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>). Knowledge of the probability of alternative possible realisations of <span class="math inline">\(\bm Y\)</span> is largely irrelevant. What we want to know about is <span class="math inline">\(\bm \theta\)</span>.</p>
<p>Our only link between the observed data <span class="math inline">\(y_1, \ldots, y_n\)</span> and <span class="math inline">\(\bm \theta\)</span> is through the function <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span>. Therefore, it seems sensible that parametric statistical inference should be based on this function. We can think of <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span> as a function of <span class="math inline">\(\bm \theta\)</span> for fixed <span class="math inline">\(\bm{y}\)</span>, which describes the relative <em>likelihoods</em> of different possible (sets of) <span class="math inline">\(\bm \theta,\)</span> given observed data <span class="math inline">\(y_1, \ldots, y_n\)</span>. We write <span class="math display">\[L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)\]</span> for this <em>likelihood</em>, which is a function of the unknown parameter <span class="math inline">\(\bm \theta\)</span>. For convenience, we often drop <span class="math inline">\(\bm y\)</span> from the notation, and write <span class="math inline">\(L(\bm \theta)\)</span>.</p>
<p>The likelihood function is of central importance in parametric statistical inference. It provides a means for comparing different possible values of <span class="math inline">\(\bm \theta\)</span>, based on the probabilities (or probability densities) that they assign to the observed data <span class="math inline">\(y_1, \ldots, y_n\)</span>.</p>
<div id="notes" class="section level4 unnumbered">
<h4>Notes</h4>
<ol style="list-style-type: decimal">
<li>Frequently it is more convenient to consider the <em>log-likelihood</em> function <span class="math inline">\(\ell(\bm \theta) = \log L(\bm \theta)\)</span>.</li>
<li>Nothing in the definition of the likelihood requires <span class="math inline">\(y_1, \ldots, y_n\)</span> to be observations of independent random variables, although we shall frequently make this assumption.</li>
<li>Any factors which depend on <span class="math inline">\(y_1, \ldots, y_n\)</span> alone (and not on <span class="math inline">\(\bm \theta\)</span>) can be ignored when writing down the likelihood. Such factors give no information about the relative likelihoods of different possible values of <span class="math inline">\(\bm \theta\)</span>.</li>
</ol>

<div class="example">
<span id="exm:unnamed-chunk-8" class="example"><strong>Example 2.1  (Bernoulli)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, independent identically distributed (i.i.d.) Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\theta=(p)\)</span> and the likelihood is <span class="math display">\[
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}.
\]</span> The log-likelihood is <span class="math display">\[
\ell(p) = \log L(p) =n\bar y\log p+n(1-\bar y)\log(1-p).
\]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-9" class="example"><strong>Example 2.2  (Normal)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and the likelihood is
<span class="math display">\[\begin{align*}
L(\mu,\sigma^2) &amp;=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&amp;=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&amp;\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right).
\end{align*}\]</span>
The log-likelihood is <span class="math display">\[\ell(\mu, \sigma^2) = \log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]</span>
</div>

</div>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">2.3</span> Maximum likelihood estimation</h2>
<p>One of the primary tasks of parametric statistical inference is <em>estimation</em> of the unknown parameters <span class="math inline">\(\theta_1, \ldots, \theta_p\)</span>. Consider the value of <span class="math inline">\(\bm \theta\)</span> which maximises the likelihood function. This is the ‘most likely’ value of <span class="math inline">\(\bm \theta\)</span>, the one which makes the observed data ‘most probable’. When we are searching for an estimate of <span class="math inline">\(\bm \theta\)</span>, this would seem to be a good candidate.</p>
<p>We call the value of <span class="math inline">\(\bm \theta\)</span> which maximises the likelihood <span class="math inline">\(L(\theta)\)</span> the <em>maximum likelihood estimate</em> (MLE) of <span class="math inline">\(\bm \theta\)</span>, denoted by <span class="math inline">\(\hat{\bm \theta}\)</span>. <span class="math inline">\(\hat{\bm \theta}\)</span> depends on <span class="math inline">\(\bm y\)</span>, as different observed data samples lead to different likelihood functions. The corresponding function of <span class="math inline">\(\bm Y\)</span> is called the <em>maximum likelihood estimator</em> and is also denoted by <span class="math inline">\(\hat{\bm \theta}\)</span>.</p>
<p>Note that as <span class="math inline">\(\bm \theta=(\theta_1, \ldots, \theta_p)\)</span>, the MLE for any component of <span class="math inline">\(\bm \theta\)</span> is given by the corresponding component of <span class="math inline">\(\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T\)</span>. Similarly, the MLE for any function of parameters <span class="math inline">\(g(\bm \theta)\)</span> is given by <span class="math inline">\(g(\hat{\bm \theta})\)</span>.</p>
<p>As <span class="math inline">\(\log\)</span> is a strictly increasing function, the value of <span class="math inline">\(\bm \theta\)</span> which maximises <span class="math inline">\(L(\bm \theta)\)</span> also maximises <span class="math inline">\(\ell(\bm \theta) = \log L (\bm \theta)\)</span>. It is almost always easier to maximise <span class="math inline">\(\ell(\bm \theta)\)</span>. This is achieved in the usual way; finding a stationary point by differentiating <span class="math inline">\(\ell(\bm \theta)\)</span> with respect to <span class="math inline">\(\theta_1, \ldots, \theta_p\)</span>, and solving the resulting <span class="math inline">\(p\)</span> simultaneous equations. It should also be checked that the stationary point is a maximum.</p>

<div class="example">
<span id="exm:unnamed-chunk-10" class="example"><strong>Example 2.3  (Bernoulli)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(p)\)</span> and the log-likelihood is <span class="math display">\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]</span> Differentiating with respect to <span class="math inline">\(p\)</span>, <span class="math display">\[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]</span> so the MLE <span class="math inline">\(\hat p\)</span> solves <span class="math display">\[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]</span> Solving this for <span class="math inline">\(\hat{p}\)</span> gives <span class="math inline">\(\hat{p}=\bar y\)</span>. Note that <span class="math display">\[\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}&lt;0\]</span> everywhere, so the stationary point is clearly a maximum.
</div>


<div class="example">
<span id="exm:unnamed-chunk-11" class="example"><strong>Example 2.4  (Normal)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and and the log-likelihood is <span class="math display">\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]</span> Differentiating with respect to <span class="math inline">\(\mu\)</span> <span class="math display">\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]</span> so <span class="math inline">\((\hat \mu, \hat \sigma^2)\)</span> solve
<span class="math display" id="eq:normalScoreMu">\[\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  \tag{2.1}
\end{equation}\]</span>
Differentiating with respect to <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]</span> so
<span class="math display" id="eq:normalScoreSs">\[\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  \tag{2.2}
\end{equation}\]</span>
<p>Solving <a href="inference.html#eq:normalScoreMu">(2.1)</a> and <a href="inference.html#eq:normalScoreSs">(2.2)</a>, we obtain <span class="math inline">\(\hat{\mu}= \bar y\)</span> and <span class="math display">\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]</span></p>
Strictly, to show that this stationary point is a maximum, we need to show that the Hessian matrix (the matrix of second derivatives with elements <span class="math inline">\([\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)\)</span>) is negative definite at <span class="math inline">\(\bm \theta=\hat{\bm \theta}\)</span>, that is <span class="math inline">\(\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}&lt;0\)</span> for every <span class="math inline">\(\bm{a}\ne {\bf 0}\)</span>. Here <span class="math display">\[
\bm{H}(\hat{\mu},\hat \sigma^2)= \begin{pmatrix}
- \frac{n}{\hat \sigma^2 } &amp; 0 \cr
0   &amp;-\frac{n}{2(\hat \sigma^2)^2} \end{pmatrix}
\]</span> which is clearly negative definite.
</div>

</div>
<div id="score" class="section level2">
<h2><span class="header-section-number">2.4</span> Score</h2>
<p>Let <span class="math display">\[
u_i(\bm \theta)\equiv{{\partial}\over{\partial\theta_i}} \ell(\theta), \quad i=1,\ldots ,p
\]</span> and <span class="math inline">\(\bm{u}(\bm \theta)\equiv[u_1(\bm \theta),\ldots ,u_p(\bm \theta)]^T\)</span>. Then we call <span class="math inline">\(\bm{u}(\bm \theta)\)</span> the <em>vector of scores</em> or <em>score vector</em>. Where <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\bm \theta=(\theta)\)</span>, the <em>score</em> is the scalar defined as <span class="math display">\[
u(\theta)\equiv{{\partial}\over{\partial\theta}}\ell(\theta).
\]</span> The maximum likelihood estimate <span class="math inline">\(\hat{\bm \theta}\)</span> satisfies <span class="math display">\[u(\hat{\bm \theta})={\bm 0},\]</span> that is, <span class="math display">\[u_i(\hat{\bm \theta})=0, \quad i=1,\ldots ,p.\]</span> Note that <span class="math inline">\(u(\bm{\theta})\)</span> is a function of <span class="math inline">\(\bm \theta\)</span> for fixed (observed) <span class="math inline">\(\bm y\)</span>. However, if we replace <span class="math inline">\(y_1, \ldots, y_n\)</span> in <span class="math inline">\(u(\bm{\theta})\)</span>, by the corresponding random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span> then we obtain a vector of random variables <span class="math inline">\(U(\bm{\theta})\equiv [U_1(\bm \theta),\ldots ,U_p(\bm \theta)]^T\)</span>.</p>
<p>An important result in likelihood theory is that the expected score at the true (but unknown) value of <span class="math inline">\(\bm \theta\)</span> is zero:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-12" class="theorem"><strong>Theorem 2.1  </strong></span>We have <span class="math inline">\(E[U(\bm{\theta})]={\bm 0}\)</span>, <em>i.e.</em> <span class="math inline">\(E[U_i(\bm \theta)]= 0,\)</span> <span class="math inline">\(i=1,\ldots ,p,\)</span> provided that</p>
<ol style="list-style-type: decimal">
<li>The expectation exists.</li>
<li>The sample space for <span class="math inline">\(\bm Y\)</span> does not depend on <span class="math inline">\(\bm \theta\)</span>.</li>
</ol>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> Our proof is for continuous <span class="math inline">\(\bm y\)</span> – in the discrete case, replace <span class="math inline">\(\int\)</span> by <span class="math inline">\(\sum\)</span>. For each <span class="math inline">\(i=1, \ldots, n\)</span>
<span class="math display">\[\begin{align*}
E[U_i(\bm \theta)]&amp;=\int U_i(\bm \theta)f_{\bm Y}(\bm y, \bm \theta) d\bm y\cr
&amp;= \int {{\partial}\over{\partial\theta_i}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= \int {{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= {{\partial}\over{\partial\theta_i}}\int f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= {{\partial}\over{\partial\theta_i}} 1 =0,
\end{align*}\]</span>
as required.
</div>


<div class="example">
<span id="exm:unnamed-chunk-14" class="example"><strong>Example 2.5  (Bernoulli)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(p)\)</span> and <span class="math display">\[u(p)=n\bar y/ p-n(1-\bar y)/(1-p).\]</span> Since <span class="math inline">\(E[U(p)] = 0\)</span>, we must have <span class="math inline">\(E[\bar Y]=p\)</span> (which we already know is correct).
</div>


<div class="example">
<span id="exm:unnamed-chunk-15" class="example"><strong>Example 2.6  (Normal)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and
<span class="math display">\[\begin{align*}
u_1(\mu,\sigma^2)&amp;= {{n(\bar y-\mu)}/{\sigma^2}}\cr
u_2(\mu,\sigma^2)&amp;= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum_{i=1}^n{(y_i-\mu)^2}
\end{align*}\]</span>
Since <span class="math inline">\(E[\bm U(\mu,\sigma^2)] = {\bm 0}\)</span>, we must have <span class="math inline">\(E[\bar Y]=\mu\)</span> and <span class="math inline">\(E[{\textstyle{1\over n}}\sum_{i=1}^n{(Y_i-\mu)^2}]=\sigma^2.\)</span>
</div>

</div>
<div id="info" class="section level2">
<h2><span class="header-section-number">2.5</span> Information</h2>
<p>Suppose that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, whose joint p.d.f. <span class="math inline">\(L(\theta)\)</span> is completely specified except for the values of <span class="math inline">\(p\)</span> unknown parameters <span class="math inline">\(\bm \theta=(\theta_1, \ldots, \theta_p)^T\)</span>. Previously, we defined the Hessian matrix <span class="math inline">\(H(\bm{\theta})\)</span> to be the matrix with components <span class="math display">\[
[H(\bm{\theta})]_{ij}\equiv{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
\]</span> We call the matrix <span class="math inline">\(-H(\bm{\theta})\)</span> the <em>observed information matrix</em>. Where <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\bm \theta=(\theta)\)</span>, the <em>observed information</em> is a scalar defined as <span class="math display">\[
-H(\theta)\equiv-{{\partial}\over{\partial\theta^2}}\ell(\theta).
\]</span></p>
<!-- Here, we are interpreting $\bm \theta$ as the true (but unknown) value of -->
<!-- the parameter. -->
<p>As with the score, if we replace <span class="math inline">\(y_1, \ldots, y_n\)</span> in <span class="math inline">\(H(\bm{\theta})\)</span>, by the corresponding random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, we obtain a matrix of random variables. Then, we define the <em>expected information matrix</em> or <em>Fisher information matrix</em> <span class="math display">\[
[\mathcal{I}(\bm \theta)]_{ij}=E(-[H(\bm{\theta})]_{ij})
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
\]</span></p>
<p>An important result in likelihood theory is that the variance-covariance matrix of the score vector is equal to the expected information matrix:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-16" class="theorem"><strong>Theorem 2.2  </strong></span>We have <span class="math inline">\(\text{Var}[U(\bm{\theta})]=\mathcal{I}(\bm \theta)\)</span>, <em>i.e.</em> <span class="math display">\[\text{Var}[U(\bm{\theta})]_{ij}= [\mathcal{I}(\bm \theta)]_{ij}, 
\quad i=1,\ldots ,p, \quad j=1,\ldots ,p\]</span> provided that</p>
<ol style="list-style-type: decimal">
<li>The variance exists.</li>
<li>The sample space for <span class="math inline">\(\bm Y\)</span> does not depend on <span class="math inline">\(\bm \theta\)</span>.</li>
</ol>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Our proof is for continuous <span class="math inline">\(\bm y\)</span> – in the discrete case, replace <span class="math inline">\(\int\)</span> by <span class="math inline">\(\sum\)</span>.</p>
For each <span class="math inline">\(i = 1,\ldots, p\)</span> and <span class="math inline">\(j = 1, \ldots, p\)</span>,
<span class="math display">\[\begin{align*}
\text{Var}[U(\bm{\theta})]_{ij}&amp;= E[U_i(\bm \theta)U_j(\bm \theta)]\cr
&amp;= \int {{\partial}\over{\partial\theta_i}} \ell(\theta)
{{\partial}\over{\partial\theta_j}} \ell(\theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= \int {{\partial}\over{\partial\theta_i}} \log f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta) f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= \int {{{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)}
{{{{\partial}\over{\partial\theta_j}}f_{\bm Y}(\bm y; \bm \theta)}\over f_{\bm Y}(\bm y; \bm \theta)} f_{\bm Y}(\bm y; \bm \theta)d\bm y\cr
&amp;= \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}}f_{\bm Y}(\bm y; \bm \theta)
 {{\partial}\over{\partial\theta_j}}  f_{\bm Y}(\bm y; \bm \theta)  d\bm y.
\end{align*}\]</span>
Now
<span class="math display">\[\begin{align*}
[\mathcal{I}(\bm \theta)]_{ij}&amp;=E\left[-{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)\right]\cr
&amp;=\int -{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \log f_{\bm Y}(\bm y; \bm \theta)  f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;=\int -{{\partial}\over{\partial\theta_i}}\left[
{{{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}\right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;=\int \left[
-{{{{\partial^2}\over{\partial\theta_i\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)}
+ {{{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)}\over  f_{\bm Y}(\bm y; \bm \theta)^2} \right]
 f_{\bm Y}(\bm y; \bm \theta) d\bm y\cr
&amp;= -{{\partial^2}\over{\partial\theta_i\partial\theta_j}}\int  f_{\bm Y}(\bm y; \bm \theta) d\bm y
+ \int \frac{1}{f_{\bm Y}(\bm y; \bm \theta)}{{\partial}\over{\partial\theta_i}} f_{\bm Y}(\bm y; \bm \theta)
{{\partial}\over{\partial\theta_j}} f_{\bm Y}(\bm y; \bm \theta)  d\bm y\cr
&amp;= \text{Var}[U(\bm{\theta})]_{ij},
\end{align*}\]</span>
as required.
</div>


<div class="example">
<span id="exm:unnamed-chunk-18" class="example"><strong>Example 2.7  (Bernoulli)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(p)\)</span> and
<span class="math display">\[\begin{align*}
u(p)&amp;= {{n\bar y}\over{ p}}-{{n(1-\bar y)}\over {(1-p)}}\cr
-H(p)&amp;= {{n\bar y}\over{ p^2}}+{{n(1-\bar y)}\over {(1-p)^2}}\cr
{\cal I}(p)&amp;= {{n}\over{ p}}+{{n}\over {(1-p)}}={{n}\over {p(1-p)}}.
\end{align*}\]</span>
</div>


<div class="example">
<span id="exm:unnamed-chunk-19" class="example"><strong>Example 2.8  (Normal)  </strong></span><span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and
<span class="math display">\[\begin{align*}
u_1(\mu,\sigma^2) &amp;=  \frac{n(\bar y-\mu)}{\sigma^2} \\
u_2(\mu,\sigma^2) &amp;= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2.
\end{align*}\]</span>
Therefore <span class="math display">\[
-\bm{H}(\mu,\sigma^2) = \begin{pmatrix}
\frac{n}{\sigma^2} &amp; \frac{n(\bar y-\mu)}{(\sigma^2)^2} \cr
\frac{n(\bar y-\mu)}{(\sigma^2)^2}&amp;
 \frac{1}{(\sigma^2)^3} \sum(y_i-\mu)^2- \frac{n}{2(\sigma^2)^2}
\end{pmatrix}
\]</span> <span class="math display">\[
{\cal I}(\mu,\sigma^2)= \begin{pmatrix}
\frac{n}{\sigma^2} &amp; 0 \cr
0&amp; \frac{n}{2(\sigma^2)^2}
\end{pmatrix}.
\]</span>
</div>

</div>
<div id="sn:asnmle" class="section level2">
<h2><span class="header-section-number">2.6</span> Asymptotic distribution of the MLE</h2>
<p>Maximum likelihood estimation is an attractive method of estimation for a number of reasons. It is intuitively sensible and usually reasonably straightforward to carry out. Even when the simultaneous equations we obtain by differentiating the log-likelihood function are impossible to solve directly, solution by numerical methods is usually feasible.</p>
<p>Perhaps the most compelling reason for considering maximum likelihood estimation is the asymptotic behaviour of maximum likelihood estimators.</p>
<p>Suppose that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of independent random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, whose joint p.d.f. <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)=\prod_{i=1}^n f_{Y_i}(y_i;\bm \theta)\)</span> is completely specified except for the values of an unknown parameter vector <span class="math inline">\(\bm \theta\)</span>, and that <span class="math inline">\(\hat{\bm \theta}\)</span> is the maximum likelihood estimator of <span class="math inline">\(\bm \theta\)</span>.</p>
<p>Then, as <span class="math inline">\(n\to\infty\)</span>, the distribution of <span class="math inline">\(\hat{\bm \theta}\)</span> tends to a multivariate normal distribution with mean vector <span class="math inline">\(\bm \theta\)</span> and variance covariance matrix <span class="math inline">\(\mathcal{I}(\bm \theta)^{-1}\)</span>.</p>
<p>Where <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\bm \theta=(\theta)\)</span>, the distribution of the MLE <span class="math inline">\(\hat{\theta}\)</span> tends to <span class="math inline">\(N[\theta,1/{\cal I}(\theta)]\)</span>.</p>
<p>For ‘large enough <span class="math inline">\(n\)</span>’, we can treat the asymptotic distribution of the MLE as an approximation. The fact that <span class="math inline">\(E(\hat{\bm \theta})\approx\bm \theta\)</span> means that the maximum likelihood estimator is <em>approximately unbiased</em> for large samples. The variance of <span class="math inline">\(\hat{\bm \theta}\)</span> is approximately <span class="math inline">\(\mathcal{I}(\bm \theta)^{-1}\)</span>. It is possible to show that this is the smallest possible variance of any unbiased estimator of <span class="math inline">\(\bm \theta\)</span> (this result is called the Cramér–Rao lower bound, which we do not prove here). Therefore the MLE is the ‘best possible’ estimator in large samples (and therefore we hope also reasonable in small samples, though we should investigate this case by case).</p>
</div>
<div id="quantifying-uncertainty-in-parameter-estimates" class="section level2">
<h2><span class="header-section-number">2.7</span> Quantifying uncertainty in parameter estimates</h2>
<p>The usefulness of an estimate is always enhanced if some kind of measure of its precision can also be provided. Usually, this will be a <em>standard error</em>, an estimate of the standard deviation of the associated estimator. For the maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span>, a standard error is given by <span class="math display">\[
s.e.(\hat{\theta})={1\over{{\cal I}(\hat{\theta})^{{1\over 2}}}},
\]</span> and for a vector parameter <span class="math inline">\(\bm \theta\)</span> <span class="math display">\[
s.e.(\hat{\theta}_i)=[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{{1\over 2}},
\quad i=1,\ldots ,p.
\]</span></p>
<p>An alternative summary of the information provided by the observed data about the location of a parameter <span class="math inline">\(\theta\)</span> and the associated precision is a <em>confidence interval</em>.</p>
<p>The asymptotic distribution of the maximum likelihood estimator can be used to provide approximate large sample confidence intervals. Asymptotically, <span class="math inline">\(\hat{\theta}_i\)</span> has a <span class="math inline">\(N(\theta_i,[\mathcal{I}(\bm \theta)^{-1}]_{ii})\)</span> distribution and we can find <span class="math inline">\(z_{1-\frac{\alpha}{2}}\)</span> such that <span class="math display">\[
P\left(- z_{1-\frac{\alpha}{2}}\le {{\hat{\theta}_i-\theta_i}\over{[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) = 1- \alpha.
\]</span> Therefore <span class="math display">\[
P\left(\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}\le\theta_i
\le\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}
\right) = 1- \alpha.
\]</span> The endpoints of this interval cannot be evaluated because they also depend on the unknown parameter vector <span class="math inline">\(\bm \theta\)</span>. However, if we replace <span class="math inline">\(\mathcal{I}(\bm \theta)\)</span> by its MLE <span class="math inline">\({\cal I}(\hat{\bm \theta})\)</span> we obtain the approximate large sample <span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval <span class="math display">\[
[\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2},
\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2}].
\]</span> For <span class="math inline">\(\alpha=0.1,0.05,0.01\)</span>, <span class="math inline">\(z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58\)</span>.</p>

<div class="example">
<span id="exm:unnamed-chunk-20" class="example"><strong>Example 2.9  (Bernoulli)  </strong></span>If <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables then asymptotically <span class="math inline">\(\hat{p}=\bar y\)</span> has a <span class="math inline">\(N(p,{p(1-p)}/ n)\)</span> distribution, and a large sample 95% confidence interval for <span class="math inline">\(p\)</span> is
<span class="math display">\[\begin{align*}
&amp; [\hat{p}- 1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2},
\hat{p}+1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2}]
\cr
&amp;=
[\hat{p}-1.96[\hat{p}(1-\hat{p})/n]^{1\over 2},
\hat{p}+1.96[\hat{p}(1-\hat{p})/n]^{1\over 2}]\cr
&amp;=
[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}].
\end{align*}\]</span>
</div>


</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="comparing-statistical-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/heogden/math3091/edit/master/inference.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH3091.pdf", "MATH3091.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
