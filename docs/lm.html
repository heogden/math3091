<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Linear Models | MATH3091: Statistical Modelling II</title>
  <meta name="description" content="The course notes for MATH3091: Statistical Modelling II" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Linear Models | MATH3091: Statistical Modelling II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Linear Models | MATH3091: Statistical Modelling II" />
  
  <meta name="twitter:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="comparing-statistical-models.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH3012: Statistical Modelling II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#elements-of-statistical-modelling"><i class="fa fa-check"></i><b>1.1</b> Elements of statistical modelling</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#regression-models"><i class="fa fa-check"></i><b>1.2</b> Regression models</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#example-data-to-be-analysed"><i class="fa fa-check"></i><b>1.3</b> Example data to be analysed</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#nitric-nitric-acid"><i class="fa fa-check"></i><b>1.3.1</b> <code>nitric</code>: Nitric acid</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#birth-weight-of-newborn-babies"><i class="fa fa-check"></i><b>1.3.2</b> <code>birth</code>: Weight of newborn babies</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#survival-time-to-death"><i class="fa fa-check"></i><b>1.3.3</b> <code>survival</code>: Time to death</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#beetle-mortality-from-carbon-disulphide"><i class="fa fa-check"></i><b>1.3.4</b> <code>beetle</code>: Mortality from carbon disulphide</a></li>
<li class="chapter" data-level="1.3.5" data-path="intro.html"><a href="intro.html#shuttle-challenger-disaster"><i class="fa fa-check"></i><b>1.3.5</b> <code>shuttle</code>: Challenger disaster</a></li>
<li class="chapter" data-level="1.3.6" data-path="intro.html"><a href="intro.html#heart-treatment-for-heart-attack"><i class="fa fa-check"></i><b>1.3.6</b> <code>heart</code>: Treatment for heart attack</a></li>
<li class="chapter" data-level="1.3.7" data-path="intro.html"><a href="intro.html#accident-road-traffic-accidents"><i class="fa fa-check"></i><b>1.3.7</b> <code>accident</code>: Road traffic accidents</a></li>
<li class="chapter" data-level="1.3.8" data-path="intro.html"><a href="intro.html#lymphoma-lymphoma-patients"><i class="fa fa-check"></i><b>1.3.8</b> <code>lymphoma</code>: Lymphoma patients</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Parametric statistical inference</a><ul>
<li class="chapter" data-level="2.1" data-path="inference.html"><a href="inference.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="inference.html"><a href="inference.html#the-likelihood-function"><i class="fa fa-check"></i><b>2.2</b> The likelihood function</a></li>
<li class="chapter" data-level="2.3" data-path="inference.html"><a href="inference.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>2.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4" data-path="inference.html"><a href="inference.html#score"><i class="fa fa-check"></i><b>2.4</b> Score</a></li>
<li class="chapter" data-level="2.5" data-path="inference.html"><a href="inference.html#info"><i class="fa fa-check"></i><b>2.5</b> Information</a></li>
<li class="chapter" data-level="2.6" data-path="inference.html"><a href="inference.html#sn:asnmle"><i class="fa fa-check"></i><b>2.6</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="2.7" data-path="inference.html"><a href="inference.html#quantifying-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>2.7</b> Quantifying uncertainty in parameter estimates</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html"><i class="fa fa-check"></i><b>3</b> Comparing statistical models</a><ul>
<li class="chapter" data-level="3.1" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#hypothesis-testing"><i class="fa fa-check"></i><b>3.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="3.3" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#sn:lrt"><i class="fa fa-check"></i><b>3.3</b> Likelihood ratio tests for nested hypotheses</a></li>
<li class="chapter" data-level="3.4" data-path="comparing-statistical-models.html"><a href="comparing-statistical-models.html#information-criteria-for-model-comparison"><i class="fa fa-check"></i><b>3.4</b> Information criteria for model comparison</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>4</b> Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="lm.html"><a href="lm.html#the-linear-model"><i class="fa fa-check"></i><b>4.1</b> The linear model</a></li>
<li class="chapter" data-level="4.2" data-path="lm.html"><a href="lm.html#eglm"><i class="fa fa-check"></i><b>4.2</b> Examples of linear model structure</a></li>
<li class="chapter" data-level="4.3" data-path="lm.html"><a href="lm.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>4.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="4.4" data-path="lm.html"><a href="lm.html#properties-of-the-mle"><i class="fa fa-check"></i><b>4.4</b> Properties of the MLE</a></li>
<li class="chapter" data-level="4.5" data-path="lm.html"><a href="lm.html#comparing-linear-models"><i class="fa fa-check"></i><b>4.5</b> Comparing linear models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>5</b> Generalised Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="glm.html"><a href="glm.html#regression-models-for-non-normal-data"><i class="fa fa-check"></i><b>5.1</b> Regression models for non-normal data</a></li>
<li class="chapter" data-level="5.2" data-path="glm.html"><a href="glm.html#sn:ef"><i class="fa fa-check"></i><b>5.2</b> The exponential family</a></li>
<li class="chapter" data-level="5.3" data-path="glm.html"><a href="glm.html#components-of-a-generalised-linear-model"><i class="fa fa-check"></i><b>5.3</b> Components of a generalised linear model</a><ul>
<li class="chapter" data-level="5.3.1" data-path="glm.html"><a href="glm.html#the-random-component"><i class="fa fa-check"></i><b>5.3.1</b> The random component</a></li>
<li class="chapter" data-level="5.3.2" data-path="glm.html"><a href="glm.html#the-systematic-or-structural-component"><i class="fa fa-check"></i><b>5.3.2</b> The systematic (or structural) component</a></li>
<li class="chapter" data-level="5.3.3" data-path="glm.html"><a href="glm.html#the-link-function"><i class="fa fa-check"></i><b>5.3.3</b> The link function</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="glm.html"><a href="glm.html#examples-of-generalised-linear-models"><i class="fa fa-check"></i><b>5.4</b> Examples of generalised linear models</a><ul>
<li class="chapter" data-level="5.4.1" data-path="glm.html"><a href="glm.html#the-linear-model-1"><i class="fa fa-check"></i><b>5.4.1</b> The linear model</a></li>
<li class="chapter" data-level="5.4.2" data-path="glm.html"><a href="glm.html#models-for-binary-data"><i class="fa fa-check"></i><b>5.4.2</b> Models for binary data</a></li>
<li class="chapter" data-level="5.4.3" data-path="glm.html"><a href="glm.html#sn:count"><i class="fa fa-check"></i><b>5.4.3</b> Models for count data</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="glm.html"><a href="glm.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>5.5</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="5.6" data-path="glm.html"><a href="glm.html#sn:glminfer"><i class="fa fa-check"></i><b>5.6</b> Confidence intervals</a></li>
<li class="chapter" data-level="5.7" data-path="glm.html"><a href="glm.html#sn:compglm"><i class="fa fa-check"></i><b>5.7</b> Comparing generalised linear models</a><ul>
<li class="chapter" data-level="5.7.1" data-path="glm.html"><a href="glm.html#sn:glmlrt"><i class="fa fa-check"></i><b>5.7.1</b> The likelihood ratio test</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="glm.html"><a href="glm.html#scaled-deviance-and-the-saturated-model"><i class="fa fa-check"></i><b>5.8</b> Scaled deviance and the saturated model</a></li>
<li class="chapter" data-level="5.9" data-path="glm.html"><a href="glm.html#sn:unknowndisp"><i class="fa fa-check"></i><b>5.9</b> Models with unknown <span class="math inline">\(a(\phi)\)</span></a></li>
<li class="chapter" data-level="5.10" data-path="glm.html"><a href="glm.html#residuals"><i class="fa fa-check"></i><b>5.10</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chap-categorical.html"><a href="chap-categorical.html"><i class="fa fa-check"></i><b>6</b> Models for categorical data</a><ul>
<li class="chapter" data-level="6.1" data-path="chap-categorical.html"><a href="chap-categorical.html#contingency-tables"><i class="fa fa-check"></i><b>6.1</b> Contingency tables</a></li>
<li class="chapter" data-level="6.2" data-path="chap-categorical.html"><a href="chap-categorical.html#log-linear-models"><i class="fa fa-check"></i><b>6.2</b> Log-linear models</a></li>
<li class="chapter" data-level="6.3" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:multinomial"><i class="fa fa-check"></i><b>6.3</b> Multinomial sampling</a></li>
<li class="chapter" data-level="6.4" data-path="chap-categorical.html"><a href="chap-categorical.html#product-multinomial-sampling"><i class="fa fa-check"></i><b>6.4</b> Product multinomial sampling</a></li>
<li class="chapter" data-level="6.5" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:loginter"><i class="fa fa-check"></i><b>6.5</b> Interpreting log-linear models for two-way tables</a></li>
<li class="chapter" data-level="6.6" data-path="chap-categorical.html"><a href="chap-categorical.html#interpreting-log-linear-models-for-multiway-tables"><i class="fa fa-check"></i><b>6.6</b> Interpreting log-linear models for multiway tables</a></li>
<li class="chapter" data-level="6.7" data-path="chap-categorical.html"><a href="chap-categorical.html#simpsons-paradox"><i class="fa fa-check"></i><b>6.7</b> Simpson’s paradox</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3091: Statistical Modelling II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="lm" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Linear Models</h1>
<div id="the-linear-model" class="section level2">
<h2><span class="header-section-number">4.1</span> The linear model</h2>
<p>In practical applications, we often distinguish between a <em>response</em> variable and a group of <em>explanatory</em> variables. The aim is to determine the pattern of dependence of the response variable on the explanatory variables. We denote the <span class="math inline">\(n\)</span> observations of the response variable by <span class="math inline">\(\bm{y}=(y_1,y_2,\ldots ,y_n)^T\)</span>. These are assumed to be observations of <em>random variables</em> <span class="math inline">\(\bm Y=(Y_1,Y_2,\ldots ,Y_n)^T\)</span>. Associated with each <span class="math inline">\(y_i\)</span> is a vector <span class="math inline">\(\bm{x}_i=(x_{i1},x_{i2},\ldots ,x_{ip})^T\)</span> of values of <span class="math inline">\(p\)</span> explanatory variables.</p>
In a linear model, we assume that
<span class="math display" id="eq:lmNonMat">\[\begin{align}
Y_i&amp;= \beta_1 x_{i1} +\beta_2 x_{i2} +\ldots + \beta_p x_{ip} + \epsilon_i \cr
&amp;= \sum_{j=1}^p x_{ij} \beta_j + \epsilon_i \cr
&amp;=  \bm{x}_i^T\bm{\beta} + \epsilon_i \cr
&amp;= [\bm{X}\bm{\beta}]_i + \epsilon_i,\qquad i=1,\ldots ,n  \tag{4.1}
\end{align}\]</span>
where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> independently, <span class="math display">\[\bm{X}= \begin{pmatrix}
\bm{x}_1^T \\
\vdots \\
\bm{x}_n^T
\end{pmatrix}
=\begin{pmatrix}
x_{11} &amp; \cdots &amp;x_{1p}\cr
\vdots &amp; \ddots &amp;\vdots\cr
x_{n1} &amp;\cdots &amp;x_{np}
\end{pmatrix}\]</span> and <span class="math inline">\(\bm{\beta}=(\beta_1,\ldots ,\beta_p)^T\)</span> is a vector of fixed but unknown parameters describing the dependence of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(\bm{x}_i\)</span>. The four ways of describing the linear model in <a href="lm.html#eq:lmNonMat">(4.1)</a> are equivalent, but the most economical is the matrix form
<span class="math display" id="eq:lmMat">\[\begin{equation}
\bm{Y}=\bm{X}\bm{\beta} + \bm{\epsilon}. \tag{4.2}
\end{equation}\]</span>
<p>where <span class="math inline">\(\bm{\epsilon}=(\epsilon_1,\epsilon_2,\ldots ,\epsilon_n)^T\)</span>.</p>
<p>The <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(\bm{X}\)</span> consists of known (observed) constants and is called the <em>design matrix</em>. The <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\bm{X}\)</span> is <span class="math inline">\(\bm{x}_i^T\)</span>, the explanatory data corresponding to the <span class="math inline">\(i\)</span>th observation of the response. The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\bm{X}\)</span> contains the <span class="math inline">\(n\)</span> observations of the <span class="math inline">\(j\)</span>th explanatory variable.</p>
<p>The error vector <span class="math inline">\(\bm{\epsilon}\)</span> has a multivariate normal distribution with mean vector <span class="math inline">\({\bf 0}\)</span> and variance covariance matrix <span class="math inline">\(\sigma^2\bm{I}\)</span>, since <span class="math inline">\(\text{Var}(\epsilon_i)=\sigma^2\)</span>, and <span class="math inline">\(\text{Cov}(\epsilon_i,\epsilon_j)=0\)</span>, as <span class="math inline">\(\epsilon_1, \ldots, \epsilon_n\)</span> are independent of one another. It follows from <a href="lm.html#eq:lmMat">(4.2)</a> that the distribution of <span class="math inline">\(\bm Y\)</span> is multivariate normal with mean vector <span class="math inline">\(\bm{X}\bm{\beta}\)</span> and variance covariance matrix <span class="math inline">\(\sigma^2\bm{I}\)</span>, <em>i.e.</em> <span class="math inline">\(\bm Y\sim N(\bm{X}\bm{\beta},\sigma^2\bm{I})\)</span>.</p>
</div>
<div id="eglm" class="section level2">
<h2><span class="header-section-number">4.2</span> Examples of linear model structure</h2>

<div class="example">
<span id="exm:unnamed-chunk-24" class="example"><strong>Example 4.1  (The null model)  </strong></span>If we do not include any variables <span class="math inline">\(x_i\)</span> in the model, we have <span class="math display">\[
Y_i=\beta_0 + \epsilon_i, \qquad \epsilon_i \sim N(0, \sigma^2), \qquad i = 1, \ldots, n,
\]</span> so <span class="math display">\[
\bm{X}=\begin{pmatrix}
1\cr
1\cr
\vdots
\cr 1
\end{pmatrix},
\qquad
\bm{\beta}=(\beta_0).
\]</span> This is one (dummy) explanatory variable. In practice, this variable is present in all models.
</div>


<div class="example">
<span id="exm:unnamed-chunk-25" class="example"><strong>Example 4.2  (Simple linear regression)  </strong></span>If we include a single variable <span class="math inline">\(x_i\)</span> in the model, we might have <span class="math display">\[
Y_i=\beta_0+\beta_1 x_i + \epsilon_i, \qquad \epsilon_i \sim N(0, \sigma^2) \qquad i = 1, \ldots, n
\]</span> so <span class="math display">\[
\bm{X}=\begin{pmatrix} 1&amp;x_1\cr 1&amp;x_2\cr \vdots&amp;\vdots\cr 1&amp;x_n \end{pmatrix},
\qquad \bm{\beta}=\begin{pmatrix} \beta_0\cr\beta_1 \end{pmatrix}.
\]</span> There are two explanatory variables: the dummy variable and one ‘real’ variable.
</div>


<div class="example">
<span id="exm:polylm" class="example"><strong>Example 4.3  (Polynomial regression)  </strong></span>If we want to allow for a non-linear impact of <span class="math inline">\(x_i\)</span> on the mean of <span class="math inline">\(Y_i\)</span>, we might model <span class="math display">\[
Y_i=\beta_0+\beta_1 x_i+\beta_2 x_i^2 +\ldots +\beta_{p-1} x_i^{p-1} + \epsilon_i,
\qquad \epsilon_i \sim N(0, \sigma^2), \qquad i = 1, \ldots, n,
\]</span> so <span class="math display">\[
\bm{X}= \begin{pmatrix}
1&amp;x_1&amp;x_1^2&amp;\cdots&amp;x_1^{p-1}\cr
1&amp;x_2&amp;x_2^2&amp;\cdots&amp;x_2^{p-1}\cr
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr
1&amp;x_n&amp;x_n^2&amp;\cdots&amp;x_n^{p-1} \end{pmatrix},
\qquad \bm{\beta}= \begin{pmatrix} \beta_0\cr\beta_1\cr\vdots\cr\beta_{p-1} \end{pmatrix}.
\]</span> There are <span class="math inline">\(p\)</span> explanatory variables: the dummy variable and one ‘real’ variable, transformed to <span class="math inline">\(p-1\)</span> variables.
</div>


<div class="example">
<span id="exm:unnamed-chunk-26" class="example"><strong>Example 4.4  (Multiple regression)  </strong></span>To include multiple explanatory variables, we might model <span class="math display">\[
Y_i=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2} +\ldots +\beta_{p-1} x_{i\,p-1} + \epsilon_i, 
\qquad \epsilon_i \sim N(0, \sigma^2), \qquad i = 1, \ldots, n,
\]</span> so <span class="math display">\[
\bm{X}= \begin{pmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1\,p-1}\cr
1&amp;x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2\,p-1}\cr
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr
1&amp;x_{n1}&amp;x_{n2}&amp;\cdots&amp;x_{n\,p-1} 
\end{pmatrix},
\qquad \bm{\beta}=\begin{pmatrix} \beta_0\cr\beta_1\cr\vdots\cr\beta_{p-1} \end{pmatrix}.
\]</span> There are <span class="math inline">\(p\)</span> explanatory variables: the dummy variable and <span class="math inline">\(p-1\)</span> ‘real’ variables.
</div>
 
<div class="example">
<p><span id="exm:catlm" class="example"><strong>Example 4.5  (One categorical explanatory variable)  </strong></span>Suppose <span class="math inline">\(x_i\)</span> is a categorical variable, taking values in a set of <span class="math inline">\(k\)</span> possible categories. For simplicity of notation, we will give each category a number, and write <span class="math inline">\(x_i \in \{1, \ldots, k\}\)</span>. We wish to model <span class="math display">\[Y_i = \mu_{x_i} + \epsilon_i, 
\qquad \epsilon_i \sim N(0, \sigma^2), \qquad i = 1, \ldots, n,\]</span> so that the mean of <span class="math inline">\(Y_i\)</span> is the same for all observations in the same category, but differs for different categories.</p>
<p>We could rewrite this model to include an intercept, as <span class="math display">\[Y_i = \beta_0 + \beta_{x_i} + \epsilon_i, 
\qquad \epsilon_i \sim N(0, \sigma^2), \qquad i = 1, \ldots, n,\]</span> so that <span class="math inline">\(\mu_j = \beta_0 + \beta_{j}\)</span>, for <span class="math inline">\(j = 1, \ldots, k\)</span>. It is not possible to estimate all of the <span class="math inline">\(\bm \beta\)</span> parameters separately, as they only affect the distribution through the combination <span class="math inline">\(\beta_0 + \beta_{j}\)</span>. Instead, we choose a <strong>reference category</strong> <span class="math inline">\(l\)</span>, and set <span class="math inline">\(\beta_l = 0\)</span>. The intercept term <span class="math inline">\(\beta_0\)</span> then gives the mean for the reference category, with <span class="math inline">\(\beta_j\)</span> giving the difference in mean between category <span class="math inline">\(j\)</span> and the reference category. In <code>R</code>, categorical variables are called <code>factor</code>s, and by default the reference category will be the first category when the names of the categories (the <code>levels</code> of the <code>factor</code>) are sorted alphabetically.</p>
We can rewrite the model as a form of multiple regression by first defining a new explanatory variable <span class="math inline">\(\bm{z}_i\)</span> <span class="math display">\[\bm{z}_i = (z_{i1}, \ldots, z_{ik})^T,\]</span> where <span class="math display">\[z_{ij} = \begin{cases} 1 &amp; \text{if $x_i = j$} \\
0 &amp; \text{otherwise}. \end{cases}\]</span> <span class="math inline">\(\bm{z}_i\)</span> is sometimes called the <strong>one-hot encoding</strong> of <span class="math inline">\(x_i\)</span>, as it contains precisely one <span class="math inline">\(1\)</span> (corresponding to the category <span class="math inline">\(x_i\)</span>), and is <span class="math inline">\(0\)</span> everywhere else. We then have <span class="math display">\[Y_i=\beta_0+\beta_1 z_{i1}+\beta_2 z_{i2} +\ldots +\beta_{k} z_{ik} + \epsilon_i,\]</span> so <span class="math display">\[\bm{X}= \begin{pmatrix}
1 &amp; z_{11} &amp; z_{12} &amp;\cdots &amp; z_{1k}\cr
1 &amp; z_{21} &amp; z_{22} &amp;\cdots &amp; z_{2k}\cr
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr
1 &amp; z_{n1} &amp; z_{n2} &amp;\cdots &amp; z_{nk}
\end{pmatrix},
\qquad \bm{\beta}= \begin{pmatrix} \beta_0\cr\beta_1\cr\vdots\cr\beta_{k} \end{pmatrix},\]</span> where each row of <span class="math inline">\(\bm X\)</span> will have two ones, and the remaining entries will be zero.
</div>
 
<div class="example">
<p><span id="exm:catlm2" class="example"><strong>Example 4.6  (Two categorical explanatory variables)  </strong></span>Suppose we have two categorical variables <span class="math inline">\(x_{i1} \in \{1, \ldots, k_1\}\)</span> and <span class="math inline">\(x_{i2} \in \{1, \ldots, k_2\}.\)</span> We might consider a model <span class="math display">\[Y_i=\beta_0+\beta^{(1)}_{x_{i1}} + \beta^{(2)}_{x_{i2}} + \epsilon_i,
\qquad \epsilon_i \sim N(0, \sigma^2), \qquad i = 1, \ldots, n,\]</span> where <span class="math display">\[\bm \beta = \left(\beta_0, \beta^{(1)}_1, \ldots, \beta^{(1)}_{k_1}, \beta^{(2)}_1, \ldots, \beta^{(2)}_{k_2}\right)^T,\]</span> where as in Example <a href="lm.html#exm:catlm">4.5</a> we choose reference categories <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span> for each categorical variable, and set <span class="math inline">\(\beta^{(1)}_{l_1} = \beta^{(2)}_{l_2} = 0\)</span>. The terms <span class="math inline">\(\beta^{(1)}_j\)</span> are called the <strong>main effects</strong> for the categorical variables <span class="math inline">\(x_{i1}\)</span>, and <span class="math inline">\(\beta^{(2)}_j\)</span> are the main effects for <span class="math inline">\(x_{i2}\)</span>.</p>
We might also want to allow an <strong>interaction</strong> between <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span>, letting <span class="math display">\[Y_i=\beta_0+\beta^{(1)}_{x_{i1}} + \beta^{(2)}_{x_{i2}} + \beta^{(1, 2)}_{x_{i1}, x_{i2}} 
+ \epsilon_i,\]</span> where <span class="math display">\[\bm \beta = \left(\beta_0, \beta^{(1)}_1, \ldots, \beta^{(1)}_{k_1}, \beta^{(2)}_1, \ldots, \beta^{(2)}_{k_2},\beta^{(1, 2)}_{11}, \beta^{(1, 2)}_{1, 2}, \ldots, 
\beta^{(1, 2)}_{k_1, k_2}\right)^T.\]</span> The terms <span class="math inline">\(\beta^{(1, 2)}_{j_1, j_2}\)</span> are called the <strong>interaction effects</strong>. This model is equivalent to <span class="math display">\[Y_i= \mu_{x_{i1}, x_{i2}} 
+ \epsilon_i,\]</span> allowing a different mean for each possible combination of categories. To allow us to estimate the parameters, given reference categories <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span>, we set <span class="math display">\[\beta^{(1)}_{l_1} = \beta^{(2)}_{l_2} = 0; \qquad
\beta^{(1, 2)}_{l_1, j} = 0, \quad j = 1, \ldots, k_2; \qquad
\beta^{(1, 2)}_{j, l_2} = 0, \quad j = 1, \ldots, k_1.\]</span> As in Example <a href="lm.html#exm:catlm">4.5</a>, it is possible to rewrite the model with a design matrix <span class="math inline">\(\bm X\)</span>, by using one-hot encoding of <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span>.
</div>

</div>
<div id="maximum-likelihood-estimation-1" class="section level2">
<h2><span class="header-section-number">4.3</span> Maximum likelihood estimation</h2>
<p>The regression coefficients <span class="math inline">\(\beta_1, \ldots, \beta_p\)</span> describe the pattern by which the response depends on the explanatory variables. We use the observed data <span class="math inline">\(y_1, \ldots, y_n\)</span> to <em>estimate</em> this pattern of dependence.</p>
The likelihood for a linear model is
<span class="math display" id="eq:lmLikelihood">\[\begin{equation}
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{x}_i^T\bm{\beta})^2\right).
\tag{4.3}
\end{equation}\]</span>
<p>This is maximised with respect to <span class="math inline">\((\bm{\beta},\sigma^2)\)</span> at <span class="math display">\[
\hat{\bm{\beta}}=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
\]</span> and <span class="math display">\[
\hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n \left(y_i-\bm{x}_i^T\hat{\bm{\beta}}\right)^2.
\]</span></p>
<p>The corresponding fitted values are <span class="math display">\[\hat{\bm{y}}=\bm{X}\hat{\bm{\beta}}=\bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}\]</span> or <span class="math display">\[\hat{y}_i=\bm{x}_i^T\hat{\bm{\beta}}, \quad i = 1, \ldots, n.\]</span></p>
<p>The residuals <span class="math inline">\(\bm{r} = (r_1, \ldots, r_n)\)</span> are <span class="math inline">\(\bm{r}=\bm{y}-\bm{\hat y}\)</span> or <span class="math inline">\(r_i=y_i-\bm{x}_i^T\hat{\bm{\beta}}\)</span> for <span class="math inline">\(i = 1, \ldots, n.\)</span>. These residuals describe the variability in the observed responses <span class="math inline">\(y_1, \ldots, y_n\)</span> which has not been explained by the linear model. We call <span class="math display">\[D= \sum_{i=1}^n r_i^2 = \sum_{i=1}^n \left(y_i-\bm{x}_i^T\hat{\bm{\beta}}\right)^2\]</span> the <em>residual sum of squares</em> or <em>deviance</em> for the linear model.</p>
</div>
<div id="properties-of-the-mle" class="section level2">
<h2><span class="header-section-number">4.4</span> Properties of the MLE</h2>
<p>As <span class="math inline">\(\bm Y\)</span> is normally distributed, and <span class="math inline">\(\hat{\bm{\beta}}= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{Y}\)</span> is a linear function of <span class="math inline">\(\bm Y\)</span>, then <span class="math inline">\(\hat{\bm{\beta}}\)</span> must also be normally distributed. We have <span class="math inline">\(E(\hat{\bm{\beta}})=\bm{\beta}\)</span> and <span class="math inline">\(\text{Var}(\hat{\bm{\beta}})=\sigma^2(\bm{X}^T\bm{X})^{-1}\)</span>, so <span class="math display">\[\hat{\bm{\beta}} \sim N(\bm{\beta}, \sigma^2(\bm{X}^T\bm{X})^{-1}).\]</span></p>
<p>It is possible to prove (although we shall not do so here) that <span class="math display">\[
{D\over\sigma^2}\sim\chi^2_{n-p}
\]</span> which implies that <span class="math display">\[
E(\hat \sigma^2)={{n-p}\over n}\sigma^2,
\]</span> so the maximum likelihood estimator is biased for <span class="math inline">\(\sigma^2\)</span> (although still asymptotically unbiased as <span class="math inline">\({{n-p}\over n}\to 1\)</span> as <span class="math inline">\(n\to\infty\)</span>). We often use the unbiased estimator of <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
\tilde \sigma^2={D\over {n-p}}={1\over {n-p}}\sum_{i=1}^n r_i^2.
\]</span> The denominator <span class="math inline">\(n-p\)</span>, the number of observations minus the number of linear coefficients in the model is called the <em>degrees of freedom</em> of the model. Therefore, we estimate the residual variance by the deviance divided by the degrees of freedom.</p>
</div>
<div id="comparing-linear-models" class="section level2">
<h2><span class="header-section-number">4.5</span> Comparing linear models</h2>
<p>If we have a set of competing linear models which might explain the dependence of the response on the explanatory variables, we will want to determine which of the models is most appropriate.</p>
<p>As described previously, we proceed by comparing models pairwise using a likelihood ratio test. For linear models this kind of comparison is restricted to situations where one of the models, <span class="math inline">\(H_0\)</span>, is <em>nested</em> in the other, <span class="math inline">\(H_1\)</span>. This usually means that the explanatory variables present in <span class="math inline">\(H_0\)</span> are a subset of those present in <span class="math inline">\(H_1\)</span>. In this case model <span class="math inline">\(H_0\)</span> is a special case of model <span class="math inline">\(H_1\)</span>, where certain coefficients are set equal to zero. We let <span class="math inline">\(\bm \theta\)</span> represent the collection of linear parameters for model <span class="math inline">\(H_1\)</span>, together with the residual variance <span class="math inline">\(\sigma^2\)</span>, and let <span class="math inline">\(\Theta^{(1)}\)</span> be the unrestricted parameter space for <span class="math inline">\(\bm \theta\)</span>. Then <span class="math inline">\(\Theta^{(0)}\)</span> is the parameter space corresponding to model <span class="math inline">\(H_0\)</span>, <em>i.e.</em> with the appropriate coefficients constrained to zero.</p>
<p>We will assume that model <span class="math inline">\(H_1\)</span> contains <span class="math inline">\(p\)</span> linear parameters and model <span class="math inline">\(H_0\)</span> a subset of <span class="math inline">\(q&lt;p\)</span> of these. Without loss of generality, we can think of <span class="math inline">\(H_1\)</span> as the model <span class="math display">\[
Y_i=\sum_{j=1}^p x_{ij} \beta_j + \epsilon_i, \quad i = 1, \ldots, n
\]</span> and <span class="math inline">\(H_0\)</span> being the same model with <span class="math display">\[\beta_{q+1}=\beta_{q+2}=\cdots=\beta_p=0.
\]</span></p>
Now, a likelihood ratio test of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span> has a critical region of the form <span class="math display">\[
C=\left\{ \bm{y}:{{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(1)}}L(\bm{\beta},\sigma^2)}\over
{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(0)}}L(\bm{\beta},\sigma^2)}} &gt;k\right\}
\]</span> where <span class="math inline">\(k\)</span> is determined by <span class="math inline">\(\alpha\)</span>, the size of the test, so <span class="math display">\[
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm{\beta},\sigma^2)=\alpha.
\]</span> For a linear model, <span class="math display">\[
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{x}_i^T\bm{\beta})^2\right).
\]</span> This is maximised with respect to <span class="math inline">\((\bm{\beta},\sigma^2)\)</span> at <span class="math inline">\(\bm{\beta}=\hat{\bm{\beta}}\)</span> and <span class="math inline">\(\sigma^2=\hat \sigma^2=D/n\)</span>. Therefore
<span class="math display">\[\begin{align*}
\max_{\bm{\beta},\sigma^2} L(\bm{\beta},\sigma^2)&amp;=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over{2D}} \sum_{i=1}^n (y_i-\bm{x}_i^T\hat{\bm{\beta}})^2\right)\cr
&amp;=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over2}\right).
\end{align*}\]</span>
<p>This form applies for both <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> and <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span>, with only the model changing. Let the deviances under models <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> be denoted by <span class="math inline">\(D_0\)</span> and <span class="math inline">\(D_1\)</span> respectively. Then the critical region for the likelihood ratio test is of the form <span class="math display">\[{{(2\pi D_1/n)^{-{n\over 2}}}\over{(2\pi D_0/n)^{-{n\over 2}}}}&gt;k\]</span> so <span class="math display">\[\left({{D_0}\over{D_1}}\right)^{n\over 2}&gt;k,\]</span> and <span class="math display">\[\left({{D_0}\over{D_1}}-1\right){{n-p}\over{p-q}}&gt;k&#39;\]</span> for some <span class="math inline">\(k&#39;\)</span>. Rearranging, <span class="math display">\[{{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}&gt;k&#39;.\]</span> We refer to the left hand side of this inequality as the <span class="math inline">\(F\)</span>-statistic. We reject the simpler model <span class="math inline">\(H_0\)</span> in favour of the more complex model <span class="math inline">\(H_1\)</span> if <span class="math inline">\(F\)</span> is ‘too large’.</p>
<p>As we have required <span class="math inline">\(H_0\)</span> to be nested in <span class="math inline">\(H_1\)</span>, <span class="math inline">\(F \sim F_{p-q, \, n-p}\)</span> when <span class="math inline">\(H_0\)</span> is true. To see this, note that <span class="math display">\[
{{D_0}\over\sigma^2}={{D_0-D_1}\over\sigma^2}+{{D_1}\over\sigma^2}.
\]</span> Furthermore, under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(D_1/\sigma^2 \sim \chi^2_{n-p}\)</span> and <span class="math inline">\(D_0/\sigma^2 \sim \chi^2_{n-q}\)</span>. It is possible to show (although we will not do so here) that under <span class="math inline">\(H_0\)</span>, <span class="math inline">\((D_0-D_1)/\sigma^2\)</span> and <span class="math inline">\(D_0/\sigma^2\)</span> are independent. Therefore, from the properties of the chi-squared distribution, it follows that under <span class="math inline">\(H_0\)</span>, <span class="math inline">\((D_0-D_1)/\sigma^2 \sim \chi^2_{p-q}\)</span>, and <span class="math inline">\(F \sim F_{p-q,\,n-p}\)</span> distribution.</p>
<p>Therefore, the precise critical region can be evaluated given the size, <span class="math inline">\(\alpha\)</span>, of the test. We reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> when <span class="math display">\[
{{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}&gt;k
\]</span> where <span class="math inline">\(k\)</span> is the <span class="math inline">\(100(1-\alpha)\%\)</span> point of the <span class="math inline">\(F_{p-q,\,n-p}\)</span> distribution.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="comparing-statistical-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/heogden/math3091/edit/master/lm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH3091.pdf", "MATH3091.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
