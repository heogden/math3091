<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Generalised Linear Models | MATH3091: Statistical Modelling II</title>
  <meta name="description" content="The course notes for MATH3091: Statistical Modelling II" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Generalised Linear Models | MATH3091: Statistical Modelling II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Generalised Linear Models | MATH3091: Statistical Modelling II" />
  
  <meta name="twitter:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="prelim.html"/>
<link rel="next" href="chap-categorical.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH3012: Statistical Modelling II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="prelim.html"><a href="prelim.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="prelim.html"><a href="prelim.html#elements-of-statistical-modelling"><i class="fa fa-check"></i><b>1.1</b> Elements of statistical modelling</a></li>
<li class="chapter" data-level="1.2" data-path="prelim.html"><a href="prelim.html#regression-models"><i class="fa fa-check"></i><b>1.2</b> Regression Models</a></li>
<li class="chapter" data-level="1.3" data-path="prelim.html"><a href="prelim.html#example-data-to-be-analysed"><i class="fa fa-check"></i><b>1.3</b> Example data to be analysed</a><ul>
<li class="chapter" data-level="1.3.1" data-path="prelim.html"><a href="prelim.html#nitric-nitric-acid"><i class="fa fa-check"></i><b>1.3.1</b> <code>nitric</code>: Nitric acid</a></li>
<li class="chapter" data-level="1.3.2" data-path="prelim.html"><a href="prelim.html#birth-weight-of-newborn-babies"><i class="fa fa-check"></i><b>1.3.2</b> <code>birth</code>: Weight of newborn babies</a></li>
<li class="chapter" data-level="1.3.3" data-path="prelim.html"><a href="prelim.html#survival-time-to-death"><i class="fa fa-check"></i><b>1.3.3</b> <code>survival</code>: Time to death</a></li>
<li class="chapter" data-level="1.3.4" data-path="prelim.html"><a href="prelim.html#beetle-mortality-from-carbon-disulphide"><i class="fa fa-check"></i><b>1.3.4</b> <code>beetle</code>: Mortality from carbon disulphide</a></li>
<li class="chapter" data-level="1.3.5" data-path="prelim.html"><a href="prelim.html#shuttle-challenger-disaster"><i class="fa fa-check"></i><b>1.3.5</b> <code>shuttle</code>: Challenger disaster</a></li>
<li class="chapter" data-level="1.3.6" data-path="prelim.html"><a href="prelim.html#heart-treatment-for-heart-attack"><i class="fa fa-check"></i><b>1.3.6</b> <code>heart</code>: Treatment for heart attack</a></li>
<li class="chapter" data-level="1.3.7" data-path="prelim.html"><a href="prelim.html#accident-road-traffic-accidents"><i class="fa fa-check"></i><b>1.3.7</b> <code>accident</code>: Road traffic accidents</a></li>
<li class="chapter" data-level="1.3.8" data-path="prelim.html"><a href="prelim.html#lymphoma-lymphoma-patients"><i class="fa fa-check"></i><b>1.3.8</b> <code>lymphoma</code>: Lymphoma patients</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="prelim.html"><a href="prelim.html#likelihood-based-statistical-theory"><i class="fa fa-check"></i><b>1.4</b> Likelihood-based statistical theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="prelim.html"><a href="prelim.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.4.1</b> The likelihood function</a></li>
<li class="chapter" data-level="1.4.2" data-path="prelim.html"><a href="prelim.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="prelim.html"><a href="prelim.html#score"><i class="fa fa-check"></i><b>1.4.3</b> Score</a></li>
<li class="chapter" data-level="1.4.4" data-path="prelim.html"><a href="prelim.html#info"><i class="fa fa-check"></i><b>1.4.4</b> Information</a></li>
<li class="chapter" data-level="1.4.5" data-path="prelim.html"><a href="prelim.html#sn:asnmle"><i class="fa fa-check"></i><b>1.4.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.4.6" data-path="prelim.html"><a href="prelim.html#comparing-statistical-models"><i class="fa fa-check"></i><b>1.4.6</b> Comparing statistical models</a></li>
<li class="chapter" data-level="1.4.7" data-path="prelim.html"><a href="prelim.html#sn:lrt"><i class="fa fa-check"></i><b>1.4.7</b> The log-likelihood ratio statistic</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="prelim.html"><a href="prelim.html#sn:lm"><i class="fa fa-check"></i><b>1.5</b> Linear Models</a><ul>
<li class="chapter" data-level="1.5.1" data-path="prelim.html"><a href="prelim.html#introduction"><i class="fa fa-check"></i><b>1.5.1</b> Introduction</a></li>
<li class="chapter" data-level="1.5.2" data-path="prelim.html"><a href="prelim.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.5.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.5.3" data-path="prelim.html"><a href="prelim.html#properties-of-the-mle"><i class="fa fa-check"></i><b>1.5.3</b> Properties of the MLE</a></li>
<li class="chapter" data-level="1.5.4" data-path="prelim.html"><a href="prelim.html#comparing-linear-models"><i class="fa fa-check"></i><b>1.5.4</b> Comparing linear models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>2</b> Generalised Linear Models</a><ul>
<li class="chapter" data-level="2.1" data-path="glm.html"><a href="glm.html#sn:ef"><i class="fa fa-check"></i><b>2.1</b> The Exponential family</a></li>
<li class="chapter" data-level="2.2" data-path="glm.html"><a href="glm.html#components-of-a-generalised-linear-model"><i class="fa fa-check"></i><b>2.2</b> Components of a generalised linear model</a><ul>
<li class="chapter" data-level="2.2.1" data-path="glm.html"><a href="glm.html#the-random-component"><i class="fa fa-check"></i><b>2.2.1</b> The random component</a></li>
<li class="chapter" data-level="2.2.2" data-path="glm.html"><a href="glm.html#the-systematic-or-structural-component"><i class="fa fa-check"></i><b>2.2.2</b> The systematic (or structural) component</a></li>
<li class="chapter" data-level="2.2.3" data-path="glm.html"><a href="glm.html#the-link-function"><i class="fa fa-check"></i><b>2.2.3</b> The link function</a></li>
<li class="chapter" data-level="2.2.4" data-path="glm.html"><a href="glm.html#the-linear-model"><i class="fa fa-check"></i><b>2.2.4</b> The linear model</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="glm.html"><a href="glm.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>2.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4" data-path="glm.html"><a href="glm.html#sn:glminfer"><i class="fa fa-check"></i><b>2.4</b> Inference</a></li>
<li class="chapter" data-level="2.5" data-path="glm.html"><a href="glm.html#sn:compglm"><i class="fa fa-check"></i><b>2.5</b> Comparing generalised linear models</a><ul>
<li class="chapter" data-level="2.5.1" data-path="glm.html"><a href="glm.html#sn:glmlrt"><i class="fa fa-check"></i><b>2.5.1</b> The generalised likelihood ratio test</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="glm.html"><a href="glm.html#scaled-deviance-and-the-saturated-model"><i class="fa fa-check"></i><b>2.6</b> Scaled deviance and the saturated model</a></li>
<li class="chapter" data-level="2.7" data-path="glm.html"><a href="glm.html#sn:unknowndisp"><i class="fa fa-check"></i><b>2.7</b> Models with unknown <span class="math inline">\(a(\phi)\)</span></a></li>
<li class="chapter" data-level="2.8" data-path="glm.html"><a href="glm.html#residuals"><i class="fa fa-check"></i><b>2.8</b> Residuals</a></li>
<li class="chapter" data-level="2.9" data-path="glm.html"><a href="glm.html#example-binary-regression"><i class="fa fa-check"></i><b>2.9</b> Example: Binary Regression</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-categorical.html"><a href="chap-categorical.html"><i class="fa fa-check"></i><b>3</b> Categorical data</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-categorical.html"><a href="chap-categorical.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:multinomial"><i class="fa fa-check"></i><b>3.2</b> Multinomial sampling</a></li>
<li class="chapter" data-level="3.3" data-path="chap-categorical.html"><a href="chap-categorical.html#product-multinomial-sampling"><i class="fa fa-check"></i><b>3.3</b> Product multinomial sampling</a></li>
<li class="chapter" data-level="3.4" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:loginter"><i class="fa fa-check"></i><b>3.4</b> Interpreting log-linear models</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3091: Statistical Modelling II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="glm" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Generalised Linear Models</h1>
<div id="sn:ef" class="section level2">
<h2><span class="header-section-number">2.1</span> The Exponential family</h2>
A probability distribution is said to be a member of the exponential family if its probability density function (or probability function, if discrete) can be written in the form
<span class="math display" id="eq:ef">\[\begin{equation}
  f_Y(y;\theta,\phi)=\exp\left({{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\right).
  \tag{2.1}
\end{equation}\]</span>
<p>The parameter <span class="math inline">\(\theta\)</span> is called the <em>natural</em> or <em>canonical</em> parameter. The parameter <span class="math inline">\(\phi\)</span> is usually assumed known. If it is unknown then it is often called the <em>nuisance</em> parameter.</p>
<p>The density <a href="glm.html#eq:ef">(2.1)</a> can be thought of as a likelihood resulting from a single observation <span class="math inline">\(y\)</span>. Then the log-likelihood is <span class="math display">\[\ell(\theta,\phi)={{y\theta-b(\theta)}\over{a(\phi)}} +c(y,\phi)\]</span> and the score is <span class="math display">\[u(\theta)=\frac{\partial}{\partial \theta}\ell(\theta,\phi)
={{y-\frac{\partial}{\partial \theta} b(\theta)}\over{a(\phi)}}
={{y- b&#39;(\theta)}\over{a(\phi)}}.\]</span> The Hessian is <span class="math display">\[H(\theta)=\frac{\partial^2}{\partial \theta^2}\ell(\theta,\phi)
=-{{\frac{\partial^2}{\partial \theta^2} b(\theta)}\over{a(\phi)}}
=-{{b&#39;&#39;(\theta)}\over{a(\phi)}}\]</span> so the expected information is <span class="math display">\[{\cal I}(\theta)=E[-H(\theta)]={{b&#39;&#39;(\theta)}\over{a(\phi)}}.\]</span> From the properties of the score function in Section <a href="prelim.html#score">1.4.3</a>, we know that <span class="math inline">\(E[U(\theta)]=0\)</span>. Therefore <span class="math display">\[E\left[{{Y- b&#39;(\theta)}\over{a(\phi)}}\right]=0,\]</span> so <span class="math inline">\(E[Y]=b&#39;(\theta)\)</span>. We often denote the mean by <span class="math inline">\(\mu\)</span>, so <span class="math inline">\(\mu=b&#39;(\theta)\)</span>.</p>
<p>Furthermore, <span class="math display">\[
\text{Var}[U(\theta)]=
\text{Var}\left[{{Y- b&#39;(\theta)}\over{a(\phi)}}\right]=
{{\text{Var}[Y]}\over{a(\phi)^2}},
\]</span> as <span class="math inline">\(b&#39;(\theta)\)</span> and <span class="math inline">\(a(\phi)\)</span> are constants (not random variables). We also know from Section <a href="prelim.html#info">1.4.4</a> that <span class="math inline">\(\text{Var}[U(\theta)]={\cal I}(\theta)\)</span>. Therefore <span class="math display">\[
\text{Var}[Y]=a(\phi)^2\text{Var}[U(\theta)]=a(\phi)^2 {\cal I}(\theta)
= a(\phi)b&#39;&#39;(\theta).
\]</span> and hence the mean and variance of a random variable with probability density function (or probability function) of the form <a href="glm.html#eq:ef">(2.1)</a> are <span class="math inline">\(b&#39;(\theta)\)</span> and <span class="math inline">\(a(\phi)b&#39;&#39;(\theta)\)</span> respectively.</p>
<p>The variance is the product of two functions; <span class="math inline">\(b&#39;&#39;(\theta)\)</span> depends on the canonical parameter <span class="math inline">\(\theta\)</span> (and hence <span class="math inline">\(\mu\)</span>) only and is called the <em>variance function</em> (<span class="math inline">\(V(\mu)\equiv b&#39;&#39;(\theta)\)</span>); <span class="math inline">\(a(\phi)\)</span> is sometimes of the form <span class="math inline">\(a(\phi)=\sigma^2/w\)</span> where <span class="math inline">\(w\)</span> is a known <em>weight</em> and <span class="math inline">\(\sigma^2\)</span> is called the <em>dispersion parameter</em> or <em>scale parameter</em>.</p>
<strong>Example (Normal distribution)</strong>. Suppose <span class="math inline">\(Y\sim N(\mu, \, \sigma^2)\)</span>. Then
<span class="math display">\[\begin{align*}
f_Y(y;\mu,\sigma^2)&amp;= {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y-\mu)^2\right)\quad\;\; y\in\mathbb{R};\;\;\mu\in\mathbb{R}\cr
&amp;= \exp\left({{y\mu-{1\over 2}\mu^2}\over \sigma^2}-{1\over 2}\left[
{{y^2}\over\sigma^2}+\log(2\pi\sigma^2)\right]\right).
\end{align*}\]</span>
<p>This is in the form <a href="glm.html#eq:ef">(2.1)</a>, with <span class="math inline">\(\theta=\mu\)</span>, <span class="math inline">\(b(\theta)={1\over 2}\theta^2\)</span>, <span class="math inline">\(a(\phi)=\sigma^2\)</span> and <span class="math display">\[c(y,\phi)=-{1\over 2}\left[
{{y^2}\over{a(\phi)}}+\log(2\pi a[\phi])\right].
\]</span> Therefore <span class="math display">\[E(Y)=b&#39;(\theta)=\theta=\mu,\]</span> <span class="math display">\[\text{Var}(Y)=a(\phi)b&#39;&#39;(\theta)=\sigma^2\]</span> and the variance function is <span class="math display">\[V(\mu)=1.\]</span></p>
<strong>Example (Poisson distribution)</strong>. Suppose <span class="math inline">\(Y\sim \text{Poisson}(\lambda)\)</span>. Then
<span class="math display">\[\begin{align*}
f_Y(y;\lambda)&amp;= {{\exp(-\lambda)\lambda^y}\over{y!}}
\qquad y\in\{0,1,\ldots\};\quad\lambda\in{\cal R}_+\cr
&amp;= \exp\left(y\log\lambda-\lambda-\log y!\right).
\end{align*}\]</span>
<p>This is in the form <a href="glm.html#eq:ef">(2.1)</a>, with <span class="math inline">\(\theta=\log\lambda\)</span>, <span class="math inline">\(b(\theta)=\exp\theta\)</span>, <span class="math inline">\(a(\phi)=1\)</span> and <span class="math inline">\(c(y,\phi)=-\log y!\)</span>. Therefore <span class="math display">\[E(Y)=b&#39;(\theta)=\exp\theta=\lambda,\]</span> <span class="math display">\[\text{Var}(Y)=a(\phi)b&#39;&#39;(\theta)=\exp\theta=\lambda\]</span> and the variance function is <span class="math display">\[V(\mu)=\mu.\]</span></p>
<strong>Example (Bernoulli distribution)</strong>. Suppose <span class="math inline">\(Y\sim \text{Bernoulli}(p)\)</span>. Then
<span class="math display">\[\begin{align*}
f_Y(y;p)&amp;= p^y(1-p)^{1-y}\qquad y\in\{0,1\};\quad
p\in(0,1)\cr
&amp;= \exp\left(y\log{p\over{1-p}}+\log(1-p)\right)
\end{align*}\]</span>
<p>This is in the form <a href="glm.html#eq:ef">(2.1)</a>, with <span class="math inline">\(\theta=\log{p\over{1-p}}\)</span>, <span class="math inline">\(b(\theta)=\log(1+\exp\theta)\)</span>, <span class="math inline">\(a(\phi)=1\)</span> and <span class="math inline">\(c(y,\phi)=0\)</span>. Therefore <span class="math display">\[E(Y)=b&#39;(\theta)={{\exp\theta}\over{1+\exp\theta}}=p,\]</span> <span class="math display">\[\text{Var}(Y)=a(\phi)b&#39;&#39;(\theta)={{\exp\theta}\over{(1+\exp\theta})^2}=p(1-p)\]</span> and the variance function is <span class="math display">\[V(\mu)=\mu(1-\mu).\]</span></p>
<strong>Example (Binomial distribution)</strong>. Suppose <span class="math inline">\(Y^*\sim \text{Binomial}(n,p)\)</span>. Here, <span class="math inline">\(n\)</span> is assumed known (as usual) and the random variable <span class="math inline">\(Y= Y^*/n\)</span> is taken as the <em>proportion</em> of successes, so
<span class="math display">\[\begin{align*}
f_Y(y;p)&amp;=\left({n\atop{ny}}\right) p^{ny} (1-p)^{n(1-y)}\qquad
y\in\left\{0,{1\over n},{2\over n},\ldots ,1\right\};  \quad p\in(0,1)\cr
&amp;= \exp\left({{y\log{p\over{1-p}}+\log(1-p)}\over{1\over n}}
+\log\!\left({n\atop{ny}}\right)\right).
\end{align*}\]</span>
<p>This is in the form <a href="glm.html#eq:ef">(2.1)</a>, with <span class="math inline">\(\theta=\log{p\over{1-p}}\)</span>, <span class="math inline">\(b(\theta)=\log(1+\exp\theta)\)</span>, <span class="math inline">\(a(\phi)={1\over n}\)</span> and <span class="math inline">\(c(y,\phi)=\log\!\left({n\atop{ny}}\right)\)</span>. Therefore <span class="math display">\[E(Y)=b&#39;(\theta)={{\exp\theta}\over{1+\exp\theta}}=p,\]</span> <span class="math display">\[\text{Var}(Y)=a(\phi)b&#39;&#39;(\theta)={1\over n}{{\exp\theta}\over{(1+\exp\theta})^2}=
{{p(1-p)}\over n}\]</span> and the variance function is <span class="math display">\[V(\mu)=\mu(1-\mu).\]</span> Here, we can write <span class="math inline">\(a(\phi)\equiv \sigma^2/w\)</span> where the scale parameter <span class="math inline">\(\sigma^2=1\)</span> and the weight <span class="math inline">\(w\)</span> is <span class="math inline">\(n\)</span>, the binomial denominator.</p>
</div>
<div id="components-of-a-generalised-linear-model" class="section level2">
<h2><span class="header-section-number">2.2</span> Components of a generalised linear model</h2>
<div id="the-random-component" class="section level3">
<h3><span class="header-section-number">2.2.1</span> The random component</h3>
As in a linear model, the aim is to determine the pattern of dependence of a response variable on explanatory variables. We denote the <span class="math inline">\(n\)</span> observations of the response by <span class="math inline">\(\bm{y}=(y_1,y_2,\ldots ,y_n)^T\)</span>. In a generalised linear model (GLM), these are assumed to be observations of <em>independent</em> random variables <span class="math inline">\(\bm{Y}=(Y_1,Y_2,\ldots ,Y_n)^T\)</span>, which take the same distribution from the exponential family. In other words, the functions <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> and usually the scale parameter <span class="math inline">\(\phi\)</span> are the same for all observations, but the canonical parameter <span class="math inline">\(\theta\)</span> may differ. Therefore, we write <span class="math display">\[
f_{Y_i}(y_i;\theta_i,\phi_i)=
\exp\left({{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+c(y_i,\phi_i)\right)
\]</span> and the joint density for <span class="math inline">\(\bm{Y}=(Y_1,Y_2,\ldots ,Y_n)^T\)</span> is
<span class="math display" id="eq:glmrandom">\[\begin{align}
f_{\bm{Y}}(\bm{y};\bm{\theta},\bm{\phi})
&amp;= \prod_{i=1}^n f_{Y_i}(y_i;\theta_i,\phi_i) \cr
&amp;= \exp\left(\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\right) \tag{2.2}
\end{align}\]</span>
<p>where <span class="math inline">\(\bm{\theta}=(\theta_1,\ldots ,\theta_n)^T\)</span> is the collection of canonical parameters and <span class="math inline">\(\bm{\phi}=(\phi_1,\ldots ,\phi_n)^T\)</span> is the collection of nuisance parameters (where they exist).</p>
<p>Note that for a particular sample of observed responses, <span class="math inline">\(\bm{y}=(y_1,y_2,\ldots ,y_n)^T\)</span>, <a href="glm.html#eq:glmrandom">(2.2)</a> is the likelihood function <span class="math inline">\(L(\bm{\theta}, \bm{\phi})\)</span> for <span class="math inline">\(\bm{\theta}\)</span> and <span class="math inline">\(\bm{\phi}\)</span>.</p>
</div>
<div id="the-systematic-or-structural-component" class="section level3">
<h3><span class="header-section-number">2.2.2</span> The systematic (or structural) component</h3>
Associated with each <span class="math inline">\(y_i\)</span> is a vector <span class="math inline">\(\bm{x}_i=(x_{i1},x_{i2},\ldots ,x_{ip})^T\)</span> of <span class="math inline">\(p\)</span> explanatory variables. In a generalised linear model, the distribution of the response variable <span class="math inline">\(Y_i\)</span> depends on <span class="math inline">\(\bm{x}_i\)</span> through the <em>linear predictor</em> <span class="math inline">\(\eta_i\)</span> where
<span class="math display" id="eq:glmsys">\[\begin{align}
\eta_i &amp;=\beta_1 x_{i1} +\beta_2 x_{i2} +\ldots + \beta_p x_{ip} \notag \\
&amp;= \sum_{j=1}^p x_{ij} \beta_j \notag \\
&amp;=  \bm{x}_i^T \bm{\beta} \notag \\
&amp;= [\bm{X}\bm{\beta}]_i,\qquad i=1,\ldots ,n,
  \tag{2.3}
\end{align}\]</span>
where, as with a linear model, <span class="math display">\[
\bm{X}=\begin{pmatrix} \bm{x}_1^T\cr\vdots\cr \bm{x}_n^T \end{pmatrix}
=\begin{pmatrix}
x_{11}&amp;\cdots&amp;x_{1p}\cr\vdots&amp;\ddots&amp;\vdots\cr x_{n1}&amp;\cdots&amp;x_{np}\end{pmatrix}
\]</span> and <span class="math inline">\(\bm{\beta}=(\beta_1,\ldots ,\beta_p)^T\)</span> is a vector of fixed but unknown parameters describing the dependence of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(\bm{x}_i\)</span>. The four ways of describing the linear predictor in <a href="glm.html#eq:glmsys">(2.3)</a> are equivalent, but the most economical is the matrix form
<span class="math display" id="eq:eta">\[\begin{equation}
\bm{\eta}=\bm{X}\bm{\beta}.
  \tag{2.4}
\end{equation}\]</span>
<p>Again, we call the <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(\bm{X}\)</span> the <em>design matrix</em>. The <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\bm{X}\)</span> is <span class="math inline">\(\bm{x}_i^T\)</span>, the explanatory data corresponding to the <span class="math inline">\(i\)</span>th observation of the response. The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\bm{X}\)</span> contains the <span class="math inline">\(n\)</span> observations of the <span class="math inline">\(j\)</span>th explanatory variable.</p>
</div>
<div id="the-link-function" class="section level3">
<h3><span class="header-section-number">2.2.3</span> The link function</h3>
<p>For specifying the pattern of dependence of the response variable on the explanatory variables, the canonical parameters <span class="math inline">\(\theta_1,\ldots,\theta_n\)</span> in <a href="glm.html#eq:glmrandom">(2.2)</a> are not of direct interest. Furthermore, we have already specified that the distribution of <span class="math inline">\(Y_i\)</span> should depend on <span class="math inline">\(\bm{x}_i\)</span> through the linear predictor <span class="math inline">\(\eta_i\)</span>. It is the parameters <span class="math inline">\(\beta_1,\ldots ,\beta_p\)</span> of the linear predictor which are of primary interest.</p>
<p>The link between the distribution of <span class="math inline">\(\bm{Y}\)</span> and the linear predictor <span class="math inline">\(\bm{\eta}\)</span> is provided by the <em>link function</em> <span class="math inline">\(g\)</span>, <span class="math display">\[
\eta_i=g(\mu_i),\quad i = 1, \ldots, n,
\]</span> where <span class="math inline">\(\mu_i\equiv E(Y_i),\;i = 1, \ldots, n\)</span>. Hence, the dependence of the distribution of the response on the explanatory variables is established as <span class="math display">\[
g(E[Y_i])=g(\mu_i)=\eta_i=\bm{x}_i^T\bm{\beta},\quad i = 1, \ldots, n,
\]</span></p>
<p>In principle, the link function <span class="math inline">\(g\)</span> can be any one-to-one differentiable function. However, we note that <span class="math inline">\(\eta_i\)</span> can in principle take any value in <span class="math inline">\(\mathbb{R}\)</span> (as we make no restriction on possible values taken by explanatory variables or model parameters). However, for some exponential family distributions <span class="math inline">\(\mu_i\)</span> is restricted. For example, for the Poisson distribution <span class="math inline">\(\mu_i\in\mathbb{R}_+\)</span>; for the Bernoulli distribution <span class="math inline">\(\mu_i\in(0,1)\)</span>. If <span class="math inline">\(g\)</span> is not chosen carefully, then there may exist a possible <span class="math inline">\(\bm{x}_i\)</span> and <span class="math inline">\(\bm{\beta}\)</span> such that <span class="math inline">\(\eta_i\ne g(\mu_i)\)</span> for any possible value of <span class="math inline">\(\mu_i\)</span>. Therefore, ‘sensible’ choices of link function map the set of allowed values for <span class="math inline">\(\mu_i\)</span> onto <span class="math inline">\(\mathbb{R}\)</span>.</p>
Recall that for a random variable <span class="math inline">\(Y\)</span> with a distribution from the exponential family, <span class="math inline">\(E(Y)=b&#39;(\theta)\)</span>. Hence, for a generalised linear model <span class="math display">\[
\mu_i=E(Y_i)=b&#39;(\theta_i),\quad i = 1, \ldots, n.
\]</span> Therefore <span class="math display">\[
\theta_i=b^{&#39;-1}(\mu_i),\quad i = 1, \ldots, n
\]</span> and as <span class="math inline">\(g(\mu_i)=\eta_i=\bm{x}_i^T\bm{\beta}\)</span>, then
<span class="math display" id="eq:thetai">\[\begin{equation}
\theta_i=b^{&#39;-1}(g^{-1}[\bm{x}_i^T\bm{\beta}]),\quad i = 1, \ldots, n.
\tag{2.5}
\end{equation}\]</span>
<p>Hence, we can express the joint density <a href="glm.html#eq:glmrandom">(2.2)</a> in terms of the coefficients <span class="math inline">\(\bm{\beta}\)</span>, and for observed data <span class="math inline">\(\bm{y}\)</span>, this is the likelihood <span class="math inline">\(L(\bm{\beta})\)</span> for <span class="math inline">\(\bm{\beta}\)</span>. As <span class="math inline">\(\bm{\beta}\)</span> is our parameter of real interest (describing the dependence of the response on the explanatory variables) this likelihood will play a crucial role.</p>
<p>Note that considerable simplification is obtained in <a href="glm.html#eq:thetai">(2.5)</a> if the functions <span class="math inline">\(g\)</span> and <span class="math inline">\(b^{&#39;-1}\)</span> are identical. Then <span class="math display">\[
\theta_i=\bm{x}_i^T\bm{\beta}\qquad i = 1, \ldots, n
\]</span> and the resulting likelihood is <span class="math display">\[
L(\bm{\beta})=
\exp\left(\sum_{i=1}^n{{y_i\bm{x}_i^T\bm{\beta}-b(\bm{x}_i^T\bm{\beta})}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)\right).
\]</span> The link function <span class="math display">\[
g(\mu)\equiv b^{&#39;-1}(\mu)
\]</span> is called the <em>canonical</em> link function. Under the canonical link, the canonical parameter is equal to the linear predictor.</p>
<p>The canonical link functions are:</p>
<table style="width:100%;">
<colgroup>
<col width="16%" />
<col width="16%" />
<col width="15%" />
<col width="16%" />
<col width="21%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th><span class="math inline">\(b(\theta)\)</span></th>
<th><span class="math inline">\(b&#39;(\theta)\equiv\mu\)</span></th>
<th><span class="math inline">\(b^{&#39;-1}(\mu)\equiv\theta\)</span></th>
<th>Link</th>
<th>Name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal</td>
<td><span class="math inline">\({1\over 2}\theta^2\)</span></td>
<td><span class="math inline">\(\theta\)</span></td>
<td><span class="math inline">\(\mu\)</span></td>
<td><span class="math inline">\(g(\mu)=\mu\)</span></td>
<td>Identity</td>
</tr>
<tr class="even">
<td>Poisson</td>
<td><span class="math inline">\(\exp\theta\)</span></td>
<td><span class="math inline">\(\exp\theta\)</span></td>
<td><span class="math inline">\(\log\mu\)</span></td>
<td><span class="math inline">\(g(\mu)=\log\mu\)</span></td>
<td>Log</td>
</tr>
<tr class="odd">
<td>Binomial</td>
<td><span class="math inline">\(\log(1+\exp\theta)\)</span></td>
<td><span class="math inline">\(\frac{\exp\theta}{1+\exp\theta}\)</span></td>
<td><span class="math inline">\(\log{\frac{\mu}{1-\mu}}\)</span></td>
<td><span class="math inline">\(g(\mu)=\log{\frac{\mu}{1-\mu}}\)</span></td>
<td>Logit</td>
</tr>
</tbody>
</table>
</div>
<div id="the-linear-model" class="section level3">
<h3><span class="header-section-number">2.2.4</span> The linear model</h3>
<p>The linear model considered in Section <a href="prelim.html#sn:lm">1.5</a> is also a generalised linear model. We assume <span class="math inline">\({Y_1,\ldots ,Y_n}\)</span> are independent normally distributed random variables, and the normal distribution is a member of the exponential family.</p>
<p>Furthermore, the explanatory variables enter a linear model through the linear predictor <span class="math display">\[
\eta_i=\bm{x}_i^T\bm{\beta}, \quad i = 1, \ldots, n.
\]</span></p>
<p>Finally, the link between <span class="math inline">\(E(\bm{Y})=\bm{\mu}\)</span> and the linear predictor <span class="math inline">\(\bm{\eta}\)</span> is through the (canonical) identity link function <span class="math display">\[
\mu_i=\eta_i, \quad i = 1, \ldots, n.
\]</span></p>
</div>
</div>
<div id="maximum-likelihood-estimation-2" class="section level2">
<h2><span class="header-section-number">2.3</span> Maximum likelihood estimation</h2>
<p>The regression coefficients <span class="math inline">\({\beta_1,\ldots ,\beta_p}\)</span> describe the pattern by which the response depends on the explanatory variables. We use the observed data <span class="math inline">\({y_1,\ldots ,y_n}\)</span> to <em>estimate</em> this pattern of dependence.</p>
As usual, we maximise the log-likelihood function which, from <a href="glm.html#eq:glmrandom">(2.2)</a>, can be written
<span class="math display" id="eq:glmloglikelihood">\[\begin{equation}
\ell(\bm{\beta},\bm{\phi})=
\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+\sum_{i=1}^nc(y_i,\phi_i)
  \tag{2.6}
\end{equation}\]</span>
and depends on <span class="math inline">\(\bm{\beta}\)</span> through
<span class="math display">\[\begin{align*}
\theta_i &amp;= (b&#39;)^{-1}(\mu_i), \cr
\mu_i&amp;= g^{-1}(\eta_i), \cr
\eta_i&amp;=\bm{x}_i^T\bm{\beta}=\sum_{i=1}^p x_{ij} \beta_j, \quad i = 1, \ldots, n.
\end{align*}\]</span>
<p>To find <span class="math inline">\(\hat{\bm{\beta}}\)</span>, we consider the scores <span class="math display">\[
u_k(\bm{\beta})={\partial\over{\partial\beta_k}}
\ell(\bm{\beta},\bm{\phi})\qquad k=1,\ldots ,p
\]</span> and then find <span class="math inline">\(\hat{\bm{\beta}}\)</span> to solve <span class="math inline">\(u_k(\hat{\bm{\beta}})=0\)</span> for <span class="math inline">\(k=1,\ldots ,p.\)</span></p>
From <a href="glm.html#eq:glmloglikelihood">(2.6)</a>
<span class="math display">\[\begin{align*}
u_k(\bm{\beta})&amp;= {\partial\over{\partial\beta_k}}\ell(\bm{\beta},\bm{\phi})\cr
&amp;= {\partial\over{\partial\beta_k}}\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}
+{\partial\over{\partial\beta_k}}\sum_{i=1}^nc(y_i,\phi_i)\cr
&amp;= \sum_{i=1}^n{\partial\over{\partial\beta_k}}
\left[{{y_i\theta_i-b(\theta_i)}\over{a(\phi_i)}}\right]\cr
&amp;=\sum_{i=1}^n{\partial\over{\partial\theta_i}}\left[{{y_i\theta_i-b(\theta_i)}
\over{a(\phi_i)}}\right]{{\partial\theta_i}\over{\partial\mu_i}}
{{\partial\mu_i}\over{\partial\eta_i}}{{\partial\eta_i}\over{\partial\beta_k}}\cr
&amp;= \sum_{i=1}^n{{y_i-b&#39;(\theta_i)}
\over{a(\phi_i)}}{{\partial\theta_i}\over{\partial\mu_i}}
{{\partial\mu_i}\over{\partial\eta_i}}{{\partial\eta_i}\over{\partial\beta_k}}, \quad{k=1,\ldots ,p},\cr
\end{align*}\]</span>
where
<span class="math display">\[\begin{align*}
{{\partial\theta_i}\over{\partial\mu_i}}&amp;=\left[{{\partial\mu_i}\over{\partial\theta_i}}\right]^{-1}
={1\over{b&#39;&#39;(\theta_i)}}\cr
{{\partial\mu_i}\over{\partial\eta_i}}&amp;=\left[{{\partial\eta_i}\over{\partial\mu_i}}\right]^{-1}
={1\over{g&#39;(\mu_i)}}\cr
{{\partial\eta_i}\over{\partial\beta_k}}&amp;=
{\partial\over{\partial\beta_k}}\sum_{j=1}^p x_{ij}\beta_j=x_{ik}.
\end{align*}\]</span>
Therefore
<span class="math display" id="eq:scoreglm">\[\begin{equation}
u_k(\bm{\beta})= \sum_{i=1}^n{{y_i-b&#39;(\theta_i)}\over{a(\phi_i)}}
{{x_{ik}}\over{b&#39;&#39;(\theta_i)g&#39;(\mu_i)}}
=\sum_{i=1}^n{{y_i-\mu_i}\over{\text{Var}(Y_i)}}
{{x_{ik}}\over{g&#39;(\mu_i)}},\quad{k=1,\ldots ,p},
  \tag{2.7}
\end{equation}\]</span>
<p>which depends on <span class="math inline">\(\bm{\beta}\)</span> through <span class="math inline">\(\mu_i\equiv E(Y_i)\)</span> and <span class="math inline">\(\text{Var}(Y_i),\)</span> <span class="math inline">\(i = 1, \ldots, n\)</span>.</p>
<p>In theory, we solve the <span class="math inline">\(p\)</span> simultaneous equations <span class="math inline">\(u_k(\hat{\bm{\beta}})=0,\;{k=1,\ldots ,p}\)</span> to evaluate <span class="math inline">\(\hat{\bm{\beta}}\)</span>. In practice, these equations are usually non-linear and have no analytic solution. Therefore, we rely on numerical methods to solve them.</p>
First, we note that the Hessian and Fisher information matrices can be derived directly from <a href="glm.html#eq:scoreglm">(2.7)</a>. <span class="math display">\[
[\bm{H}(\bm{\beta})]_{jk}={{\partial^2}\over{\partial\beta_j\partial\beta_k}}\ell(\bm{\beta},\bm{\phi})
={\partial\over{\partial\beta_j}}u_k(\bm{\beta}).
\]</span> Therefore
<span class="math display">\[\begin{align*}
[\bm{H}(\bm{\beta})]_{jk}
&amp;={\partial\over{\partial\beta_j}}\sum_{i=1}^n{{y_i-\mu_i}\over{\text{Var}(Y_i)}}
{{x_{ik}}\over{g&#39;(\mu_i)}}\cr
&amp;=\sum_{i=1}^n{{-{{\partial\mu_i}\over{\partial\beta_j}}}\over{\text{Var}(Y_i)}}
{{x_{ik}}\over{g&#39;(\mu_i)}} +\sum_{i=1}^n(y_i-\mu_i){\partial\over{\partial\beta_j}}
\left[{{x_{ik}}\over{\text{Var}(Y_i)
g&#39;(\mu_i)}}\right]
\end{align*}\]</span>
and
<span class="math display">\[\begin{align*}
[{\cal I}(\bm{\beta})]_{jk}
&amp;=\sum_{i=1}^n{{{{\partial\mu_i}\over{\partial\beta_j}}}\over{\text{Var}(Y_i)}}
{{x_{ik}}\over{g&#39;(\mu_i)}} -\sum_{i=1}^n(E[Y_i]-\mu_i){\partial\over{\partial\beta_j}}
\left[{{x_{ik}}\over{\text{Var}(Y_i)
g&#39;(\mu_i)}}\right]\cr
&amp;=\sum_{i=1}^n{{{{\partial\mu_i}\over{\partial\beta_j}}}\over{\text{Var}(Y_i)}}
{{x_{ik}}\over{g&#39;(\mu_i)}}\cr
&amp;=\sum_{i=1}^n{{x_{ij}x_{ik}}\over{\text{Var}(Y_i)g&#39;(\mu_i)^2}}.
\end{align*}\]</span>
Hence we can write
<span class="math display" id="eq:infoglm">\[\begin{equation}
{\cal I}(\bm{\beta})=\bm{X}^T\bm{W}\bm{X}
  \tag{2.8}
\end{equation}\]</span>
<p>where <span class="math display">\[
\bm{X}=\begin{pmatrix} \bm{x}_1^T\cr\vdots\cr \bm{x}_n^T \end{pmatrix}
=\begin{pmatrix}
x_{11}&amp;\cdots&amp;x_{1p}\cr\vdots&amp;\ddots&amp;\vdots\cr x_{n1}&amp;\cdots&amp;x_{np}
\end{pmatrix},
\]</span> <span class="math display">\[
\bm{W}={\rm diag}(\bm{w})=
\begin{pmatrix}
w_1&amp;0&amp;\cdots&amp;0\cr
0&amp;w_2&amp;&amp;\vdots\cr
\vdots&amp;&amp;\ddots&amp;0\cr
0&amp;\cdots&amp;0&amp;w_n
\end{pmatrix}
\]</span> and <span class="math display">\[
w_i={1\over{\text{Var}(Y_i)g&#39;(\mu_i)^2}},\quad i = 1, \ldots, n.
\]</span> The Fisher information matrix <span class="math inline">\(\mathcal{I}(\bm{\beta})\)</span> depends on <span class="math inline">\(\bm{\beta}\)</span> through <span class="math inline">\(\bm{\mu}\)</span> and <span class="math inline">\(\text{Var}(Y_i),\;i = 1, \ldots, n\)</span>.</p>
We notice that the score in <a href="glm.html#eq:scoreglm">(2.7)</a> may now be written as <span class="math display">\[u_k(\bm{\beta})=\sum_{i=1}^n(y_i-\mu_i)x_{ik}w_ig&#39;(\mu_i)
=\sum_{i=1}^n x_{ik}w_iz_i,\quad{k=1,\ldots ,p},\]</span> where <span class="math display">\[
z_i=(y_i-\mu_i)g&#39;(\mu_i),\quad i = 1, \ldots, n.
\]</span> Therefore
<span class="math display" id="eq:scoreglmsimple">\[\begin{equation}
\bm{u}(\bm{\beta})=\bm{X}^T\bm{W}\bm{z}.
  \tag{2.9}
\end{equation}\]</span>
<p>One possible method to solve the <span class="math inline">\(p\)</span> simultaneous equations <span class="math inline">\({\bm{u}}(\hat{\bm{\beta}})={\bf 0}\)</span> that give <span class="math inline">\(\hat{\bm{\beta}}\)</span> is the (multivariate) Newton-Raphson method.</p>
If <span class="math inline">\(\bm{\beta}^{(m)}\)</span> is the current estimate of <span class="math inline">\(\hat{\bm{\beta}}\)</span> then the next estimate is
<span class="math display" id="eq:NRiter">\[\begin{equation}
\bm{\beta}^{(m+1)}=\bm{\beta}^{(m)}-\bm{H}(\bm{\beta}^{(m)})^{-1}\bm{u}(\bm{\beta}^{(m)}).
\tag{2.10}
\end{equation}\]</span>
In practice, an alternative to Newton-Raphson replaces <span class="math inline">\(\bm{H}(\bm{\theta})\)</span> in <a href="glm.html#eq:NRiter">(2.10)</a> with <span class="math inline">\(E[\bm{H}(\bm{\theta})]\equiv-\mathcal{I}(\bm{\beta})\)</span>. Therefore, if <span class="math inline">\(\bm{\beta}^{(m)}\)</span> is the current estimate of <span class="math inline">\(\hat{\bm{\beta}}\)</span> then the next estimate is
<span class="math display" id="eq:FSiter">\[\begin{equation}
\bm{\beta}^{(m+1)}=\bm{\beta}^{(m)}+{\cal I}(\bm{\beta}^{(m)})^{-1}\bm{u}(\bm{\beta}^{(m)}).
\tag{2.11}
\end{equation}\]</span>
The resulting iterative algorithm is called <em>Fisher scoring</em>. Notice that if we substitute <a href="glm.html#eq:infoglm">(2.8)</a> and <a href="glm.html#eq:scoreglmsimple">(2.9)</a> into <a href="glm.html#eq:FSiter">(2.11)</a> we get
<span class="math display">\[\begin{align*}
\bm{\beta}^{(m+1)}&amp;=\bm{\beta}^{(m)}+[\bm{X}^T\bm{W}^{(m)}\bm{X}]^{-1}\bm{X}^T\bm{W}^{(m)}\bm{z}^{(m)}\cr
&amp;=[\bm{X}^T\bm{W}^{(m)}\bm{X}]^{-1}[\bm{X}^T\bm{W}^{(m)}\bm{X}\bm{\beta}^{(m)}+\bm{X}^T\bm{W}^{(m)}\bm{z}^{(m)}]\cr
&amp;=[\bm{X}^T\bm{W}^{(m)}\bm{X}]^{-1}\bm{X}^T\bm{W}^{(m)}[\bm{X}\bm{\beta}^{(m)}+\bm{z}^{(m)}]\cr
&amp;=[\bm{X}^T\bm{W}^{(m)}\bm{X}]^{-1}\bm{X}^T\bm{W}^{(m)}[\bm{\eta}^{(m)}+\bm{z}^{(m)}],
\end{align*}\]</span>
<p>where <span class="math inline">\(\bm{\eta}^{(m)},\,\bm{W}^{(m)}\)</span> and <span class="math inline">\(\bm{z}^{(m)}\)</span> are all functions of <span class="math inline">\(\bm{\beta}^{(m)}\)</span>.</p>
<p>Note that this is a weighted least squares equation, that is <span class="math inline">\(\bm{\beta}^{(m+1)}\)</span> minimises the weighted sum of squares <span class="math display">\[
(\bm{\eta}+\bm{z}-\bm{X}\bm{\beta})^T\bm{W}(\bm{\eta}+\bm{z}-\bm{X}\bm{\beta})=
\sum_{i=1}^n w_i\left(\eta_i+z_i-\bm{x}_i^T\bm{\beta}\right)^2
\]</span> as a function of <span class="math inline">\(\bm{\beta}\)</span> where <span class="math inline">\(w_1,\ldots ,w_n\)</span> are the weights and <span class="math inline">\(\bm{\eta}+\bm{z}\)</span> is called the <em>adjusted dependent variable</em>. Therefore, the Fisher scoring algorithm proceeds as follows.</p>
<ol style="list-style-type: decimal">
<li>Choose an initial estimate <span class="math inline">\(\bm{\beta}^{(m)}\)</span> for <span class="math inline">\(\hat{\bm{\beta}}\)</span> at <span class="math inline">\(m=0\)</span>.</li>
<li>Evaluate <span class="math inline">\(\bm{\eta}^{(m)},\,\bm{W}^{(m)}\)</span> and <span class="math inline">\(\bm{z}^{(m)}\)</span> at <span class="math inline">\(\bm{\beta}^{(m)}\)</span>.</li>
<li>Calculate <span class="math display">\[\bm{\beta}^{(m+1)} =[\bm{X}^T\bm{W}^{(m)}\bm{X}]^{-1}\bm{X}^T\bm{W}^{(m)}[\bm{\eta}^{(m)}+\bm{z}^{(m)}].\]</span></li>
<li>If <span class="math inline">\(||\bm{\beta}^{(m+1)}-\bm{\beta}^{(m)} ||&gt; \epsilon\)</span>, for some prespecified (small) tolerance <span class="math inline">\(\epsilon\)</span> then set <span class="math inline">\(m\to m+1\)</span> and go to 2.</li>
<li>Use <span class="math inline">\(\bm{\beta}^{(m+1)}\)</span> as the solution for <span class="math inline">\(\hat{\bm{\beta}}\)</span>.</li>
</ol>
<p>As this algorithm involves iteratively minimising a weighted sum of squares, it is sometimes known as <em>iteratively (re)weighted least squares</em>.</p>
<p><strong>Notes</strong></p>
<ol style="list-style-type: decimal">
<li>Recall that the canonical link function is <span class="math inline">\(g(\mu)=b^{&#39;-1}(\mu)\)</span> and with this link <span class="math inline">\(\eta_i=g(\mu_i)=\theta_i\)</span>. Then <span class="math display">\[
{1\over{g&#39;(\mu_i)}}={{\partial\mu_i}\over{\partial\eta_i}}
={{\partial\mu_i}\over{\partial\theta_i}}=b&#39;&#39;(\theta_i),\quad i = 1, \ldots, n.
\]</span> Therefore <span class="math inline">\(\text{Var}(Y_i)g&#39;(\mu_i)=a(\phi_i)\)</span> which does not depend on <span class="math inline">\(\bm{\beta}\)</span>, and hence <span class="math display">\[
{\partial\over{\partial\beta_j}}\left[{{x_{ik}}\over{\text{Var}(Y_i)g&#39;(\mu_i)}}\right]=0
\]</span> for all <span class="math inline">\(j=1,\ldots ,p\)</span>. It follows that <span class="math inline">\(\bm{H}(\bm{\theta})=-\mathcal{I}(\bm{\beta})\)</span> and, for the canonical link, Newton-Raphson and Fisher scoring are equivalent.</li>
<li>The linear model is a generalised linear model with identity link, <span class="math inline">\(\eta_i=g(\mu_i)=\mu_i\)</span> and <span class="math inline">\(\text{Var}(Y_i)=\sigma^2\)</span> for all <span class="math inline">\(i = 1, \ldots, n\)</span>. Therefore <span class="math inline">\(w_i=[\text{Var}(Y_i)g&#39;(\mu_i)^2]^{-1}=\sigma^{-2}\)</span> and <span class="math inline">\(z_i=(y_i-\mu_i)g&#39;(\mu_i)=y_i-\eta_i\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span>. Hence <span class="math inline">\(\bm{z}+\bm{\eta}=\bm{y}\)</span> and <span class="math inline">\(\bm{W}=\sigma^{-2}\bm{I}\)</span>, neither of which depend on <span class="math inline">\(\bm{\beta}\)</span>. So the Fisher scoring algorithm converges in a single iteration to the usual least squares estimate.</li>
<li>Estimation of an unknown scale parameter <span class="math inline">\(\sigma^2\)</span> is discussed later. A common (to all <span class="math inline">\(i\)</span>) <span class="math inline">\(\sigma^2\)</span> has no effect on <span class="math inline">\(\hat{\bm{\beta}}\)</span>.</li>
</ol>
</div>
<div id="sn:glminfer" class="section level2">
<h2><span class="header-section-number">2.4</span> Inference</h2>
<p>Recall from Section <a href="prelim.html#sn:asnmle">1.4.5</a> that the maximum likelihood estimator <span class="math inline">\(\hat{\bm{\beta}}\)</span> is asymptotically normally distributed with mean <span class="math inline">\(\bm{\beta}\)</span> (it is unbiased) and variance covariance matrix <span class="math inline">\({\cal I}(\bm{\beta})^{-1}\)</span>. For ‘large enough <span class="math inline">\(n\)</span>’ we treat this distribution as an approximation.</p>
<p>Therefore, standard errors (estimated standard deviations) are given by <span class="math display">\[
s.e.(\hat{\beta}_i)=[{\cal I}(\hat{\bm{\beta}})^{-1}]_{ii}^{{1\over 2}}
=[(\bm{X}^T\hat{\bm{W}}\bm{X})^{-1}]_{ii}^{{1\over 2}}
\qquad i=1,\ldots ,p.
\]</span> where the diagonal matrix <span class="math inline">\(\hat{\bm{W}}={\rm diag}(\hat{\bm{w}})\)</span> is evaluated at <span class="math inline">\(\hat{\bm{\beta}}\)</span>, that is <span class="math inline">\(\hat{w}_i=(\hat{\text{Var}}(Y_i)g&#39;(\hat{\mu}_i)^2)^{-1}\)</span> where <span class="math inline">\(\hat{\mu}_i\)</span> and <span class="math inline">\(\hat{\text{Var}}(Y_i)\)</span> are evaluated at <span class="math inline">\(\hat{\bm{\beta}}\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span>. Furthermore, if <span class="math inline">\(\text{Var}(Y_i)\)</span> depends on an unknown scale parameter, then this too must be estimated in the standard error.</p>
<p>The asymptotic distribution of the maximum likelihood estimator can be used to provide approximate large sample confidence intervals. For given <span class="math inline">\(\alpha\)</span> we can find <span class="math inline">\(z_{1-\frac{\alpha}{2}}\)</span> such that <span class="math display">\[
P\left(-z_{1-\frac{\alpha}{2}}\le {{\hat{\beta}_i-\beta_i}\over{[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) =1-\alpha.
\]</span> Therefore <span class="math display">\[
P\left(\hat{\beta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}\le\beta_i
\le\hat{\beta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm{\beta})^{-1}]_{ii}^{1\over 2}
\right) =1-\alpha.
\]</span> The endpoints of this interval cannot be evaluated because they also depend on the unknown parameter vector <span class="math inline">\(\bm{\beta}\)</span>. However, if we replace <span class="math inline">\({\cal I}(\bm{\beta})\)</span> by its MLE <span class="math inline">\({\cal I}(\hat{\bm{\beta}})\)</span> we obtain the approximate large sample 100<span class="math inline">\((1-\alpha)\)</span>% confidence interval <span class="math display">\[
[\hat{\beta}_i-s.e.(\hat{\beta}_i)z_{1-\frac{\alpha}{2}}\,,\,
\hat{\beta}_i+s.e.(\hat{\beta}_i)z_{1-\frac{\alpha}{2}}].
\]</span> For <span class="math inline">\(\alpha=0.10,0.05,0.01\)</span>, <span class="math inline">\(z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58\)</span>, respectively.</p>
</div>
<div id="sn:compglm" class="section level2">
<h2><span class="header-section-number">2.5</span> Comparing generalised linear models</h2>
<div id="sn:glmlrt" class="section level3">
<h3><span class="header-section-number">2.5.1</span> The generalised likelihood ratio test</h3>
<p>If we have a set of competing generalised linear models which might explain the dependence of the response on the explanatory variables, we will want to determine which of the models is most appropriate. Recall that we have three main requirements of a statistical model; plausibility, parsimony and goodness of fit, of which parsimony and goodness of fit are statistical issues.</p>
<p>As with linear models, we proceed by comparing models pairwise using a generalised likelihood ratio test. This kind of comparison is restricted to situations where one of the models, <span class="math inline">\(H_0\)</span>, is <em>nested</em> in the other, <span class="math inline">\(H_1\)</span>. Then the asymptotic distribution of the log likelihood ratio statistic under <span class="math inline">\(H_0\)</span> is a chi-squared distribution with known degrees of freedom.</p>
<p>For generalised linear models, ‘nested’ means that <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> are</p>
<ol style="list-style-type: decimal">
<li>based on the same exponential family distribution, and</li>
<li>have the same link function, but</li>
<li>the explanatory variables present in <span class="math inline">\(H_0\)</span> are a subset of those present in <span class="math inline">\(H_1\)</span>.</li>
</ol>
We will assume that model <span class="math inline">\(H_1\)</span> contains <span class="math inline">\(p\)</span> linear parameters and model <span class="math inline">\(H_0\)</span> a subset of <span class="math inline">\(q&lt;p\)</span> of these. Without loss of generality, we can think of <span class="math inline">\(H_1\)</span> as the model <span class="math display">\[
\eta_i=\sum_{j=1}^p x_{ij} \beta_j \qquad i = 1, \ldots, n
\]</span> and <span class="math inline">\(H_0\)</span> is the same model with

<p>Then model <span class="math inline">\(H_0\)</span> is a special case of model <span class="math inline">\(H_1\)</span>, where certain coefficients are set equal to zero, and therefore <span class="math inline">\(\Theta^{(0)}\)</span>, the set of values of the canonical parameter <span class="math inline">\(\bm{\theta}\)</span> allowed by <span class="math inline">\(H_0\)</span>, is a subset of <span class="math inline">\(\Theta^{(1)}\)</span>, the set of values allowed by <span class="math inline">\(H_1\)</span>.</p>
Now, the log likelihood ratio statistic for a test of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span> is
<span class="math display" id="eq:LRglm">\[\begin{align}
L_{01}&amp;\equiv 2\log \left({{\max_{\bm{\theta}\in \Theta^{(1)}} L(\bm{\theta})}\over
{\max_{\bm{\theta}\in \Theta^{(0)}}L(\bm{\theta})}}\right)\cr
&amp;=2\log L(\hat{\bm{\theta}}^{(1)})-2\log L(\hat{\bm{\theta}}^{(0)}),  
  \tag{2.12}
\end{align}\]</span>
<p>where <span class="math inline">\(\hat{\bm{\theta}}^{(1)}\)</span> and <span class="math inline">\(\hat{\bm{\theta}}^{(0)}\)</span> follow from <span class="math inline">\(b&#39;(\hat{\theta}_i)=\hat{\mu}_i\)</span>, <span class="math inline">\(g(\hat{\mu}_i)=\hat{\eta_i}\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span> where <span class="math inline">\(\hat{\bm{\eta}}\)</span> for each model is the linear predictor evaluated at the corresponding maximum likelihood estimate for <span class="math inline">\(\bm{\beta}\)</span>. Here, we assume that <span class="math inline">\(a(\phi_i),\;i = 1, \ldots, n\)</span> are known; unknown <span class="math inline">\(a(\phi)\)</span> is discussed in Section <a href="glm.html#sn:unknowndisp">2.7</a>.</p>
<p>Recall that we reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> when <span class="math inline">\(L_{01}\)</span> is ‘too large’ (the observed data are much more probable under <span class="math inline">\(H_1\)</span> than <span class="math inline">\(H_0\)</span>). To determine a threshold value <span class="math inline">\(k\)</span> for <span class="math inline">\(L_{01}\)</span>, beyond which we reject <span class="math inline">\(H_0\)</span>, we set the size of the test <span class="math inline">\(\alpha\)</span> and use the result of Section <a href="prelim.html#sn:lrt">1.4.7</a> that, because <span class="math inline">\(H_0\)</span> is nested in <span class="math inline">\(H_1\)</span>, <span class="math inline">\(L_{01}\)</span> has an asymptotic chi-squared distribution with <span class="math inline">\(p-q\)</span> degrees of freedom. For example, if <span class="math inline">\(\alpha=0.05\)</span>, we reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> when <span class="math inline">\(L_{01}\)</span> is greater than the 95% point of the <span class="math inline">\(\chi^2_{p-q}\)</span> distribution.</p>
<p>Note that setting up our model selection procedure in this way is consistent with our desire for parsimony. The simpler model is <span class="math inline">\(H_0\)</span>, and we do not reject <span class="math inline">\(H_0\)</span> in favour of the more complex model <span class="math inline">\(H_1\)</span> unless the data provide convincing evidence for <span class="math inline">\(H_1\)</span> over <span class="math inline">\(H_0\)</span>, that is unless <span class="math inline">\(H_1\)</span> fits the data significantly better.</p>
</div>
</div>
<div id="scaled-deviance-and-the-saturated-model" class="section level2">
<h2><span class="header-section-number">2.6</span> Scaled deviance and the saturated model</h2>
<p>Consider a model where <span class="math inline">\(\bm{\beta}\)</span> is <span class="math inline">\(n\)</span>-dimensional, and therefore <span class="math inline">\(\bm{\eta}=\bm{X}\bm{\beta}\)</span>. Assuming that <span class="math inline">\(\bm{X}\)</span> is invertible, then this model places no constraints on the linear predictor <span class="math inline">\(\bm{\eta}=(\eta_1,\ldots ,\eta_n)\)</span>. It can take any value in <span class="math inline">\(\mathbb{R}^n\)</span>. Correspondingly the means <span class="math inline">\(\bm{\mu}\)</span> and the canonical parameters <span class="math inline">\(\bm{\theta}\)</span> are unconstrained. The model is of dimension <span class="math inline">\(n\)</span> and can be parameterised equivalently using <span class="math inline">\(\bm{\beta}\)</span>, <span class="math inline">\(\bm{\eta}\)</span>, <span class="math inline">\(\bm{\mu}\)</span> or <span class="math inline">\(\bm{\theta}\)</span>. Such a model is called the <em>saturated</em> model.</p>
As the canonical parameters <span class="math inline">\(\bm{\theta}\)</span> are unconstrained, we can calculate their maximum likelihood estimates <span class="math inline">\(\hat{\bm{\theta}}\)</span> directly from their likelihood <a href="glm.html#eq:glmrandom">(2.2)</a> (without first having to calculate <span class="math inline">\(\hat{\bm{\beta}}\)</span>)
<span class="math display" id="eq:lsaturated">\[\begin{equation}
\ell(\bm{\theta})=\sum_{i=1}^n{{y_i\theta_i-b(\theta_i)}
\over{a(\phi_i)}}+\sum_{i=1}^nc(y_i,\phi_i).
\tag{2.13}
\end{equation}\]</span>
<p>We obtain <span class="math inline">\(\hat{\bm{\theta}}\)</span> by first differentiating with respect to <span class="math inline">\(\theta_1,\ldots ,\theta_n\)</span> to give <span class="math display">\[
{\partial\over{\partial\theta_k}}\ell(\bm{\theta})={{y_k-b&#39;(\theta_k)}
\over{a(\phi_k)}}\qquad k=1,\ldots ,n.
\]</span> Therefore <span class="math inline">\(b&#39;(\hat{\theta}_k)=y_k,\;k=1,\ldots ,n\)</span>, and it follows immediately that <span class="math inline">\(\hat{\mu}_k=y_k,\;k=1,\ldots ,n\)</span>. Hence the saturated model fits the data perfectly, as the <em>fitted values</em> <span class="math inline">\(\hat{\mu}_k\)</span> and observed values <span class="math inline">\(y_k\)</span> are the same for every observation <span class="math inline">\(k=1,\ldots ,n\)</span>.</p>
<p>The saturated model is rarely of any scientific interest in its own right. It is highly parameterised, having as many parameters as there are observations. This goes against our desire for parsimony in a model. However, every other model is necessarily nested in the saturated model, and a test comparing a model <span class="math inline">\(H_0\)</span> against the saturated model <span class="math inline">\(H_S\)</span> can be interpreted as a goodness of fit test. If the saturated model, which fits the observed data perfectly, does not provide a significantly better fit than model <span class="math inline">\(H_0\)</span>, we can conclude that <span class="math inline">\(H_0\)</span> is an acceptable fit to the data.</p>
<p>The log likelihood ratio statistic for a test of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_S\)</span> is, from <a href="glm.html#eq:LRglm">(2.12)</a> <span class="math display">\[
L_{0s}=2\log L(\hat{\bm{\theta}}^{(s)})-2\log L(\hat{\bm{\theta}}^{(0)}),
\]</span> where <span class="math inline">\(\hat{\bm{\theta}}^{(s)}\)</span> follows from <span class="math inline">\(b&#39;(\hat{\bm{\theta}})=\hat{\bm{\mu}}=\bm{y}\)</span> and <span class="math inline">\(\hat{\bm{\theta}}^{(0)}\)</span> is a function of the corresponding maximum likelihood estimate for <span class="math inline">\(\bm{\beta}=(\beta_1,\ldots ,\beta_q)^T\)</span>. Under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(L_{0s}\)</span> has an asymptotic chi-squared distribution with <span class="math inline">\(n-q\)</span> degrees of freedom. Therefore, if <span class="math inline">\(L_{0s}\)</span> is ‘too large’ (for example, larger than the 95% point of the <span class="math inline">\(\chi^2_{n-q}\)</span> distribution) then we reject <span class="math inline">\(H_0\)</span> as a plausible model for the data, as it does not fit the data adequately.</p>
<p>The <em>degrees of freedom</em> of model <span class="math inline">\(H_0\)</span> is defined to be the degrees of freedom for this test, <span class="math inline">\(n-q\)</span>, the number of observations minus the number of linear parameters of <span class="math inline">\(H_0\)</span>. We call <span class="math inline">\(L_{0s}\)</span> the <em>scaled deviance</em> (<code>R</code> calls it the <em>residual deviance</em>) of model <span class="math inline">\(H_0\)</span>.</p>
From <a href="glm.html#eq:LRglm">(2.12)</a> and <a href="glm.html#eq:lsaturated">(2.13)</a> we can write the deviance of model <span class="math inline">\(H_0\)</span> as
<span class="math display" id="eq:dsaturated">\[\begin{equation}
L_{0s}=2\sum_{i=1}^n{{y_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i]
-[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]}
\over{a(\phi_i)}},
  \tag{2.14}
\end{equation}\]</span>
<p>which can be calculated using the observed data, provided that <span class="math inline">\(a(\phi_i),\; i = 1, \ldots, n\)</span> is known.</p>

<p><strong>Notes</strong></p>
<ol style="list-style-type: decimal">
<li>The log likelihood ratio statistic <a href="glm.html#eq:LRglm">(2.12)</a> for testing <span class="math inline">\(H_0\)</span> against a non-saturated alternative <span class="math inline">\(H_1\)</span> can be written as
<span class="math display" id="eq:LRsaturated">\[\begin{align}
L_{01}&amp;=2\log L(\hat{\bm{\theta}}^{(1)})-2\log L(\hat{\bm{\theta}}^{(0)})\cr
&amp;=[2\log L(\hat{\bm{\theta}}^{(s)})-2\log L(\hat{\bm{\theta}}^{(0)})]
-[2\log L(\hat{\bm{\theta}}^{(s)})-2\log L(\hat{\bm{\theta}}^{(1)})]\cr
&amp;=L_{0s}-L_{1s}. 
  \tag{2.15}
\end{align}\]</span>
Therefore the log likelihood ratio statistic for comparing two nested models is the difference of their deviances. Furthermore, as <span class="math inline">\(p-q=(n-q)-(n-p)\)</span>, the degrees of freedom for the test is the difference in degrees of freedom of the two models.</li>
<li>The asymptotic theory used to derive the distribution of the log likelihood ratio statistic under <span class="math inline">\(H_0\)</span> does not really apply to the goodness of fit test (comparison with the saturated model). However, for binomial or Poisson data, we can proceed as long as the relevant binomial or Poisson distributions are likely to be reasonably approximated by normal distributions (<em>i.e.</em> for binomials with large denominators or Poissons with large means). However, for Bernoulli data, we cannot use the scaled deviance as a goodness of fit statistic in this way.</li>
<li>An alternative goodness of fit statistic for a model <span class="math inline">\(H_0\)</span> is Pearson’s <span class="math inline">\(X^2\)</span> given by
<span class="math display" id="eq:pearsonGoF">\[\begin{equation}
X^2=\sum_{i=1}^n {{(y_i-\hat{\mu}_i^{(0)})^2}\over{\hat{\text{Var}}(Y_i)}}.
  \tag{2.16}
\end{equation}\]</span>
<span class="math inline">\(X^2\)</span> is small when the squared differences between observed and fitted values (scaled by variance) is small. Hence, large values of <span class="math inline">\(X^2\)</span> correspond to poor fitting models. In fact, <span class="math inline">\(X^2\)</span> and <span class="math inline">\(L_{0s}\)</span> are asymptotically equivalent and under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(X^2\)</span>, like <span class="math inline">\(L_{0s}\)</span>, has an asymptotic chi-squared distribution with <span class="math inline">\(n-q\)</span> degrees of freedom. However, the asymptotics associated with <span class="math inline">\(X^2\)</span> are often more reliable for small samples, so if there is a discrepancy between <span class="math inline">\(X^2\)</span> and <span class="math inline">\(L_{0s}\)</span>, it is usually safer to base a test of goodness of fit on <span class="math inline">\(X^2\)</span>.</li>
<li>Although the deviance for a model is expressed in <a href="glm.html#eq:dsaturated">(2.14)</a> in terms of the maximum likelihood estimates of the canonical parameters, it is more usual to express it in terms of the maximum likelihood estimates <span class="math inline">\(\hat{\mu}_i,\; i = 1, \ldots, n\)</span> of the mean parameters. For the saturated model, these are just the observed values <span class="math inline">\(y_i,\;i = 1, \ldots, n\)</span>, and for the model of interest, <span class="math inline">\(H_0\)</span>, we call them the <em>fitted values</em>. Hence, for a particular generalised linear model, the scaled deviance function describes how discrepancies between the observed and fitted values are penalised.</li>
</ol>

<strong>Example (Poisson)</strong>. Suppose <span class="math inline">\(Y_i\sim \text{Poisson}(\lambda_i),\;i = 1, \ldots, n\)</span>. Recall from Section <a href="glm.html#sn:ef">2.1</a> that <span class="math inline">\(\theta=\log\lambda\)</span>, <span class="math inline">\(b(\theta)=\exp\theta\)</span>, <span class="math inline">\(\mu=b&#39;(\theta)=\exp\theta\)</span> and <span class="math inline">\(\text{Var}(Y)=a(\phi)V(\mu)=1\cdot\mu\)</span>. Therefore, by <a href="glm.html#eq:dsaturated">(2.14)</a> and <a href="glm.html#eq:pearsonGoF">(2.16)</a>
<span class="math display">\[\begin{align*}
L_{0s}&amp;=2\sum_{i=1}^n y_i[\log\hat{\mu}^{(s)}_i-\log\hat{\mu}^{(0)}_i]
-[\hat{\mu}^{(s)}_i-\hat{\mu}^{(0)}_i]\cr
&amp;=2\sum_{i=1}^n y_i\log \left({{y_i}\over{\hat{\mu}^{(0)}_i}}\right)
-y_i+\hat{\mu}^{(0)}_i
\end{align*}\]</span>
<p>and <span class="math display">\[
X^2=\sum_{i=1}^n {{(y_i-\hat{\mu}_i^{(0)})^2}\over{\hat{\mu}_i^{(0)}}}.
\]</span></p>
<strong>Example (Binomial)</strong>. Suppose <span class="math inline">\(n_iY_i\sim\)</span> Binomial<span class="math inline">\((n_i,p_i),\;i = 1, \ldots, n\)</span>. Recall from Section <a href="glm.html#sn:ef">2.1</a> that <span class="math inline">\(\theta=\log{p\over{1-p}}\)</span>, <span class="math inline">\(b(\theta)=\log(1+\exp\theta)\)</span>, <span class="math inline">\(\mu=b&#39;(\theta)={{\exp\theta}\over{1+\exp\theta}}\)</span> and <span class="math inline">\(\text{Var}(Y)=a(\phi)V(\mu)={1\over n}\cdot\mu(1-\mu)\)</span>. Therefore, by <a href="glm.html#eq:dsaturated">(2.14)</a> and <a href="glm.html#eq:pearsonGoF">(2.16)</a>
<span class="math display">\[\begin{align*}
L_{0s}&amp;=2\sum_{i=1}^n n_iy_i\left[\log{\hat{\mu}^{(s)}_i\over{1-\hat{\mu}^{(s)}_i}}
-\log{\hat{\mu}^{(0)}_i\over{1-\hat{\mu}^{(0)}_i}}\right] 
+ 2\sum_{i=1}^n n_i \left[\log(1-\hat{\mu}^{(s)}_i)-\log(1-\hat{\mu}^{(0)}_i) \right]\cr
&amp;=2\sum_{i=1}^n \left[ n_iy_i\log \left({{y_i}\over{\hat{\mu}^{(0)}_i}}\right)
+n_i(1-y_i) \log \left({{1-y_i}\over{1-\hat{\mu}^{(0)}_i}}\right) \right]
\end{align*}\]</span>
<p>and <span class="math display">\[
X^2=\sum_{i=1}^n {{n_i(y_i-\hat{\mu}_i^{(0)})^2}\over{\hat{\mu}_i^{(0)}
(1-\hat{\mu}^{(0)}_i)}}.
\]</span> Bernoulli data are binomial with <span class="math inline">\(n_i=1,\;i = 1, \ldots, n\)</span>.</p>
</div>
<div id="sn:unknowndisp" class="section level2">
<h2><span class="header-section-number">2.7</span> Models with unknown <span class="math inline">\(a(\phi)\)</span></h2>
<p>The theory of Section <a href="glm.html#sn:compglm">2.5</a> has assumed that <span class="math inline">\(a(\phi)\)</span> is known. This is the case for both the Poisson distribution (<span class="math inline">\(a(\phi)=1\)</span>) and the binomial distribution (<span class="math inline">\(a(\phi)=1/n\)</span>). Neither the scaled deviance <a href="glm.html#eq:dsaturated">(2.14)</a> nor Pearson <span class="math inline">\(X^2\)</span> statistic <a href="glm.html#eq:pearsonGoF">(2.16)</a> can be evaluated unless <span class="math inline">\(a(\phi)\)</span> is known. Therefore, when <span class="math inline">\(a(\phi)\)</span> is not known, we cannot use the scaled deviance as a measure of goodness of fit, or to compare models using <a href="glm.html#eq:LRsaturated">(2.15)</a>. For such models, there is no equivalent goodness of fit test, but we can develop a test for comparing nested models.</p>
Here we assume that <span class="math inline">\(a(\phi_i)=\sigma^2/m_i,\;i = 1, \ldots, n\)</span> where <span class="math inline">\(\sigma^2\)</span> is a common unknown scale parameter and <span class="math inline">\(m_1,\ldots ,m_n\)</span> are known weights. (A linear model takes this form, as <span class="math inline">\(\text{Var}(Y_i)=\sigma^2,\;i = 1, \ldots, n\)</span>, so <span class="math inline">\(m_i=1,\;i = 1, \ldots, n\)</span>.) Under this assumption
<span class="math display" id="eq:devunknowndisp">\[\begin{equation}
L_{0s}={2\over\sigma^2}\sum_{i=1}^nm_iy_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i]
-m_i[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]
={1\over\sigma^2}D_{0s},
\tag{2.17}
\end{equation}\]</span>
<p>where <span class="math inline">\(D_{0s}\)</span> is defined to be twice the sum above, which can be calculated using the observed data. We call <span class="math inline">\(D_{0s}\)</span> the <em>deviance</em> of the model.</p>
In order to test nested models <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> as set up in Section <a href="glm.html#sn:glmlrt">2.5.1</a>, we calculate the test statistic
<span class="math display" id="eq:LRunknowndisp">\[\begin{align}
&amp;F={{L_{01}/(p-q)}\over{L_{1s}/(n-p)}}={{(L_{0s}-L_{1s})/(p-q)}\over{L_{1s}/(n-p)}}
\hbox{\hskip 1.2in}\cr
&amp;\;={{\left({1\over\sigma^2}D_{0s}-{1\over\sigma^2}D_{1s}\right)/(p-q)}
\over{{1\over\sigma^2}D_{1s}/(n-p)}}
={{(D_{0s}-D_{1s})/(p-q)}\over{D_{1s}/(n-p)}}.
\tag{2.18}
\end{align}\]</span>
<p>This statistic does not depend on the unknown scale parameter <span class="math inline">\(\sigma^2\)</span>, so can be calculated using the observed data. Asymptotically, if <span class="math inline">\(H_0\)</span> is true, we know that <span class="math inline">\(L_{01} \sim \chi^2_{p-q}\)</span> and <span class="math inline">\(L_{1s} \sim \chi^2_{n-p}\)</span>. Furthermore, <span class="math inline">\(L_{01}\)</span> and <span class="math inline">\(L_{1s}\)</span> are independent (not proved here) so <span class="math inline">\(F\)</span> has an asymptotic <span class="math inline">\(F_{p-q, n-p}\)</span> distribution. Hence, we compare nested generalised linear models by calculating <span class="math inline">\(F\)</span> and rejecting <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> if <span class="math inline">\(F\)</span> is too large (for example, greater than the 95% point of the relevant F distribution).</p>
<p>The dependence of the maximum likelihood equations <span class="math inline">\(\bm{u}(\hat{\bm{\beta}})={\bf 0}\)</span> on <span class="math inline">\(\sigma^2\)</span> (where <span class="math inline">\(\bm{u}\)</span> is given by <a href="glm.html#eq:scoreglm">(2.7)</a>) can be eliminated by multiplying through by <span class="math inline">\(\sigma^2\)</span>. However, inference based on the maximum likelihood estimates, as described in Section <a href="glm.html#sn:glminfer">2.4</a>, does require knowledge of <span class="math inline">\(\sigma^2\)</span>. This is because asymptotically <span class="math inline">\(\text{Var}(\hat{\bm{\beta}})\)</span> is the inverse of the Fisher information matrix <span class="math inline">\({\cal I}(\bm{\beta})=\bm{X}^T\bm{W}\bm{X}\)</span>, and this depends on <span class="math inline">\(w_i={1\over{\text{Var}(Y_i)g&#39;(\mu_i)^2}},\)</span> where <span class="math inline">\(\text{Var}(Y_i)=a(\phi_i)b&#39;&#39;(\theta_i)=\sigma^2 b&#39;&#39;(\theta_i)/m_i\)</span> here.</p>
<p>Therefore, to calculate standard errors and confidence intervals, we need to supply an estimate <span class="math inline">\(\hat \sigma^2\)</span> of <span class="math inline">\(\sigma^2\)</span>. Generally, we do not use the maximum likelihood estimate. Instead, we notice that, from <a href="glm.html#eq:devunknowndisp">(2.17)</a>, <span class="math inline">\(L_{0s}=D_{0s}/\sigma^2\)</span>, and we know that asymptotically, if model <span class="math inline">\(H_0\)</span> is an adequate fit, <span class="math inline">\(L_{0s}\)</span> has a <span class="math inline">\(\chi^2_{n-q}\)</span> distribution. Hence <span class="math display">\[
E(L_{0s})=E\left({1\over{\sigma^2}}D_{0s}\right)=n-q\quad\Rightarrow\quad
E\left({1\over{n-q}}D_{0s}\right)=\sigma^2.
\]</span> Therefore the deviance of a model divided by its degrees of freedom is an asymptotically unbiased estimator of the scale parameter <span class="math inline">\(\sigma^2\)</span>. Hence <span class="math inline">\(\hat\sigma^2=D_{0s}/(n-q)\)</span>.</p>
An alternative estimator of <span class="math inline">\(\sigma^2\)</span> is based on the Pearson <span class="math inline">\(X^2\)</span> statistic. As <span class="math inline">\(\text{Var}(Y)=a(\phi)V(\mu)=\sigma^2 V(\mu)/m\)</span> here, then from <a href="glm.html#eq:pearsonGoF">(2.16)</a>
<span class="math display" id="eq:pearsonGoFunknown">\[\begin{equation}
X^2={1\over\sigma^2}
\sum_{i=1}^n {{m_i(y_i-\hat{\mu}_i^{(0)})^2}\over{{V}(\hat{\mu}_i^{(0)})}}.
  \tag{2.19}
\end{equation}\]</span>
<p>Again, if <span class="math inline">\(H_0\)</span> is an adequate fit, <span class="math inline">\(X^2\)</span> has an chi-squared distribution with <span class="math inline">\(n-q\)</span> degrees of freedom, so <span class="math display">\[
\hat \sigma^2={1\over{n-q}}
\sum_{i=1}^n {{m_i(y_i-\hat{\mu}_i^{(0)})^2}\over{{V}(\hat{\mu}_i^{(0)})}}
\]</span> is an alternative unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. This estimator tends to be more reliable in small samples.</p>
<strong>Example (Normal)</strong>. Suppose <span class="math inline">\(Y_i\sim N(\mu_i,\sigma^2),\;i = 1, \ldots, n\)</span>. Recall from Section <a href="glm.html#sn:ef">2.1</a> that <span class="math inline">\(\theta=\mu\)</span>, <span class="math inline">\(b(\theta)=\theta^2/2\)</span>, <span class="math inline">\(\mu=b&#39;(\theta)=\theta\)</span> and <span class="math inline">\(\text{Var}(Y)=a(\phi)V(\mu)={\sigma^2}\cdot 1\)</span>, so <span class="math inline">\(m_i=1,\;i = 1, \ldots, n\)</span>. Therefore, by <a href="glm.html#eq:devunknowndisp">(2.17)</a>,
<span class="math display" id="eq:devnormal">\[\begin{equation}
D_{0s}=2\sum_{i=1}^n y_i[\hat{\mu}^{(s)}_i-\hat{\mu}^{(0)}_i]
-[\frac{1}{2}{{\hat{\mu}}^{(s)^2}_i}-\frac{1}{2}{{\hat{\mu}}^{(0)^2}_i}]
=\sum_{i=1}^n [y_i-\hat{\mu}^{(0)}_i]^2,
\tag{2.20}
\end{equation}\]</span>
<p>which is just the residual sum of squares for model <span class="math inline">\(H_0\)</span>. Therefore, we estimate <span class="math inline">\(\sigma^2\)</span> for a normal GLM by its residual sum of squares for the model divided by its degrees of freedom. From <a href="glm.html#eq:pearsonGoFunknown">(2.19)</a>, the estimate for <span class="math inline">\(\sigma^2\)</span> based on <span class="math inline">\(X^2\)</span> is identical.</p>
</div>
<div id="residuals" class="section level2">
<h2><span class="header-section-number">2.8</span> Residuals</h2>
<p>Recall that for linear models, we define the residuals to be the differences between the observed and fitted values <span class="math inline">\(y_i-\hat{\mu}^{(0)}_i,\;i = 1, \ldots, n\)</span>. From <a href="glm.html#eq:devnormal">(2.20)</a> we notice that both the scaled deviance and Pearson <span class="math inline">\(X^2\)</span> statistic for a normal GLM are the sum of the squared residuals divided by <span class="math inline">\(\sigma^2\)</span>. We can generalise this to define residuals for other generalised linear models in a natural way.</p>
<p>For any GLM we define the <em>Pearson residuals</em> to be <span class="math display">\[
r^P_i={{y_i-\hat{\mu}_i^{(0)}}\over{\hat{\text{Var}}(Y_i)^{1\over 2}}}\qquad i = 1, \ldots, n.
\]</span> Then, from <a href="glm.html#eq:pearsonGoF">(2.16)</a>, <span class="math inline">\(X^2\)</span> is the sum of the squared Pearson residuals.</p>
<p>For any GLM we define the <em>deviance residuals</em> to be <span class="math display">\[r^D_i=\text{sign}(y_i-\hat{\mu}_i^{(0)})
\left[ 2 {{y_i[\hat{\theta}^{(s)}_i-\hat{\theta}^{(0)}_i]
-[b(\hat{\theta}^{(s)}_i)-b(\hat{\theta}^{(0)}_i)]}
\over{a(\phi_i)}}\right]^{1\over 2}, \quad i = 1, \ldots, n,\]</span> where <span class="math inline">\(\text{sign}(x)=1\)</span> if <span class="math inline">\(x&gt;0\)</span> and <span class="math inline">\(-1\)</span> if <span class="math inline">\(x&lt;0\)</span>. Then, from <a href="glm.html#eq:dsaturated">(2.14)</a>, the scaled deviance, <span class="math inline">\(L_{0s}\)</span>, is the sum of the squared deviance residuals.</p>
<p>When <span class="math inline">\(a(\phi)=\sigma^2/m\)</span> and <span class="math inline">\(\sigma^2\)</span> is unknown, as in Section <a href="glm.html#sn:unknowndisp">2.7</a>, the residuals are based on <a href="glm.html#eq:devunknowndisp">(2.17)</a> and <a href="glm.html#eq:pearsonGoFunknown">(2.19)</a>, and the expressions above need to be multiplied through by <span class="math inline">\(\sigma^2\)</span> to eliminate dependence on the unknown scale parameter. Therefore, for a normal GLM the Pearson and deviance residuals are both equal to the usual residuals, <span class="math inline">\(y_i-\hat{\mu}^{(0)}_i,\;i = 1, \ldots, n\)</span>.</p>
<p>Residual plots are most commonly of use in normal linear models, where they provide an essential check of the model assumptions. This kind of check is less important for a model without an unknown scale parameter as the scaled deviance provides a useful overall assessment of fit which takes into account most aspects of the model.</p>
<p>However, when data have been collected in serial order, a plot of the deviance or Pearson residuals against the order may again be used as a check for potential serial correlation.</p>
<p>Otherwise, residual plots are most useful when a model fails to fit (scaled deviance is too high). Then, examining the residuals may give an indication of the reason(s) for lack of fit. For example, there may be a small number of outlying observations.</p>
<p>A plot of deviance or Pearson residuals against the linear predictor should produce something that looks like a random scatter. If not, then this may be due to incorrect link function, wrong scale for an explanatory variable, or perhaps a missing polynomial term in an explanatory variable.</p>
</div>
<div id="example-binary-regression" class="section level2">
<h2><span class="header-section-number">2.9</span> Example: Binary Regression</h2>
<p>In binary regression the data either follow the binomial or the Bernoulli distribution. The objective is to model the success probability <span class="math inline">\(p\)</span> as a function of the explanatory variables.</p>
<p>When the canonical (logit) link is used, we have <span class="math display">\[
\theta = \log \frac{p(\bm{x})}{1-p(\bm{x})} = \bm{x}^T\bm{\beta} = \eta.
\]</span> This implies <span class="math display">\[
p(\bm{x}) = \frac{ \exp(\eta) }{1+ \exp(\eta)} = \frac{1}{1+ \exp(-\eta)}.
\]</span> This is the cumulative distribution function (cdf) of the logistic distribution taking values in the real line <span class="math inline">\((-\infty &lt; \eta &lt; \infty)\)</span>. It can be easily verified that <span class="math inline">\(F(\eta) = \frac{1}{1+ \exp(-\eta)}\)</span> is a cdf of a random variable since it is non-negative and increases monotonically to 1 from zero.</p>
<p>The cumulative distribution functions of other distributions are also commonly used to generate link functions for binary regression. For example, if we let <span class="math display">\[
p(\bm{x}) = \Phi(\bm{x}^T \bm{\beta}) = \Phi(\eta),
\]</span> where <span class="math inline">\(\Phi(\cdot)\)</span> is the cdf of the standard normal distribution, then we get the link function <span class="math display">\[
g(\mu) = g(p) = \Phi^{-1}(\mu) = \eta,
\]</span> which is called the probit link.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prelim.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-categorical.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/heogden/math3091/edit/master/02-glm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH3091.pdf", "MATH3091.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
