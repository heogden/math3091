[
["index.html", "MATH3091: Statistical Modelling II Preface", " MATH3091: Statistical Modelling II Dr Helen Ogden Preface The pre-requisite module MATH2010: Statistical Modelling I covered in detail the theory of linear regression models, where explanatory variables are used to explain the variation in a response variable, which is assumed to be normally distributed. However, in many practical situations the data are not appropriate for such analysis. For example, the response variable may be binary, and interest may be focused on assessing the dependence of the probability of ‘success’ on potential explanatory variables. Alternatively, the response variable may be a count of events, and we may wish to infer how the rate at which events occur depends on explanatory variables. Such techniques are important in many disciplines such as finance, biology, social sciences and medicine. The aim of this module is to cover the theory and application of what are known as generalised linear models (GLMs). This is an extremely broad class of statistical models, which incorporates the linear regression models studied in MATH2010, but also allows binary and count response data to be modelled coherently. "],
["intro.html", "Chapter 1 Introduction 1.1 Elements of statistical modelling 1.2 Regression models 1.3 Example data to be analysed", " Chapter 1 Introduction 1.1 Elements of statistical modelling Probability and statistics can be characterised as the study of variability. In particular, statistical inference is the science of analysing statistical data, viewed as the outcome of some random process, in order to draw conclusions about that random process. Statistical models help us to understand the random process by which observed data have been generated. This may be of interest in itself, but also allows us to make predictions and perhaps most importantly decisions contingent on our inferences concerning the process. It is also important, as part of the modelling process, to acknowledge that our conclusions are only based on a (potentially small) sample of possible observations of the process and are therefore subject to error. The science of statistical inference therefore involves assessment of the uncertainties associated with the conclusions we draw. Probability theory is the mathematics associated with randomness and uncertainty. We usually try to describe random processes using probability models. Then, statistical inference may involve estimating any unspecified features of a model, comparing competing models, and assessing the appropriateness of a model; all in the light of observed data. In order to identify ‘good’ statistical models, we require some principles on which to base our modelling procedures. In general, we have three requirements of a statistical model Plausibility Parsimony Goodness of fit The first of these is not a statistical consideration, and a subject-matter expert usually needs to be consulted about this. For some objectives, like prediction, it might be considered unimportant. Parsimony and goodness of fit are statistical issues. Indeed, there is usually a trade-off between the two and our statistical modelling strategies will take account of this. 1.2 Regression models Many statistical models, and all the ones we shall deal with in MATH3091, can be formulated as regression models. In practical applications, we often distinguish between a response variable and a group of explanatory variables. The aim is to determine the pattern of dependence of the response variable on the explanatory variables. A regression model has the general form response = function(structure and randomness) The structural part of the model describes how the response depends on the explanatory variables and the random part defines the probability distribution of the response. Together, they produce the response and the statistical modeller’s task is to ‘separate’ these out. 1.3 Example data to be analysed 1.3.1 nitric: Nitric acid This data set relates to 21 successive days of operation of a plant oxidising ammonia to nitric acid. The response yield is ten times the percentage of ingoing ammonia that is lost as unabsorbed nitric acid (an indirect measure of the yield). The aim here is to study how the yield depends on flow of air to the plant (flow), temperature of the cooling water entering the absorption tower (temp) and concentration of nitric acid in the absorbing liquid (conc). These data will be analysed in worksheet 2 using multiple linear regression models. 1.3.2 birth: Weight of newborn babies This data set contains weights of 24 newborn babies. There are two explanatory variables, sex (Sex) and gestational age in weeks (Age) together with the response variable, birth weight in grams (Weight). The aim here is to study how birth weight depends on sex and gestational age. This data set will be analysed in worksheet 3 by using multiple linear regression models including both categorical and continuous explanatory variables. 1.3.3 survival: Time to death This data set, analysed in worksheet 4, contains survival times in 10 hour units (time) of 48 rats each allocated to one of 12 combinations of 3 poisons (poison) and 4 treatments (treatment). The aim here is to study how survival time depends on the poison and the treatment, and to determine whether there is an interaction between these two categorical variables. 1.3.4 beetle: Mortality from carbon disulphide This data set represents the number of beetles exposed (exposed) and number killed (killed) in eight groups exposed to different doses (dose) of a particular insecticide. Interest is focussed on how mortality is related to dose. It seems sensible to model the number of beetles killed in each group as the binomial random variable with probability of death depending on dose. This will be discussed in worksheet 5. 1.3.5 shuttle: Challenger disaster This data set concerns the 23 space shuttle flights before the Challenger disaster. The disaster is thought to have been caused by the failure of a number of O-rings, of which there were six in total. The data consist of four variables, the number of damaged O-rings for each pre-Challenger flight (n_damaged), together with the launch temperature in degrees Fahrenheit (temp), the pressure at which the pre-launch test of O-ring leakage was carried out (pressure) and the name of the orbiter (orbiter). The Challenger launch temperature on 20th January 1986 was 31F. The aim is to predict the probability of O-ring damage at the Challenger launch. This will be discussed in worksheet 6. 1.3.6 heart: Treatment for heart attack This data set represents the results of a clinical trial to assess the effectiveness of a thrombolytic (clot-busting) treatment for patients who have suffered an acute myocardial infarction (heart attack). There are four categorical explanatory variables, representing the site of infarction: anterior, inferior or other the time between infarction and treatment: \\(\\le 12\\) or \\(&gt;12\\) hours whether the patient was already taking Beta-blocker medication prior to the infarction, blocker: yes or no the treatment the patient was given: active or placebo. For each combination of these categorical variables, the dataset gives the total number of patients (n_patients), and the number who survived for for 35 days (n_survived). The aim is to find out how these categorical variables affect a patient’s chance of survival. These data will be analysed in worksheet 7. 1.3.7 accident: Road traffic accidents This example concerns the number of road accidents (number) and the volume of traffic (volume), on each of two roads in Cambridge (road), at various times of day (time, taking values morning, midday or afternoon). We should be able to answer questions like: Is Mill Road more dangerous than Trumpington Road? How does time of day affect the rate of road accident? These issues will be considered in worksheet 8. 1.3.8 lymphoma: Lymphoma patients The lymphoma data set represents 30 lymphoma patients classified by sex (Sex), cell type of lymphoma (Cell) and response to treatment (Remis). It is an example of data which may be represented as a three-way (\\(2\\times 2\\times 2\\)) contingency table. The aim here is to study the complex dependence structures between the three classifying factors. This is taken up in worksheet 9. "],
["inference.html", "Chapter 2 Parametric statistical inference 2.1 Introduction 2.2 The likelihood function 2.3 Maximum likelihood estimation 2.4 Score 2.5 Information 2.6 Asymptotic distribution of the MLE 2.7 Quantifying uncertainty in parameter estimates", " Chapter 2 Parametric statistical inference 2.1 Introduction Probability distributions like the binomial, Poisson and normal, enable us to calculate probabilities, and other quantities of interest (e.g. expectations) for a probability model of a random process. Therefore, given the model, we can make statements about possible outcomes of the process. Statistical inference is concerned with the inverse problem. Given outcomes of a random process (observed data), what conclusions (inferences) can we draw about the process itself? We assume that the \\(n\\) observations of the response \\(\\bm y=(y_1,\\ldots ,y_n)^T\\) are observations of random variables \\(\\bm Y=(Y_1,\\ldots ,Y_n)^T\\), which have joint p.d.f. \\(f_{\\bm Y}\\) (joint p.f. for discrete variables). We use the observed data \\(\\bm y\\) to make inferences about \\(f_{\\bm Y}\\). We usually make certain assumptions about \\(f_{\\bm Y}\\). In particular, we often assume that \\(y_1, \\ldots, y_n\\) are observations of independent random variables. Hence \\[ f_{\\bm Y}(\\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\\cdots f_{Y_n}(y_n) =\\prod_{i=1}^n f_{Y_i}(y_i). \\] In parametric statistical inference, we specify a joint distribution \\(f_{\\bm Y}\\), for \\(\\bm Y\\), which is known, except for the values of parameters \\(\\theta_1,\\theta_2,\\ldots ,\\theta_p\\) (sometimes denoted by \\(\\bm \\theta\\)). Then we use the observed data \\(\\bm y\\) to make inferences about \\(\\theta_1,\\theta_2,\\ldots ,\\theta_p\\). In this case, we usually write \\(f_{\\bm Y}\\) as \\(f_{\\bm Y}(\\bm y;\\bm \\theta)\\), to make explicit the dependence on the unknown \\(\\bm \\theta\\). 2.2 The likelihood function We often think of the joint density \\(f_{\\bm Y}(\\bm y;\\bm \\theta)\\) as a function of \\(\\bm{y}\\) for fixed \\(\\bm \\theta\\), which describes the relative probabilities of different possible values of \\(\\bm y\\), given a particular set of parameters \\(\\bm \\theta\\). However, in statistical inference, we have observed \\(y_1, \\ldots, y_n\\) (values of \\(Y_1, \\ldots, Y_n\\)). Knowledge of the probability of alternative possible realisations of \\(\\bm Y\\) is largely irrelevant. What we want to know about is \\(\\bm \\theta\\). Our only link between the observed data \\(y_1, \\ldots, y_n\\) and \\(\\bm \\theta\\) is through the function \\(f_{\\bm Y}(\\bm y;\\bm \\theta)\\). Therefore, it seems sensible that parametric statistical inference should be based on this function. We can think of \\(f_{\\bm Y}(\\bm y;\\bm \\theta)\\) as a function of \\(\\bm \\theta\\) for fixed \\(\\bm{y}\\), which describes the relative likelihoods of different possible (sets of) \\(\\bm \\theta,\\) given observed data \\(y_1, \\ldots, y_n\\). We write \\[L(\\bm \\theta; \\bm y) = f_{\\bm Y}(\\bm y;\\bm \\theta)\\] for this likelihood, which is a function of the unknown parameter \\(\\bm \\theta\\). For convenience, we often drop \\(\\bm y\\) from the notation, and write \\(L(\\bm \\theta)\\). The likelihood function is of central importance in parametric statistical inference. It provides a means for comparing different possible values of \\(\\bm \\theta\\), based on the probabilities (or probability densities) that they assign to the observed data \\(y_1, \\ldots, y_n\\). Notes Frequently it is more convenient to consider the log-likelihood function \\(\\ell(\\bm \\theta) = \\log L(\\bm \\theta)\\). Nothing in the definition of the likelihood requires \\(y_1, \\ldots, y_n\\) to be observations of independent random variables, although we shall frequently make this assumption. Any factors which depend on \\(y_1, \\ldots, y_n\\) alone (and not on \\(\\bm \\theta\\)) can be ignored when writing down the likelihood. Such factors give no information about the relative likelihoods of different possible values of \\(\\bm \\theta\\). Example 2.1 (Bernoulli) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), independent identically distributed (i.i.d.) Bernoulli\\((p)\\) random variables. Here \\(\\theta=(p)\\) and the likelihood is \\[ L(p)=\\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\\sum_{i=1}^n y_i}(1-p)^{n-\\sum_{i=1}^n y_i}. \\] The log-likelihood is \\[ \\ell(p) = \\log L(p) =n\\bar y\\log p+n(1-\\bar y)\\log(1-p). \\] Example 2.2 (Normal) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. \\(N(\\mu,\\sigma^2)\\) random variables. Here \\(\\bm \\theta=(\\mu,\\sigma^2)\\) and the likelihood is \\[\\begin{align*} L(\\mu,\\sigma^2) &amp;= \\prod_{i=1}^n {1\\over{\\sqrt{2\\pi\\sigma^2}}} \\exp\\left(-{1\\over{2\\sigma^2}}(y_i-\\mu)^2\\right) \\\\ &amp;=(2\\pi\\sigma^2)^{-{n\\over 2}} \\exp\\left(-{1\\over{2\\sigma^2}}\\sum(y_i-\\mu)^2\\right) \\\\ &amp;\\propto (\\sigma^2)^{-{n\\over 2}} \\exp\\left(-{1\\over{2\\sigma^2}}\\sum(y_i-\\mu)^2\\right). \\end{align*}\\] The log-likelihood is \\[\\ell(\\mu, \\sigma^2) = \\log L(\\mu,\\sigma^2)=-{n\\over 2}\\log(2\\pi)-{n\\over 2}\\log(\\sigma^2) -{1\\over{2\\sigma^2}}\\sum(y_i-\\mu)^2.\\] 2.3 Maximum likelihood estimation One of the primary tasks of parametric statistical inference is estimation of the unknown parameters \\(\\theta_1, \\ldots, \\theta_p\\). Consider the value of \\(\\bm \\theta\\) which maximises the likelihood function. This is the ‘most likely’ value of \\(\\bm \\theta\\), the one which makes the observed data ‘most probable’. When we are searching for an estimate of \\(\\bm \\theta\\), this would seem to be a good candidate. We call the value of \\(\\bm \\theta\\) which maximises the likelihood \\(L(\\theta)\\) the maximum likelihood estimate (MLE) of \\(\\bm \\theta\\), denoted by \\(\\hat{\\bm \\theta}\\). \\(\\hat{\\bm \\theta}\\) depends on \\(\\bm y\\), as different observed data samples lead to different likelihood functions. The corresponding function of \\(\\bm Y\\) is called the maximum likelihood estimator and is also denoted by \\(\\hat{\\bm \\theta}\\). Note that as \\(\\bm \\theta=(\\theta_1, \\ldots, \\theta_p)\\), the MLE for any component of \\(\\bm \\theta\\) is given by the corresponding component of \\(\\hat{\\bm \\theta}=(\\hat{\\theta}_1,\\ldots ,\\hat{\\theta}_p)^T\\). Similarly, the MLE for any function of parameters \\(g(\\bm \\theta)\\) is given by \\(g(\\hat{\\bm \\theta})\\). As \\(\\log\\) is a strictly increasing function, the value of \\(\\bm \\theta\\) which maximises \\(L(\\bm \\theta)\\) also maximises \\(\\ell(\\bm \\theta) = \\log L (\\bm \\theta)\\). It is almost always easier to maximise \\(\\ell(\\bm \\theta)\\). This is achieved in the usual way; finding a stationary point by differentiating \\(\\ell(\\bm \\theta)\\) with respect to \\(\\theta_1, \\ldots, \\theta_p\\), and solving the resulting \\(p\\) simultaneous equations. It should also be checked that the stationary point is a maximum. Example 2.3 (Bernoulli) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. Bernoulli\\((p)\\) random variables. Here \\(\\bm \\theta=(p)\\) and the log-likelihood is \\[\\ell(p)=n\\bar y\\log p+n(1-\\bar y)\\log(1-p).\\] Differentiating with respect to \\(p\\), \\[\\frac{\\partial}{\\partial p} \\ell(p) = \\frac{n\\bar y}{p}-\\frac{n(1-\\bar y)}{1-p}\\] so the MLE \\(\\hat p\\) solves \\[\\frac{n\\bar y}{\\hat{p}} -{{n(1-\\bar y)}\\over{1-\\hat{p}}} = 0.\\] Solving this for \\(\\hat{p}\\) gives \\(\\hat{p}=\\bar y\\). Note that \\[\\frac{\\partial^2}{\\partial p^2} \\ell(p)= {{-n\\bar y}/p^2}-{{n(1-\\bar y)}/({1-p})^2}&lt;0\\] everywhere, so the stationary point is clearly a maximum. Example 2.4 (Normal) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. \\(N(\\mu,\\sigma^2)\\) random variables. Here \\(\\bm \\theta=(\\mu,\\sigma^2)\\) and and the log-likelihood is \\[\\ell(\\mu,\\sigma^2) = -{n\\over 2}\\log(2\\pi)-{n\\over 2}\\log(\\sigma^2) -{1\\over{2\\sigma^2}}\\sum(y_i-\\mu)^2.\\] Differentiating with respect to \\(\\mu\\) \\[{\\partial\\over{\\partial \\mu}} \\ell(\\mu,\\sigma^2)= {1\\over{\\sigma^2}}\\sum(y_i-\\mu)={{n(\\bar y-\\mu)}\\over{\\sigma^2}}\\] so \\((\\hat \\mu, \\hat \\sigma^2)\\) solve \\[\\begin{equation} \\frac{n(\\bar y-\\hat{\\mu})}{\\hat \\sigma^2} = 0. \\tag{2.1} \\end{equation}\\] Differentiating with respect to \\(\\sigma^2\\) \\[\\frac{\\partial}{\\partial \\sigma^2} \\ell (\\mu,\\sigma^2)= - \\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum(y_i-\\mu)^2,\\] so \\[\\begin{equation} -{n\\over {2\\hat \\sigma^2}}+{1\\over{2(\\hat \\sigma^2)^2}}\\sum(y_i-\\hat{\\mu})^2 = 0 \\tag{2.2} \\end{equation}\\] Solving (2.1) and (2.2), we obtain \\(\\hat{\\mu}= \\bar y\\) and \\[\\hat \\sigma^2 = {1\\over n}\\sum(y_i-\\hat{\\mu})^2= {1\\over n}\\sum(y_i-\\bar y)^2.\\] Strictly, to show that this stationary point is a maximum, we need to show that the Hessian matrix (the matrix of second derivatives with elements \\([\\bm{H}(\\bm \\theta)]_{ij}={{\\partial^2}\\over{\\partial\\theta_i\\partial\\theta_j}}\\ell(\\theta)\\)) is negative definite at \\(\\bm \\theta=\\hat{\\bm \\theta}\\), that is \\(\\bm{a}^T \\bm{H}(\\hat{\\bm \\theta})\\bm{a}&lt;0\\) for every \\(\\bm{a}\\ne {\\bf 0}\\). Here \\[ \\bm{H}(\\hat{\\mu},\\hat \\sigma^2)= \\begin{pmatrix} - \\frac{n}{\\hat \\sigma^2 } &amp; 0 \\cr 0 &amp;-\\frac{n}{2(\\hat \\sigma^2)^2} \\end{pmatrix} \\] which is clearly negative definite. 2.4 Score Let \\[ u_i(\\bm \\theta)\\equiv{{\\partial}\\over{\\partial\\theta_i}} \\ell(\\theta), \\quad i=1,\\ldots ,p \\] and \\(\\bm{u}(\\bm \\theta)\\equiv[u_1(\\bm \\theta),\\ldots ,u_p(\\bm \\theta)]^T\\). Then we call \\(\\bm{u}(\\bm \\theta)\\) the vector of scores or score vector. Where \\(p=1\\) and \\(\\bm \\theta=(\\theta)\\), the score is the scalar defined as \\[ u(\\theta)\\equiv{{\\partial}\\over{\\partial\\theta}}\\ell(\\theta). \\] The maximum likelihood estimate \\(\\hat{\\bm \\theta}\\) satisfies \\[u(\\hat{\\bm \\theta})={\\bm 0},\\] that is, \\[u_i(\\hat{\\bm \\theta})=0, \\quad i=1,\\ldots ,p.\\] Note that \\(u(\\bm{\\theta})\\) is a function of \\(\\bm \\theta\\) for fixed (observed) \\(\\bm y\\). However, if we replace \\(y_1, \\ldots, y_n\\) in \\(u(\\bm{\\theta})\\), by the corresponding random variables \\(Y_1, \\ldots, Y_n\\) then we obtain a vector of random variables \\(U(\\bm{\\theta})\\equiv [U_1(\\bm \\theta),\\ldots ,U_p(\\bm \\theta)]^T\\). An important result in likelihood theory is that the expected score at the true (but unknown) value of \\(\\bm \\theta\\) is zero: Theorem 2.1 We have \\(E[U(\\bm{\\theta})]={\\bm 0}\\), i.e. \\(E[U_i(\\bm \\theta)]= 0,\\) \\(i=1,\\ldots ,p,\\) provided that The expectation exists. The sample space for \\(\\bm Y\\) does not depend on \\(\\bm \\theta\\). Proof. Our proof is for continuous \\(\\bm y\\) – in the discrete case, replace \\(\\int\\) by \\(\\sum\\). For each \\(i=1, \\ldots, n\\) \\[\\begin{align*} E[U_i(\\bm \\theta)]&amp;=\\int U_i(\\bm \\theta)f_{\\bm Y}(\\bm y, \\bm \\theta) d\\bm y\\cr &amp;= \\int {{\\partial}\\over{\\partial\\theta_i}} \\ell(\\theta) f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= \\int {{\\partial}\\over{\\partial\\theta_i}} \\log f_{\\bm Y}(\\bm y; \\bm \\theta) f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= \\int {{{{\\partial}\\over{\\partial\\theta_i}}f_{\\bm Y}(\\bm y; \\bm \\theta)}\\over f_{\\bm Y}(\\bm y; \\bm \\theta)} f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= \\int {{\\partial}\\over{\\partial\\theta_i}}f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= {{\\partial}\\over{\\partial\\theta_i}}\\int f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= {{\\partial}\\over{\\partial\\theta_i}} 1 =0, \\end{align*}\\] as required. Example 2.5 (Bernoulli) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. Bernoulli\\((p)\\) random variables. Here \\(\\bm \\theta=(p)\\) and \\[u(p)=n\\bar y/ p-n(1-\\bar y)/(1-p).\\] Since \\(E[U(p)] = 0\\), we must have \\(E[\\bar Y]=p\\) (which we already know is correct). Example 2.6 (Normal) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. \\(N(\\mu,\\sigma^2)\\) random variables. Here \\(\\bm \\theta=(\\mu,\\sigma^2)\\) and \\[\\begin{align*} u_1(\\mu,\\sigma^2)&amp;= {{n(\\bar y-\\mu)}/{\\sigma^2}}\\cr u_2(\\mu,\\sigma^2)&amp;= -{n\\over {2\\sigma^2}}+{1\\over{2(\\sigma^2)^2}}\\sum_{i=1}^n{(y_i-\\mu)^2} \\end{align*}\\] Since \\(E[\\bm U(\\mu,\\sigma^2)] = {\\bm 0}\\), we must have \\(E[\\bar Y]=\\mu\\) and \\(E[{\\textstyle{1\\over n}}\\sum_{i=1}^n{(Y_i-\\mu)^2}]=\\sigma^2.\\) 2.5 Information Suppose that \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), whose joint p.d.f. \\(L(\\theta)\\) is completely specified except for the values of \\(p\\) unknown parameters \\(\\bm \\theta=(\\theta_1, \\ldots, \\theta_p)^T\\). Previously, we defined the Hessian matrix \\(H(\\bm{\\theta})\\) to be the matrix with components \\[ [H(\\bm{\\theta})]_{ij}\\equiv{{\\partial^2}\\over {\\partial\\theta_i\\partial\\theta_j}} \\ell(\\theta) \\qquad i=1,\\ldots ,p;\\;j=1,\\ldots ,p. \\] We call the matrix \\(-H(\\bm{\\theta})\\) the observed information matrix. Where \\(p=1\\) and \\(\\bm \\theta=(\\theta)\\), the observed information is a scalar defined as \\[ -H(\\theta)\\equiv-{{\\partial}\\over{\\partial\\theta^2}}\\ell(\\theta). \\] As with the score, if we replace \\(y_1, \\ldots, y_n\\) in \\(H(\\bm{\\theta})\\), by the corresponding random variables \\(Y_1, \\ldots, Y_n\\), we obtain a matrix of random variables. Then, we define the expected information matrix or Fisher information matrix \\[ [\\mathcal{I}(\\bm \\theta)]_{ij}=E(-[H(\\bm{\\theta})]_{ij}) \\qquad i=1,\\ldots ,p;\\;j=1,\\ldots ,p. \\] An important result in likelihood theory is that the variance-covariance matrix of the score vector is equal to the expected information matrix: Theorem 2.2 We have \\(\\text{Var}[U(\\bm{\\theta})]=\\mathcal{I}(\\bm \\theta)\\), i.e. \\[\\text{Var}[U(\\bm{\\theta})]_{ij}= [\\mathcal{I}(\\bm \\theta)]_{ij}, \\quad i=1,\\ldots ,p, \\quad j=1,\\ldots ,p\\] provided that The variance exists. The sample space for \\(\\bm Y\\) does not depend on \\(\\bm \\theta\\). Proof. Our proof is for continuous \\(\\bm y\\) – in the discrete case, replace \\(\\int\\) by \\(\\sum\\). For each \\(i = 1,\\ldots, p\\) and \\(j = 1, \\ldots, p\\), \\[\\begin{align*} \\text{Var}[U(\\bm{\\theta})]_{ij}&amp;= E[U_i(\\bm \\theta)U_j(\\bm \\theta)]\\cr &amp;= \\int {{\\partial}\\over{\\partial\\theta_i}} \\ell(\\theta) {{\\partial}\\over{\\partial\\theta_j}} \\ell(\\theta) f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= \\int {{\\partial}\\over{\\partial\\theta_i}} \\log f_{\\bm Y}(\\bm y; \\bm \\theta) {{\\partial}\\over{\\partial\\theta_j}} \\log f_{\\bm Y}(\\bm y; \\bm \\theta) f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= \\int {{{{\\partial}\\over{\\partial\\theta_i}}f_{\\bm Y}(\\bm y; \\bm \\theta)}\\over f_{\\bm Y}(\\bm y; \\bm \\theta)} {{{{\\partial}\\over{\\partial\\theta_j}}f_{\\bm Y}(\\bm y; \\bm \\theta)}\\over f_{\\bm Y}(\\bm y; \\bm \\theta)} f_{\\bm Y}(\\bm y; \\bm \\theta)d\\bm y\\cr &amp;= \\int \\frac{1}{f_{\\bm Y}(\\bm y; \\bm \\theta)}{{\\partial}\\over{\\partial\\theta_i}}f_{\\bm Y}(\\bm y; \\bm \\theta) {{\\partial}\\over{\\partial\\theta_j}} f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y. \\end{align*}\\] Now \\[\\begin{align*} [\\mathcal{I}(\\bm \\theta)]_{ij}&amp;=E\\left[-{{\\partial^2}\\over {\\partial\\theta_i\\partial\\theta_j}} \\ell(\\theta)\\right]\\cr &amp;=\\int -{{\\partial^2}\\over {\\partial\\theta_i\\partial\\theta_j}} \\log f_{\\bm Y}(\\bm y; \\bm \\theta) f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;=\\int -{{\\partial}\\over{\\partial\\theta_i}}\\left[ {{{{\\partial}\\over{\\partial\\theta_j}} f_{\\bm Y}(\\bm y; \\bm \\theta)}\\over f_{\\bm Y}(\\bm y; \\bm \\theta)}\\right] f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;=\\int \\left[ -{{{{\\partial^2}\\over{\\partial\\theta_i\\partial\\theta_j}} f_{\\bm Y}(\\bm y; \\bm \\theta)}\\over f_{\\bm Y}(\\bm y; \\bm \\theta)} + {{{{\\partial}\\over{\\partial\\theta_i}} f_{\\bm Y}(\\bm y; \\bm \\theta) {{\\partial}\\over{\\partial\\theta_j}} f_{\\bm Y}(\\bm y; \\bm \\theta)}\\over f_{\\bm Y}(\\bm y; \\bm \\theta)^2} \\right] f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= -{{\\partial^2}\\over{\\partial\\theta_i\\partial\\theta_j}}\\int f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y + \\int \\frac{1}{f_{\\bm Y}(\\bm y; \\bm \\theta)}{{\\partial}\\over{\\partial\\theta_i}} f_{\\bm Y}(\\bm y; \\bm \\theta) {{\\partial}\\over{\\partial\\theta_j}} f_{\\bm Y}(\\bm y; \\bm \\theta) d\\bm y\\cr &amp;= \\text{Var}[U(\\bm{\\theta})]_{ij}, \\end{align*}\\] as required. Example 2.7 (Bernoulli) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. Bernoulli\\((p)\\) random variables. Here \\(\\bm \\theta=(p)\\) and \\[\\begin{align*} u(p)&amp;= {{n\\bar y}\\over{ p}}-{{n(1-\\bar y)}\\over {(1-p)}}\\cr -H(p)&amp;= {{n\\bar y}\\over{ p^2}}+{{n(1-\\bar y)}\\over {(1-p)^2}}\\cr {\\cal I}(p)&amp;= {{n}\\over{ p}}+{{n}\\over {(1-p)}}={{n}\\over {p(1-p)}}. \\end{align*}\\] Example 2.8 (Normal) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. \\(N(\\mu,\\sigma^2)\\) random variables. Here \\(\\bm \\theta=(\\mu,\\sigma^2)\\) and \\[\\begin{align*} u_1(\\mu,\\sigma^2) &amp;= \\frac{n(\\bar y-\\mu)}{\\sigma^2} \\\\ u_2(\\mu,\\sigma^2) &amp;= -{n\\over {2\\sigma^2}}+{1\\over{2(\\sigma^2)^2}}\\sum(y_i-\\mu)^2. \\end{align*}\\] Therefore \\[ -\\bm{H}(\\mu,\\sigma^2) = \\begin{pmatrix} \\frac{n}{\\sigma^2} &amp; \\frac{n(\\bar y-\\mu)}{(\\sigma^2)^2} \\cr \\frac{n(\\bar y-\\mu)}{(\\sigma^2)^2}&amp; \\frac{1}{(\\sigma^2)^3} \\sum(y_i-\\mu)^2- \\frac{n}{2(\\sigma^2)^2} \\end{pmatrix} \\] \\[ {\\cal I}(\\mu,\\sigma^2)= \\begin{pmatrix} \\frac{n}{\\sigma^2} &amp; 0 \\cr 0&amp; \\frac{n}{2(\\sigma^2)^2} \\end{pmatrix}. \\] 2.6 Asymptotic distribution of the MLE Maximum likelihood estimation is an attractive method of estimation for a number of reasons. It is intuitively sensible and usually reasonably straightforward to carry out. Even when the simultaneous equations we obtain by differentiating the log-likelihood function are impossible to solve directly, solution by numerical methods is usually feasible. Perhaps the most compelling reason for considering maximum likelihood estimation is the asymptotic behaviour of maximum likelihood estimators. Suppose that \\(y_1, \\ldots, y_n\\) are observations of independent random variables \\(Y_1, \\ldots, Y_n\\), whose joint p.d.f. \\(f_{\\bm Y}(\\bm y;\\bm \\theta)=\\prod_{i=1}^n f_{Y_i}(y_i;\\bm \\theta)\\) is completely specified except for the values of an unknown parameter vector \\(\\bm \\theta\\), and that \\(\\hat{\\bm \\theta}\\) is the maximum likelihood estimator of \\(\\bm \\theta\\). Then, as \\(n\\to\\infty\\), the distribution of \\(\\hat{\\bm \\theta}\\) tends to a multivariate normal distribution with mean vector \\(\\bm \\theta\\) and variance covariance matrix \\(\\mathcal{I}(\\bm \\theta)^{-1}\\). Where \\(p=1\\) and \\(\\bm \\theta=(\\theta)\\), the distribution of the MLE \\(\\hat{\\theta}\\) tends to \\(N[\\theta,1/{\\cal I}(\\theta)]\\). For ‘large enough \\(n\\)’, we can treat the asymptotic distribution of the MLE as an approximation. The fact that \\(E(\\hat{\\bm \\theta})\\approx\\bm \\theta\\) means that the maximum likelihood estimator is approximately unbiased for large samples. The variance of \\(\\hat{\\bm \\theta}\\) is approximately \\(\\mathcal{I}(\\bm \\theta)^{-1}\\). It is possible to show that this is the smallest possible variance of any unbiased estimator of \\(\\bm \\theta\\) (this result is called the Cramér–Rao lower bound, which we do not prove here). Therefore the MLE is the ‘best possible’ estimator in large samples (and therefore we hope also reasonable in small samples, though we should investigate this case by case). 2.7 Quantifying uncertainty in parameter estimates The usefulness of an estimate is always enhanced if some kind of measure of its precision can also be provided. Usually, this will be a standard error, an estimate of the standard deviation of the associated estimator. For the maximum likelihood estimator \\(\\hat{\\theta}\\), a standard error is given by \\[ s.e.(\\hat{\\theta})={1\\over{{\\cal I}(\\hat{\\theta})^{{1\\over 2}}}}, \\] and for a vector parameter \\(\\bm \\theta\\) \\[ s.e.(\\hat{\\theta}_i)=[{\\cal I}(\\hat{\\bm \\theta})^{-1}]_{ii}^{{1\\over 2}}, \\quad i=1,\\ldots ,p. \\] An alternative summary of the information provided by the observed data about the location of a parameter \\(\\theta\\) and the associated precision is a confidence interval. The asymptotic distribution of the maximum likelihood estimator can be used to provide approximate large sample confidence intervals. Asymptotically, \\(\\hat{\\theta}_i\\) has a \\(N(\\theta_i,[\\mathcal{I}(\\bm \\theta)^{-1}]_{ii})\\) distribution and we can find \\(z_{1-\\frac{\\alpha}{2}}\\) such that \\[ P\\left(- z_{1-\\frac{\\alpha}{2}}\\le {{\\hat{\\theta}_i-\\theta_i}\\over{[\\mathcal{I}(\\bm \\theta)^{-1}]_{ii}^{1\\over 2}}}\\le z_{1-\\frac{\\alpha}{2}}\\right) = 1- \\alpha. \\] Therefore \\[ P\\left(\\hat{\\theta}_i-z_{1-\\frac{\\alpha}{2}}[\\mathcal{I}(\\bm \\theta)^{-1}]_{ii}^{1\\over 2}\\le\\theta_i \\le\\hat{\\theta}_i+z_{1-\\frac{\\alpha}{2}}[\\mathcal{I}(\\bm \\theta)^{-1}]_{ii}^{1\\over 2} \\right) = 1- \\alpha. \\] The endpoints of this interval cannot be evaluated because they also depend on the unknown parameter vector \\(\\bm \\theta\\). However, if we replace \\(\\mathcal{I}(\\bm \\theta)\\) by its MLE \\({\\cal I}(\\hat{\\bm \\theta})\\) we obtain the approximate large sample \\(100(1 - \\alpha)\\%\\) confidence interval \\[ [\\hat{\\theta}_i-z_{1-\\frac{\\alpha}{2}}[{\\cal I}(\\hat{\\bm \\theta})^{-1}]_{ii}^{1\\over 2}, \\hat{\\theta}_i+z_{1-\\frac{\\alpha}{2}}[{\\cal I}(\\hat{\\bm \\theta})^{-1}]_{ii}^{1\\over 2}]. \\] For \\(\\alpha=0.1,0.05,0.01\\), \\(z_{1-\\frac{\\alpha}{2}}=1.64,1.96,2.58\\). Example 2.9 (Bernoulli) If \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. Bernoulli\\((p)\\) random variables then asymptotically \\(\\hat{p}=\\bar y\\) has a \\(N(p,{p(1-p)}/ n)\\) distribution, and a large sample 95% confidence interval for \\(p\\) is \\[\\begin{align*} &amp; [\\hat{p}- 1.96[{\\cal I}(\\hat{p})^{-1}]^{1\\over 2}, \\hat{p}+1.96[{\\cal I}(\\hat{p})^{-1}]^{1\\over 2}] \\cr &amp;= [\\hat{p}-1.96[\\hat{p}(1-\\hat{p})/n]^{1\\over 2}, \\hat{p}+1.96[\\hat{p}(1-\\hat{p})/n]^{1\\over 2}]\\cr &amp;= [\\bar y-1.96[\\bar y(1-\\bar y)/n]^{1\\over 2}, \\bar y+1.96[\\bar y(1-\\bar y)/n]^{1\\over 2}]. \\end{align*}\\] "],
["comparing-statistical-models.html", "Chapter 3 Comparing statistical models 3.1 Introduction 3.2 Hypothesis testing 3.3 Likelihood ratio tests for nested hypotheses 3.4 Information criteria for model comparison", " Chapter 3 Comparing statistical models 3.1 Introduction If we have a set of competing probability models which might have generated the observed data, we may want to determine which of the models is most appropriate. In practice, we proceed by comparing models pairwise. Suppose that we have two competing alternatives, \\(f^{(0)}_{\\bm Y}\\) (model \\(M_0\\)) and \\(f^{(1)}_{\\bm Y}\\) (model \\(M_1\\)) for \\(f_{\\bm Y}\\), the joint distribution of \\(Y_1, \\ldots, Y_n\\). Often \\(H_0\\) and \\(H_1\\) both take the same parametric form, \\(f_{\\bm Y}(\\bm{y};\\bm \\theta)\\) but with \\(\\bm \\theta\\in\\Theta^{(0)}\\) for \\(H_0\\) and \\(\\bm \\theta\\in\\Theta^{(1)}\\) for \\(H_1\\), where \\(\\Theta^{(0)}\\) and \\(\\Theta^{(1)}\\) are alternative sets of possible values for \\(\\bm \\theta\\). In the regression setting, we are often interested in determining which of a set of explanatory variables have an impact on the distribution of the response. 3.2 Hypothesis testing A hypothesis test provides one mechanism for comparing two competing statistical models. A hypothesis test does not treat the two hypotheses (models) symmetrically. One hypothesis, \\[\\text{$H_0$: the data were generated from model $M_0$},\\] is accorded special status, and referred to as the null hypothesis. The null hypothesis is the reference model, and will be assumed to be appropriate unless the observed data strongly indicate that \\(H_0\\) is inappropriate, and that \\[\\text{$H_1$: the data were generated from model $M_1$},\\] (the alternative hypothesis) should be preferred. The fact that a hypothesis test does not reject \\(H_0\\) should not be taken as evidence that \\(H_0\\) is true and \\(H_1\\) is not, or that \\(H_0\\) is better supported by the data than \\(H_1\\), merely that the data does not provide sufficient evidence to reject \\(H_0\\) in favour of \\(H_1\\). A hypothesis test is defined by its critical region or rejection region, which we shall denote by \\(C\\). \\(C\\) is a subset of \\(\\mathbb{R}^n\\) and is the set of possible \\(\\bm{y}\\) which would lead to rejection of \\(H_0\\) in favour of \\(H_1\\), i.e. If \\(\\bm{y} \\in C\\), \\(H_0\\) is rejected in favour of \\(H_1\\); If \\(\\bm{y} \\not\\in C\\), \\(H_0\\) is not rejected. As \\(\\bm Y\\) is a random variable, there remains the possibility that a hypothesis test will produce an erroneous result. We define the size (or significance level) of the test \\[\\alpha = \\max_{\\bm \\theta\\in\\Theta^{(0)}}P(\\bm Y\\in C;\\bm \\theta)\\] This is the maximum probability of erroneously rejecting \\(H_0\\), over all possible distributions for \\(\\bm Y\\) implied by \\(H_0\\). We also define the power function \\[\\omega(\\bm \\theta)= P(\\bm Y\\in C;\\bm \\theta)\\] It represents the probability of rejecting \\(H_0\\) for a particular value of \\(\\bm \\theta\\). Clearly we would like to find a test with where \\(\\omega(\\bm \\theta)\\) is large for every \\(\\bm \\theta\\in\\Theta^{(1)}\\setminus\\Theta^{(0)}\\), while at the same time avoiding erroneous rejection of \\(H_0\\). In other words, a good test will have small size, but large power. The general hypothesis testing procedure is to fix \\(\\alpha\\) to be some small value (often 0.05), so that the probability of erroneous rejection of \\(H_0\\) is limited. In doing this, we are giving \\(H_0\\) precedence over \\(H_1\\). Given our specified \\(\\alpha\\), we try to choose a test, defined by its rejection region \\(C\\), to make \\(\\omega(\\bm \\theta)\\) as large as possible for \\(\\bm \\theta\\in\\Theta^{(1)}\\setminus\\Theta^{(0)}\\). 3.3 Likelihood ratio tests for nested hypotheses Suppose that \\(H_0\\) and \\(H_1\\) both take the same parametric form, \\(f_{\\bm Y}(\\bm{y};\\bm \\theta)\\) with \\(\\bm \\theta\\in\\Theta^{(0)}\\) for \\(H_0\\) and \\(\\bm \\theta\\in\\Theta^{(1)}\\) for \\(H_1\\), where \\(\\Theta^{(0)}\\) and \\(\\Theta^{(1)}\\) are alternative sets of possible values for \\(\\bm \\theta\\). A likelihood ratio test of \\(H_0\\) against \\(H_1\\) has a critical region of the form \\[\\begin{equation} C=\\left\\{ \\bm{y}: \\frac{\\max_{\\bm \\theta\\in \\Theta^{(1)}}L(\\bm \\theta)} {\\max_{\\bm \\theta\\in \\Theta^{(0)}}L(\\bm \\theta)} &gt;k\\right\\} \\tag{3.1} \\end{equation}\\] where \\(k\\) is determined by \\(\\alpha\\), the size of the test, so \\[\\max_{\\bm \\theta\\in\\Theta^{(0)}}P(\\bm{y}\\in C;\\bm \\theta)=\\alpha.\\] Therefore, we will only reject \\(H_0\\) if \\(H_1\\) offers a distribution for \\(Y_1, \\ldots, Y_n\\) which makes the observed data much more probable than any distribution under \\(H_0\\). This is intuitively appealing and tends to produce good tests (large power) across a wide range of examples. In order to determine \\(k\\) in (3.1), we need to know the distribution of the likelihood ratio, or an equivalent statistic, under \\(H_0\\). In general, this will not be available to us. However, we can make use of an important asymptotic result. First we notice that, as \\(\\log\\) is a strictly increasing function, the rejection region is equivalent to \\[ C=\\left\\{ \\bm{y}: 2\\log \\left({{\\max_{\\bm \\theta\\in \\Theta^{(1)}}L(\\bm \\theta)}\\over {\\max_{\\bm \\theta\\in \\Theta^{(0)}}L(\\bm \\theta)}}\\right) &gt;k&#39;\\right\\} \\] where \\[ \\max_{\\bm \\theta\\in\\Theta^{(0)}}P(\\bm{y}\\in C;\\bm \\theta)=\\alpha. \\] Write \\[L_{01}\\equiv 2\\log \\left(\\frac{\\max_{\\bm \\theta\\in \\Theta^{(1)}}L(\\bm \\theta)} {\\max_{\\bm \\theta\\in \\Theta^{(0)}}L(\\bm \\theta)}\\right)\\] for the log-likelihood ratio test statistic. Provided that \\(H_0\\) is nested within \\(H_1\\), the following result provides a useful large-\\(n\\) approximation to the distribution of \\(L_{01}\\). Theorem 3.1 Suppose that \\(H_0\\): \\(\\bm \\theta\\in\\Theta^{(0)}\\) and \\(H_1\\): \\(\\bm \\theta\\in\\Theta^{(1)}\\), where \\(\\Theta^{(0)}\\subset\\Theta^{(1)}\\). Let \\(d_0 = \\dim(\\Theta^{(0)})\\) and \\(d_1 = \\dim(\\Theta^{(1)})\\). Under \\(H_0\\), the distribution of \\(L_{01}\\) tends towards \\(\\chi^2_{d_1 - d_0}\\) as \\(n \\rightarrow \\infty\\). Proof. First we note that in the case where \\(\\bm \\theta\\) is one-dimensional and \\(\\bm \\theta=(\\theta)\\), a Taylor series expansion of \\(\\ell(\\theta)\\) around the MLE \\(\\hat{\\theta}\\) gives \\[ \\ell(\\theta)=\\ell(\\hat{\\theta})+(\\theta-\\hat{\\theta}) U(\\hat{\\theta})+{1\\over 2}(\\theta-\\hat{\\theta})^2 U&#39;(\\hat{\\theta}) +\\;\\ldots \\] Now, \\(U(\\hat{\\theta})=0\\), and if we approximate \\(U&#39;(\\hat{\\theta})\\equiv H(\\hat{\\theta})\\) by \\(E[H(\\theta)]\\equiv -{\\cal I}(\\theta)\\), and also ignore higher order terms, we obtain \\[ 2[\\ell (\\hat{\\theta})-\\ell(\\theta)]= (\\theta-\\hat{\\theta})^2 {\\cal I}(\\theta) \\] As \\(\\hat{\\theta}\\) is asymptotically \\(N[\\theta,{\\cal I}(\\theta)^{-1}]\\), \\((\\theta-\\hat{\\theta})^2 {\\cal I}(\\theta)\\) is asymptotically \\(\\chi^2_1\\), and hence so is \\(2[\\ell(\\hat \\theta)-\\ell (\\theta)]\\). Similarly it can be shown that when \\(\\bm \\theta\\in\\Theta\\), a multidimensional space, \\(2[\\ell(\\bm{\\hat \\theta})-\\ell (\\bm \\theta)]\\) is asymptotically \\(\\chi^2_p\\), where \\(p\\) is the dimension of \\(\\Theta\\). Now, suppose that \\(H_0\\) is true and \\(\\bm \\theta\\in\\Theta^{(0)}\\) and therefore \\(\\bm \\theta\\in\\Theta^{(1)}\\). Furthermore, suppose that \\(\\ell(\\bm \\theta)\\) is maximised in \\(\\Theta^{(0)}\\) by \\(\\hat{\\bm \\theta}^{(0)}\\) and is maximised in \\(\\Theta^{(1)}\\) by \\(\\hat{\\bm \\theta}^{(1)}\\). Then \\[\\begin{align*} L_{01}&amp;\\equiv 2\\log \\left({{\\max_{\\bm \\theta\\in \\Theta^{(1)}}L(\\bm \\theta)}\\over {\\max_{\\bm \\theta\\in \\Theta^{(0)}}L(\\bm \\theta)}}\\right)\\cr &amp;= 2\\log L(\\hat{\\bm \\theta}^{(1)})-2\\log L(\\hat{\\bm \\theta}^{(0)})\\cr &amp;= 2[\\log L(\\hat{\\bm \\theta}^{(1)})-\\log L(\\bm \\theta)] -2[\\log L(\\hat{\\bm \\theta}^{(0)})-\\log L(\\bm \\theta)]\\cr &amp;= L_1-L_0. \\end{align*}\\] Therefore \\(L_1=L_{01}+L_0\\) and we know that, under \\(H_0\\), \\(L_1\\) has a \\(\\chi^2_{d_1}\\) distribution and \\(L_0\\) has a \\(\\chi^2_{d_0}\\) distribution. Furthermore, it is possible to show (although we will not do so here) that under \\(H_0\\), \\(L_{01}\\) and \\(L_0\\) are independent. It can also be shown that under \\(H_0\\) the difference \\(L_1-L_0\\) can be expressed as a quadratic form of normal random variables. Therefore, it follows that under \\(H_0\\), the log likelihood ratio statistic \\(L_{01}\\) has a \\(\\chi^2_{d_1-d_0}\\) distribution. Example 3.1 (Bernoulli) \\(y_1, \\ldots, y_n\\) are observations of \\(Y_1, \\ldots, Y_n\\), i.i.d. Bernoulli\\((p)\\) random variables. Suppose that we require a size \\(\\alpha\\) test of the hypothesis \\(H_0\\): \\(p=p_0\\) against the general alternative \\(H_1\\): ‘\\(p\\) is unrestricted’ where \\(\\alpha\\) and \\(p_0\\) are specified. Here \\(\\bm \\theta=(p)\\), \\(\\Theta^{(0)}=\\{p_0\\}\\) and \\(\\Theta^{(1)}=(0,1)\\) and the log likelihood ratio statistic is \\[ L_{01}=2n\\bar y\\log\\left({{\\bar y}\\over{p_0}}\\right) +2n(1-\\bar y)\\log\\left({{1-\\bar y}\\over{1-p_0}}\\right). \\] As \\(d_1=1\\) and \\(d_0=0\\), under \\(H_0\\), the log likelihood ratio statistic has an asymptotic \\(\\chi^2_1\\) distribution. For a log likelihood ratio test, we only reject \\(H_0\\) in favour of \\(H_1\\) when the test statistic is too large (observed data are much more probable under model \\(H_1\\) than under model \\(H_0\\)), so in this case we reject \\(H_0\\) when the observed value of the test statistic above is ‘too large’ to have come from a \\(\\chi^2_1\\) distribution. What we mean by ‘too large’ depends on the significance level \\(\\alpha\\) of the test. For example, if \\(\\alpha=0.05\\), a common choice, then we should reject \\(H_0\\) if the test statistic is greater than the 3.84, the 95% point of the \\(\\chi^2_1\\) distribution. 3.4 Information criteria for model comparison It is more difficult to use the likelihood ratio test of Section 3.3 to compare two models if those models are not nested. An alternative approach is to record some criterion measuring the quality of the model for each of a candidate set of models, then choose the model which is the best according to this criterion. When we were estimating the unknown parameters \\(\\theta\\) of a model, we chose the value which maximised the likelihood: that is, the value of \\(\\theta\\) that maximises the probability of observing the data we actually saw. It is tempting to use a similar system for choosing between two models, and to choose the model which has the greater likelihood, under which the probability of seeing the data we actually observed is maximised. However, if we do this we will always end up choosing complicated models, which fit the observed data very closely, but do not meet our requirement of parsimony. For a given model depending on parameters \\(\\theta \\in \\mathbb{R}^p\\), let \\(\\hat L\\) be the likelihood function for that model evaluated at the MLE \\(\\hat \\theta\\). It is not sensible to choose between models by maximising \\(\\hat L\\) directly, and instead it is common to choose a model to maximise a criteria of the form \\[\\hat L - \\text{penalty},\\] where the penalty term will be large for complex models, and small for simple models. Equivalently, we may choose between models by minimising a criteria of the form \\[ - 2 \\hat L + \\text{penalty}.\\] By convention, many commonly-used criteria for model comparison take this form. For instance, the Akaike information criterion (AIC) is \\[\\text{AIC} = - 2 \\hat L + 2 p,\\] where \\(p\\) is the dimension of the unknown parameter in the candidate model, and the Bayesian information criterion (BIC) is \\[\\text{BIC} = - 2 \\hat L + \\log(n) p,\\] where \\(n\\) is the number of observations. "],
["lm.html", "Chapter 4 Linear Models 4.1 The linear model 4.2 Maximum likelihood estimation 4.3 Properties of the MLE 4.4 Comparing linear models", " Chapter 4 Linear Models 4.1 The linear model In practical applications, we often distinguish between a response variable and a group of explanatory variables. The aim is to determine the pattern of dependence of the response variable on the explanatory variables. We denote the \\(n\\) observations of the response variable by \\(\\bm{y}=(y_1,y_2,\\ldots ,y_n)^T\\). These are assumed to be observations of random variables \\(\\bm Y=(Y_1,Y_2,\\ldots ,Y_n)^T\\). Associated with each \\(y_i\\) is a vector \\(\\bm{x}_i=(x_{i1},x_{i2},\\ldots ,x_{ip})^T\\) of values of \\(p\\) explanatory variables. In a linear model, we assume that \\[\\begin{align} Y_i&amp;= \\beta_1 x_{i1} +\\beta_2 x_{i2} +\\ldots + \\beta_p x_{ip} + \\epsilon_i \\cr &amp;= \\sum_{j=1}^p x_{ij} \\beta_j + \\epsilon_i \\cr &amp;= \\bm{x}_i^T\\bm{\\beta} + \\epsilon_i \\cr &amp;= [\\bm{X}\\bm{\\beta}]_i + \\epsilon_i,\\qquad i=1,\\ldots ,n \\tag{4.1} \\end{align}\\] where \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently, \\[\\bm{X}= \\begin{pmatrix} \\bm{x}_1^T \\\\ \\vdots \\\\ \\bm{x}_n^T \\end{pmatrix} =\\begin{pmatrix} x_{11} &amp; \\cdots &amp;x_{1p}\\cr \\vdots &amp; \\ddots &amp;\\vdots\\cr x_{n1} &amp;\\cdots &amp;x_{np} \\end{pmatrix}\\] and \\(\\bm{\\beta}=(\\beta_1,\\ldots ,\\beta_p)^T\\) is a vector of fixed but unknown parameters describing the dependence of \\(Y_i\\) on \\(\\bm{x}_i\\). The four ways of describing the linear model in (4.1) are equivalent, but the most economical is the matrix form \\[\\begin{equation} \\bm{Y}=\\bm{X}\\bm{\\beta} + \\bm{\\epsilon}. \\tag{4.2} \\end{equation}\\] where \\(\\bm{\\epsilon}=(\\epsilon_1,\\epsilon_2,\\ldots ,\\epsilon_n)^T\\). The \\(n\\times p\\) matrix \\(\\bm{X}\\) consists of known (observed) constants and is called the design matrix. The \\(i\\)th row of \\(\\bm{X}\\) is \\(\\bm{x}_i^T\\), the explanatory data corresponding to the \\(i\\)th observation of the response. The \\(j\\)th column of \\(\\bm{X}\\) contains the \\(n\\) observations of the \\(j\\)th explanatory variable. The error vector \\(\\bm{\\epsilon}\\) has a multivariate normal distribution with mean vector \\({\\bf 0}\\) and variance covariance matrix \\(\\sigma^2\\bm{I}\\), since \\(\\text{Var}(\\epsilon_i)=\\sigma^2\\), and \\(\\text{Cov}(\\epsilon_i,\\epsilon_j)=0\\) as \\(\\epsilon_1, \\ldots, \\epsilon_n\\) are independent of one another. It follows from (4.2) that the distribution of \\(\\bm Y\\) is multivariate normal with mean vector \\(\\bm{X}\\bm{\\beta}\\) and variance covariance matrix \\(\\sigma^2\\bm{I}\\), i.e. \\(\\bm Y\\sim N(\\bm{X}\\bm{\\beta},\\sigma^2\\bm{I})\\). Example 4.1 (The null model) The null model \\[ Y_i=\\beta_1 + \\epsilon_i \\qquad i = 1, \\ldots, n \\] \\[ \\bm{X}=\\begin{pmatrix} 1\\cr 1\\cr \\vdots \\cr 1 \\end{pmatrix} \\qquad \\bm{\\beta}=(\\beta_1). \\] One (dummy) explanatory variable. In practice, this variable is present in all models. Example 4.2 (Simple linear regression) Simple linear regression \\[ Y_i=\\beta_1+\\beta_2 x_i + \\epsilon_i \\qquad i = 1, \\ldots, n \\] \\[ \\bm{X}=\\begin{pmatrix} 1&amp;x_1\\cr 1&amp;x_2\\cr \\vdots&amp;\\vdots\\cr 1&amp;x_n \\end{pmatrix} \\qquad \\bm{\\beta}=\\begin{pmatrix} \\beta_1\\cr\\beta_2 \\end{pmatrix} \\] Two explanatory variables; the dummy variable and one ‘real’ variable. Example 4.3 (Polynomial regression) \\[ Y_i=\\beta_1+\\beta_2 x_i+\\beta_3 x_i^2 +\\ldots +\\beta_p x_i^{p-1} + \\epsilon_i \\qquad i = 1, \\ldots, n \\] \\[ \\bm{X}= \\begin{pmatrix} 1&amp;x_1&amp;x_1^2&amp;\\cdots&amp;x_1^{p-1}\\cr 1&amp;x_2&amp;x_2^2&amp;\\cdots&amp;x_2^{p-1}\\cr \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\cr 1&amp;x_n&amp;x_n^2&amp;\\cdots&amp;x_n^{p-1} \\end{pmatrix} \\qquad \\bm{\\beta}= \\begin{pmatrix} \\beta_1\\cr\\beta_2\\cr\\vdots\\cr\\beta_p \\end{pmatrix} \\] \\(p\\) explanatory variables; the dummy variable and one ‘real’ variable, transformed to \\(p-1\\) variables. Example 4.4 (Multiple regression) \\[ Y_i=\\beta_1+\\beta_2 x_{i1}+\\beta_3 x_{i2} +\\ldots +\\beta_p x_{i\\,p-1} + \\epsilon_i \\qquad i = 1, \\ldots, n \\] \\[ \\bm{X}= \\begin{pmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\\cdots&amp;x_{1\\,p-1}\\cr 1&amp;x_{21}&amp;x_{22}&amp;\\cdots&amp;x_{2\\,p-1}\\cr \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\cr 1&amp;x_{n1}&amp;x_{n2}&amp;\\cdots&amp;x_{n\\,p-1} \\end{pmatrix} \\qquad \\bm{\\beta}=\\begin{pmatrix} \\beta_1\\cr\\beta_2\\cr\\vdots\\cr\\beta_p \\end{pmatrix} \\] \\(p\\) explanatory variables; the dummy variable and \\(p-1\\) ‘real’ variables. Example 4.5 (One categorical explanatory variable) Suppose \\(x_i\\) is a categorical variable, taking values in a set of \\(k\\) possible categories \\(\\{c_1, \\ldots, c_k\\}\\). We wish to model \\[Y_i = \\alpha_{x_i} + \\epsilon_i, \\qquad i = 1, \\ldots, n,\\] so that the mean of \\(Y_i\\) is the same for all observations in the same category, but differs for different categories. We can rewrite the model as a form of multiple regression by first defining a new explanatory variable \\(\\bm{z}_i\\) (sometimes called the one-hot encoding) \\[\\bm{z}_i = (z_{i1}, \\ldots, z_{ik})^T,\\] where \\[z_{ij} = \\begin{cases} 1 &amp; \\text{if $x_i = c_j$} \\\\ 0 &amp; \\text{otherwise}. \\end{cases}\\] We then have \\[Y_i=\\beta_1+\\beta_2 z_{i1}+\\beta_3 z_{i2} +\\ldots +\\beta_{k+1} z_{ik} + \\epsilon_i,\\] where \\(\\alpha_j = \\beta_1 + \\beta_{j+1}\\), or \\[\\bm{X}= \\begin{pmatrix} 1 &amp; z_{11} &amp; z_{12} &amp;\\cdots &amp; z_{1k}\\cr 1&amp;x_2&amp;x_2^2&amp;\\cdots&amp;x_2^{p-1}\\cr \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\cr 1&amp;x_n&amp;x_n^2&amp;\\cdots&amp;x_n^{p-1} \\end{pmatrix} \\qquad \\bm{\\beta}= \\begin{pmatrix} \\beta_1\\cr\\beta_2\\cr\\vdots\\cr\\beta_{k+1} \\end{pmatrix}\\] It is not possible to estimate all of the \\(\\bm \\beta\\) parameters separately, as they only affect the distribution through \\(\\beta_1 + \\beta_{j+1}\\). Instead, we choose a reference category \\(c_l\\), and set \\(\\beta_l = 0\\). The intercept term \\(\\beta_1\\) then gives the mean for the reference category, with \\(\\beta_j\\) giving the difference in mean between category \\(c_j\\) and the reference category. In R, categorical variables are called factors, and by default the reference category will be the first category when the names of the categories (the levels of the factor) are sorted alphabetically. 4.2 Maximum likelihood estimation The regression coefficients \\(\\beta_1, \\ldots, \\beta_p\\) describe the pattern by which the response depends on the explanatory variables. We use the observed data \\(y_1, \\ldots, y_n\\) to estimate this pattern of dependence. The likelihood for a linear model is \\[\\begin{equation} L(\\bm{\\beta},\\sigma^2)=\\left(2\\pi\\sigma^2\\right)^{-{n\\over 2}} \\exp\\left(-{1\\over{2\\sigma^2}} \\sum_{i=1}^n (y_i-\\bm{x}_i^T\\bm{\\beta})^2\\right). \\tag{4.3} \\end{equation}\\] This is maximised with respect to \\((\\bm{\\beta},\\sigma^2)\\) at \\[ \\hat{\\bm{\\beta}}=(\\bm{X}^T\\bm{X})^{-1}\\bm{X}^T\\bm{y} \\] and \\[ \\hat \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(y_i-\\bm{x}_i^T\\hat{\\bm{\\beta}}\\right)^2. \\] The corresponding fitted values are \\[\\hat{\\bm{y}}=\\bm{X}\\hat{\\bm{\\beta}}=\\bm{X}(\\bm{X}^T\\bm{X})^{-1}\\bm{X}^T\\bm{y}\\] or \\[\\hat{y}_i=\\bm{x}_i^T\\hat{\\bm{\\beta}}, \\quad i = 1, \\ldots, n.\\] The residuals \\(\\bm{r} = (r_1, \\ldots, r_n)\\) are \\(\\bm{r}=\\bm{y}-\\bm{\\hat y}\\) or \\(r_i=y_i-\\bm{x}_i^T\\hat{\\bm{\\beta}}\\) for \\(i = 1, \\ldots, n.\\). These residuals describe the variability in the observed responses \\(y_1, \\ldots, y_n\\) which has not been explained by the linear model. We call \\[D= \\sum_{i=1}^n r_i^2 = \\sum_{i=1}^n \\left(y_i-\\bm{x}_i^T\\hat{\\bm{\\beta}}\\right)^2\\] the residual sum of squares or deviance for the linear model. 4.3 Properties of the MLE As \\(\\bm Y\\) is normally distributed, and \\(\\hat{\\bm{\\beta}}= (\\bm{X}^T\\bm{X})^{-1}\\bm{X}^T\\bm{Y}\\) is a linear function of \\(\\bm Y\\), then \\(\\hat{\\bm{\\beta}}\\) must also be normally distributed. We have \\(E(\\hat{\\bm{\\beta}})=\\bm{\\beta}\\) and \\(\\text{Var}(\\hat{\\bm{\\beta}})=\\sigma^2(\\bm{X}^T\\bm{X})^{-1}\\), so \\[\\hat{\\bm{\\beta}} \\sim N(\\bm{\\beta}, \\sigma^2(\\bm{X}^T\\bm{X})^{-1}).\\] It is possible to prove (although we shall not do so here) that \\[ {D\\over\\sigma^2}\\sim\\chi^2_{n-p} \\] which implies that \\[ E(\\hat \\sigma^2)={{n-p}\\over n}\\sigma^2, \\] so the maximum likelihood estimator is biased for \\(\\sigma^2\\) (although still asymptotically unbiased as \\({{n-p}\\over n}\\to 1\\) as \\(n\\to\\infty\\)). We often use the unbiased estimator of \\(\\sigma^2\\) \\[ \\tilde \\sigma^2={D\\over {n-p}}={1\\over {n-p}}\\sum_{i=1}^n r_i^2. \\] The denominator \\(n-p\\), the number of observations minus the number of linear coefficients in the model is called the degrees of freedom of the model. Therefore, we estimate the residual variance by the deviance divided by the degrees of freedom. 4.4 Comparing linear models If we have a set of competing linear models which might explain the dependence of the response on the explanatory variables, we will want to determine which of the models is most appropriate. As described previously, we proceed by comparing models pairwise using a likelihood ratio test. For linear models this kind of comparison is restricted to situations where one of the models, \\(H_0\\), is nested in the other, \\(H_1\\). This usually means that the explanatory variables present in \\(H_0\\) are a subset of those present in \\(H_1\\). In this case model \\(H_0\\) is a special case of model \\(H_1\\), where certain coefficients are set equal to zero. We let \\(\\bm \\theta\\) represent the collection of linear parameters for model \\(H_1\\), together with the residual variance \\(\\sigma^2\\), and let \\(\\Theta^{(1)}\\) be the unrestricted parameter space for \\(\\bm \\theta\\). Then \\(\\Theta^{(0)}\\) is the parameter space corresponding to model \\(H_0\\), i.e. with the appropriate coefficients constrained to zero. We will assume that model \\(H_1\\) contains \\(p\\) linear parameters and model \\(H_0\\) a subset of \\(q&lt;p\\) of these. Without loss of generality, we can think of \\(H_1\\) as the model \\[ Y_i=\\sum_{j=1}^p x_{ij} \\beta_j + \\epsilon_i, \\quad i = 1, \\ldots, n \\] and \\(H_0\\) being the same model with \\[\\beta_{q+1}=\\beta_{q+2}=\\cdots=\\beta_p=0. \\] Now, a likelihood ratio test of \\(H_0\\) against \\(H_1\\) has a critical region of the form \\[ C=\\left\\{ \\bm{y}:{{\\max_{(\\bm{\\beta},\\sigma^2)\\in \\Theta^{(1)}}L(\\bm{\\beta},\\sigma^2)}\\over {\\max_{(\\bm{\\beta},\\sigma^2)\\in \\Theta^{(0)}}L(\\bm{\\beta},\\sigma^2)}} &gt;k\\right\\} \\] where \\(k\\) is determined by \\(\\alpha\\), the size of the test, so \\[ \\max_{\\bm \\theta\\in\\Theta^{(0)}}P(\\bm{y}\\in C;\\bm{\\beta},\\sigma^2)=\\alpha. \\] For a linear model, \\[ L(\\bm{\\beta},\\sigma^2)=\\left(2\\pi\\sigma^2\\right)^{-{n\\over 2}} \\exp\\left(-{1\\over{2\\sigma^2}} \\sum_{i=1}^n (y_i-\\bm{x}_i^T\\bm{\\beta})^2\\right). \\] This is maximised with respect to \\((\\bm{\\beta},\\sigma^2)\\) at \\(\\bm{\\beta}=\\hat{\\bm{\\beta}}\\) and \\(\\sigma^2=\\hat \\sigma^2=D/n\\). Therefore \\[\\begin{align*} \\max_{\\bm{\\beta},\\sigma^2} L(\\bm{\\beta},\\sigma^2)&amp;=(2\\pi D/n)^{-{n\\over 2}} \\exp\\left(-{n\\over{2D}} \\sum_{i=1}^n (y_i-\\bm{x}_i^T\\hat{\\bm{\\beta}})^2\\right)\\cr &amp;=(2\\pi D/n)^{-{n\\over 2}} \\exp\\left(-{n\\over2}\\right). \\end{align*}\\] This form applies for both \\(\\bm \\theta\\in\\Theta^{(0)}\\) and \\(\\bm \\theta\\in\\Theta^{(1)}\\), with only the model changing. Let the deviances under models \\(H_0\\) and \\(H_1\\) be denoted by \\(D_0\\) and \\(D_1\\) respectively. Then the critical region for the likelihood ratio test is of the form \\[{{(2\\pi D_1/n)^{-{n\\over 2}}}\\over{(2\\pi D_0/n)^{-{n\\over 2}}}}&gt;k\\] so \\[\\left({{D_0}\\over{D_1}}\\right)^{n\\over 2}&gt;k,\\] and \\[\\left({{D_0}\\over{D_1}}-1\\right){{n-p}\\over{p-q}}&gt;k&#39;\\] for some \\(k&#39;\\). Rearranging, \\[{{(D_0-D_1)/(p-q)}\\over{D_1/(n-p)}}&gt;k&#39;.\\] We refer to the left hand side of this inequality as the \\(F\\)-statistic. We reject the simpler model \\(H_0\\) in favour of the more complex model \\(H_1\\) if \\(F\\) is ‘too large’. As we have required \\(H_0\\) to be nested in \\(H_1\\), \\(F \\sim F_{p-q, \\, n-p}\\) when \\(H_0\\) is true. To see this, note that \\[ {{D_0}\\over\\sigma^2}={{D_0-D_1}\\over\\sigma^2}+{{D_1}\\over\\sigma^2}. \\] Furthermore, under \\(H_0\\), \\(D_1/\\sigma^2 \\sim \\chi^2_{n-p}\\) and \\(D_0/\\sigma^2 \\sim \\chi^2_{n-q}\\). It is possible to show (although we will not do so here) that under \\(H_0\\), \\((D_0-D_1)/\\sigma^2\\) and \\(D_0/\\sigma^2\\) are independent. Therefore, from the properties of the chi-squared distribution, it follows that under \\(H_0\\), \\((D_0-D_1)/\\sigma^2 \\sim \\chi^2_{p-q}\\), and \\(F \\sim F_{p-q,\\,n-p}\\) distribution. Therefore, the precise critical region can be evaluated given the size, \\(\\alpha\\), of the test. We reject \\(H_0\\) in favour of \\(H_1\\) when \\[ {{(D_0-D_1)/(p-q)}\\over{D_1/(n-p)}}&gt;k \\] where \\(k\\) is the \\(100(1-\\alpha)\\%\\) point of the \\(F_{p-q,\\,n-p}\\) distribution. "],
["glm.html", "Chapter 5 Generalised Linear Models 5.1 Regression models for non-normal data 5.2 The exponential family 5.3 Components of a generalised linear model 5.4 Examples of generalised linear models 5.5 Maximum likelihood estimation 5.6 Confidence intervals 5.7 Comparing generalised linear models 5.8 Scaled deviance and the saturated model 5.9 Models with unknown \\(a(\\phi)\\) 5.10 Residuals", " Chapter 5 Generalised Linear Models 5.1 Regression models for non-normal data The linear model of Chapter 4 assumes each response \\(Y_i \\sim N(\\mu_i, \\sigma^2)\\), where the mean \\(\\mu_i\\) depends on explanatory variables through \\(\\mu_i = \\bm x_i^T \\bm \\beta\\). For many types of data, this assumption of normality of the response may not be justified. For instance, we might have a binary response (\\(Y_i \\in \\{0, 1\\}\\)), for instance representing whether or not a patient recovers from a disease. A natural model is that \\(Y_i \\sim \\text{Bernoulli}(p_i)\\), and we might want to model how the ‘success’ probability \\(p_i\\) depends on explanatory variables \\(\\bm x_i\\). a count response (\\(Y_i \\in \\{0, 1, 2, 3, \\ldots\\}\\)), for instance representing the number of customers arrive at a shop. A natural model is that \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\), and we might want to model how the rate \\(\\lambda_i\\) depends on explanatory variables. In Section 5.2, we define the exponential family, which includes the Bernoulli and Poisson distributions as special cases. In a generalised linear model, the response distribution is assumed to be a member of the exponential family. To complete the specification of a generalised linear model, we will need to model how the parameters of the response distribution (e.g. the success probability \\(p_i\\) or the rate \\(\\lambda_i\\)) depend on explanatory variables \\(\\bm x_i\\). We need to do this in a way which respects constraints on the possible values which these parameters may take: for instance, we should not model \\(p_i = \\bm x_i^T \\bm \\beta\\) directly, as we need to enforce \\(p_i \\in [0, 1]\\). 5.2 The exponential family A probability distribution is said to be a member of the exponential family if its probability density function (or probability function, if discrete) can be written in the form \\[\\begin{equation} f_Y(y;\\theta,\\phi)=\\exp\\left({{y\\theta-b(\\theta)}\\over{a(\\phi)}} +c(y,\\phi)\\right). \\tag{5.1} \\end{equation}\\] The parameter \\(\\theta\\) is called the natural or canonical parameter. The parameter \\(\\phi\\) is usually assumed known. If it is unknown then it is often called the nuisance parameter. The density (5.1) can be thought of as a likelihood resulting from a single observation \\(y\\). Then the log-likelihood is \\[\\ell(\\theta,\\phi)={{y\\theta-b(\\theta)}\\over{a(\\phi)}} +c(y,\\phi)\\] and the score is \\[u(\\theta)=\\frac{\\partial}{\\partial \\theta}\\ell(\\theta,\\phi) ={{y-\\frac{\\partial}{\\partial \\theta} b(\\theta)}\\over{a(\\phi)}} ={{y- b&#39;(\\theta)}\\over{a(\\phi)}}.\\] The Hessian is \\[H(\\theta)=\\frac{\\partial^2}{\\partial \\theta^2}\\ell(\\theta,\\phi) =-{{\\frac{\\partial^2}{\\partial \\theta^2} b(\\theta)}\\over{a(\\phi)}} =-{{b&#39;&#39;(\\theta)}\\over{a(\\phi)}}\\] so the expected information is \\[{\\cal I}(\\theta)=E[-H(\\theta)]={{b&#39;&#39;(\\theta)}\\over{a(\\phi)}}.\\] From the properties of the score function in Section 2.4, we know that \\(E[U(\\theta)]=0\\). Therefore \\[E\\left[{{Y- b&#39;(\\theta)}\\over{a(\\phi)}}\\right]=0,\\] so \\(E[Y]=b&#39;(\\theta)\\). We often denote the mean by \\(\\mu\\), so \\(\\mu=b&#39;(\\theta)\\). Furthermore, \\[ \\text{Var}[U(\\theta)]= \\text{Var}\\left[{{Y- b&#39;(\\theta)}\\over{a(\\phi)}}\\right]= {{\\text{Var}[Y]}\\over{a(\\phi)^2}}, \\] as \\(b&#39;(\\theta)\\) and \\(a(\\phi)\\) are constants (not random variables). We also know from Section 2.5 that \\(\\text{Var}[U(\\theta)]={\\cal I}(\\theta)\\). Therefore \\[ \\text{Var}[Y]=a(\\phi)^2\\text{Var}[U(\\theta)]=a(\\phi)^2 {\\cal I}(\\theta) = a(\\phi)b&#39;&#39;(\\theta). \\] and hence the mean and variance of a random variable with probability density function (or probability function) of the form (5.1) are \\(b&#39;(\\theta)\\) and \\(a(\\phi)b&#39;&#39;(\\theta)\\) respectively. The variance is the product of two functions; \\(b&#39;&#39;(\\theta)\\) depends on the canonical parameter \\(\\theta\\) (and hence \\(\\mu\\)) only and is called the variance function (\\(V(\\mu)\\equiv b&#39;&#39;(\\theta)\\)); \\(a(\\phi)\\) is sometimes of the form \\(a(\\phi)=\\sigma^2/w\\) where \\(w\\) is a known weight and \\(\\sigma^2\\) is called the dispersion parameter or scale parameter. Example 5.1 (Normal distribution) Suppose \\(Y\\sim N(\\mu, \\, \\sigma^2)\\). Then \\[\\begin{align*} f_Y(y;\\mu,\\sigma^2)&amp;= {1\\over{\\sqrt{2\\pi\\sigma^2}}} \\exp\\left(-{1\\over{2\\sigma^2}}(y-\\mu)^2\\right)\\quad\\;\\; y\\in\\mathbb{R};\\;\\;\\mu\\in\\mathbb{R}\\cr &amp;= \\exp\\left({{y\\mu-{1\\over 2}\\mu^2}\\over \\sigma^2}-{1\\over 2}\\left[ {{y^2}\\over\\sigma^2}+\\log(2\\pi\\sigma^2)\\right]\\right). \\end{align*}\\] This is in the form (5.1), with \\(\\theta=\\mu\\), \\(b(\\theta)={1\\over 2}\\theta^2\\), \\(a(\\phi)=\\sigma^2\\) and \\[c(y,\\phi)=-{1\\over 2}\\left[ {{y^2}\\over{a(\\phi)}}+\\log(2\\pi a[\\phi])\\right]. \\] Therefore \\[E(Y)=b&#39;(\\theta)=\\theta=\\mu,\\] \\[\\text{Var}(Y)=a(\\phi)b&#39;&#39;(\\theta)=\\sigma^2\\] and the variance function is \\[V(\\mu)=1.\\] Example 5.2 (Poisson distribution) Suppose \\(Y\\sim \\text{Poisson}(\\lambda)\\). Then \\[\\begin{align*} f_Y(y;\\lambda)&amp;= {{\\exp(-\\lambda)\\lambda^y}\\over{y!}} \\qquad y\\in\\{0,1,\\ldots\\};\\quad\\lambda\\in{\\cal R}_+\\cr &amp;= \\exp\\left(y\\log\\lambda-\\lambda-\\log y!\\right). \\end{align*}\\] This is in the form (5.1), with \\(\\theta=\\log\\lambda\\), \\(b(\\theta)=\\exp\\theta\\), \\(a(\\phi)=1\\) and \\(c(y,\\phi)=-\\log y!\\). Therefore \\[E(Y)=b&#39;(\\theta)=\\exp\\theta=\\lambda,\\] \\[\\text{Var}(Y)=a(\\phi)b&#39;&#39;(\\theta)=\\exp\\theta=\\lambda\\] and the variance function is \\[V(\\mu)=\\mu.\\] Example 5.3 (Bernoulli distribution) Suppose \\(Y\\sim \\text{Bernoulli}(p)\\). Then \\[\\begin{align*} f_Y(y;p)&amp;= p^y(1-p)^{1-y}\\qquad y\\in\\{0,1\\};\\quad p\\in(0,1)\\cr &amp;= \\exp\\left(y\\log{p\\over{1-p}}+\\log(1-p)\\right) \\end{align*}\\] This is in the form (5.1), with \\(\\theta=\\log{p\\over{1-p}}\\), \\(b(\\theta)=\\log(1+\\exp\\theta)\\), \\(a(\\phi)=1\\) and \\(c(y,\\phi)=0\\). Therefore \\[E(Y)=b&#39;(\\theta)={{\\exp\\theta}\\over{1+\\exp\\theta}}=p,\\] \\[\\text{Var}(Y)=a(\\phi)b&#39;&#39;(\\theta)={{\\exp\\theta}\\over{(1+\\exp\\theta})^2}=p(1-p)\\] and the variance function is \\[V(\\mu)=\\mu(1-\\mu).\\] Example 5.4 (Binomial distribution) Suppose \\(Y^*\\sim \\text{Binomial}(n,p)\\). Here, \\(n\\) is assumed known (as usual) and the random variable \\(Y= Y^*/n\\) is taken as the proportion of successes, so \\[\\begin{align*} f_Y(y;p)&amp;=\\left({n\\atop{ny}}\\right) p^{ny} (1-p)^{n(1-y)}\\qquad y\\in\\left\\{0,{1\\over n},{2\\over n},\\ldots ,1\\right\\}; \\quad p\\in(0,1)\\cr &amp;= \\exp\\left({{y\\log{p\\over{1-p}}+\\log(1-p)}\\over{1\\over n}} +\\log\\!\\left({n\\atop{ny}}\\right)\\right). \\end{align*}\\] This is in the form (5.1), with \\(\\theta=\\log{p\\over{1-p}}\\), \\(b(\\theta)=\\log(1+\\exp\\theta)\\), \\(a(\\phi)={1\\over n}\\) and \\(c(y,\\phi)=\\log\\!\\left({n\\atop{ny}}\\right)\\). Therefore \\[E(Y)=b&#39;(\\theta)={{\\exp\\theta}\\over{1+\\exp\\theta}}=p,\\] \\[\\text{Var}(Y)=a(\\phi)b&#39;&#39;(\\theta)={1\\over n}{{\\exp\\theta}\\over{(1+\\exp\\theta})^2}= {{p(1-p)}\\over n}\\] and the variance function is \\[V(\\mu)=\\mu(1-\\mu).\\] Here, we can write \\(a(\\phi)\\equiv \\sigma^2/w\\) where the scale parameter \\(\\sigma^2=1\\) and the weight \\(w\\) is \\(n\\), the binomial denominator. 5.3 Components of a generalised linear model 5.3.1 The random component As in a linear model, the aim is to determine the pattern of dependence of a response variable on explanatory variables. We denote the \\(n\\) observations of the response by \\(\\bm{y}=(y_1,y_2,\\ldots ,y_n)^T\\). In a generalised linear model (GLM), these are assumed to be observations of independent random variables \\(\\bm{Y}=(Y_1,Y_2,\\ldots ,Y_n)^T\\), which take the same distribution from the exponential family. In other words, the functions \\(a\\), \\(b\\) and \\(c\\) and usually the scale parameter \\(\\phi\\) are the same for all observations, but the canonical parameter \\(\\theta\\) may differ. Therefore, we write \\[ f_{Y_i}(y_i;\\theta_i,\\phi_i)= \\exp\\left({{y_i\\theta_i-b(\\theta_i)}\\over{a(\\phi_i)}} +c(y_i,\\phi_i)\\right) \\] and the joint density for \\(\\bm{Y}=(Y_1,Y_2,\\ldots ,Y_n)^T\\) is \\[\\begin{align} f_{\\bm{Y}}(\\bm{y};\\bm{\\theta},\\bm{\\phi}) &amp;= \\prod_{i=1}^n f_{Y_i}(y_i;\\theta_i,\\phi_i) \\cr &amp;= \\exp\\left(\\sum_{i=1}^n{{y_i\\theta_i-b(\\theta_i)}\\over{a(\\phi_i)}} +\\sum_{i=1}^nc(y_i,\\phi_i)\\right) \\tag{5.2} \\end{align}\\] where \\(\\bm{\\theta}=(\\theta_1,\\ldots ,\\theta_n)^T\\) is the collection of canonical parameters and \\(\\bm{\\phi}=(\\phi_1,\\ldots ,\\phi_n)^T\\) is the collection of nuisance parameters (where they exist). Note that for a particular sample of observed responses, \\(\\bm{y}=(y_1,y_2,\\ldots ,y_n)^T\\), (5.2) is the likelihood function \\(L(\\bm{\\theta}, \\bm{\\phi})\\) for \\(\\bm{\\theta}\\) and \\(\\bm{\\phi}\\). 5.3.2 The systematic (or structural) component Associated with each \\(y_i\\) is a vector \\(\\bm{x}_i=(x_{i1},x_{i2},\\ldots ,x_{ip})^T\\) of \\(p\\) explanatory variables. In a generalised linear model, the distribution of the response variable \\(Y_i\\) depends on \\(\\bm{x}_i\\) through the linear predictor \\(\\eta_i\\) where \\[\\begin{align} \\eta_i &amp;=\\beta_1 x_{i1} +\\beta_2 x_{i2} +\\ldots + \\beta_p x_{ip} \\notag \\\\ &amp;= \\sum_{j=1}^p x_{ij} \\beta_j \\notag \\\\ &amp;= \\bm{x}_i^T \\bm{\\beta} \\notag \\\\ &amp;= [\\bm{X}\\bm{\\beta}]_i,\\qquad i=1,\\ldots ,n, \\tag{5.3} \\end{align}\\] where, as with a linear model, \\[ \\bm{X}=\\begin{pmatrix} \\bm{x}_1^T\\cr\\vdots\\cr \\bm{x}_n^T \\end{pmatrix} =\\begin{pmatrix} x_{11}&amp;\\cdots&amp;x_{1p}\\cr\\vdots&amp;\\ddots&amp;\\vdots\\cr x_{n1}&amp;\\cdots&amp;x_{np}\\end{pmatrix} \\] and \\(\\bm{\\beta}=(\\beta_1,\\ldots ,\\beta_p)^T\\) is a vector of fixed but unknown parameters describing the dependence of \\(Y_i\\) on \\(\\bm{x}_i\\). The four ways of describing the linear predictor in (5.3) are equivalent, but the most economical is the matrix form \\[\\begin{equation} \\bm{\\eta}=\\bm{X}\\bm{\\beta}. \\tag{5.4} \\end{equation}\\] Again, we call the \\(n\\times p\\) matrix \\(\\bm{X}\\) the design matrix. The \\(i\\)th row of \\(\\bm{X}\\) is \\(\\bm{x}_i^T\\), the explanatory data corresponding to the \\(i\\)th observation of the response. The \\(j\\)th column of \\(\\bm{X}\\) contains the \\(n\\) observations of the \\(j\\)th explanatory variable. 5.3.3 The link function For specifying the pattern of dependence of the response variable on the explanatory variables, the canonical parameters \\(\\theta_1,\\ldots,\\theta_n\\) in (5.2) are not of direct interest. Furthermore, we have already specified that the distribution of \\(Y_i\\) should depend on \\(\\bm{x}_i\\) through the linear predictor \\(\\eta_i\\). It is the parameters \\(\\beta_1,\\ldots ,\\beta_p\\) of the linear predictor which are of primary interest. The link between the distribution of \\(\\bm{Y}\\) and the linear predictor \\(\\bm{\\eta}\\) is provided by the link function \\(g\\), \\[ \\eta_i=g(\\mu_i),\\quad i = 1, \\ldots, n, \\] where \\(\\mu_i\\equiv E(Y_i),\\;i = 1, \\ldots, n\\). Hence, the dependence of the distribution of the response on the explanatory variables is established as \\[ g(E[Y_i])=g(\\mu_i)=\\eta_i=\\bm{x}_i^T\\bm{\\beta},\\quad i = 1, \\ldots, n, \\] In principle, the link function \\(g\\) can be any one-to-one differentiable function. However, we note that \\(\\eta_i\\) can in principle take any value in \\(\\mathbb{R}\\) (as we make no restriction on possible values taken by explanatory variables or model parameters). However, for some exponential family distributions \\(\\mu_i\\) is restricted. For example, for the Poisson distribution \\(\\mu_i\\in\\mathbb{R}_+\\); for the Bernoulli distribution \\(\\mu_i\\in(0,1)\\). If \\(g\\) is not chosen carefully, then there may exist a possible \\(\\bm{x}_i\\) and \\(\\bm{\\beta}\\) such that \\(\\eta_i\\ne g(\\mu_i)\\) for any possible value of \\(\\mu_i\\). Therefore, ‘sensible’ choices of link function map the set of allowed values for \\(\\mu_i\\) onto \\(\\mathbb{R}\\). Recall that for a random variable \\(Y\\) with a distribution from the exponential family, \\(E(Y)=b&#39;(\\theta)\\). Hence, for a generalised linear model \\[ \\mu_i=E(Y_i)=b&#39;(\\theta_i),\\quad i = 1, \\ldots, n. \\] Therefore \\[ \\theta_i=b^{&#39;-1}(\\mu_i),\\quad i = 1, \\ldots, n \\] and as \\(g(\\mu_i)=\\eta_i=\\bm{x}_i^T\\bm{\\beta}\\), then \\[\\begin{equation} \\theta_i=b^{&#39;-1}(g^{-1}[\\bm{x}_i^T\\bm{\\beta}]),\\quad i = 1, \\ldots, n. \\tag{5.5} \\end{equation}\\] Hence, we can express the joint density (5.2) in terms of the coefficients \\(\\bm{\\beta}\\), and for observed data \\(\\bm{y}\\), this is the likelihood \\(L(\\bm{\\beta})\\) for \\(\\bm{\\beta}\\). As \\(\\bm{\\beta}\\) is our parameter of real interest (describing the dependence of the response on the explanatory variables) this likelihood will play a crucial role. Note that considerable simplification is obtained in (5.5) if the functions \\(g\\) and \\(b^{&#39;-1}\\) are identical. Then \\[ \\theta_i=\\bm{x}_i^T\\bm{\\beta}\\qquad i = 1, \\ldots, n \\] and the resulting likelihood is \\[ L(\\bm{\\beta})= \\exp\\left(\\sum_{i=1}^n{{y_i\\bm{x}_i^T\\bm{\\beta}-b(\\bm{x}_i^T\\bm{\\beta})}\\over{a(\\phi_i)}} +\\sum_{i=1}^nc(y_i,\\phi_i)\\right). \\] The link function \\[ g(\\mu)\\equiv b^{&#39;-1}(\\mu) \\] is called the canonical link function. Under the canonical link, the canonical parameter is equal to the linear predictor. The canonical link functions are: Distribution \\(b(\\theta)\\) \\(b&#39;(\\theta)\\equiv\\mu\\) \\(b^{&#39;-1}(\\mu)\\equiv\\theta\\) Link Name Normal \\({1\\over 2}\\theta^2\\) \\(\\theta\\) \\(\\mu\\) \\(g(\\mu)=\\mu\\) Identity Poisson \\(\\exp\\theta\\) \\(\\exp\\theta\\) \\(\\log\\mu\\) \\(g(\\mu)=\\log\\mu\\) Log Binomial \\(\\log(1+\\exp\\theta)\\) \\(\\frac{\\exp\\theta}{1+\\exp\\theta}\\) \\(\\log{\\frac{\\mu}{1-\\mu}}\\) \\(g(\\mu)=\\log{\\frac{\\mu}{1-\\mu}}\\) Logit 5.4 Examples of generalised linear models 5.4.1 The linear model The linear model considered in Chapter 4 is also a generalised linear model. We assume \\({Y_1,\\ldots ,Y_n}\\) are independent normally distributed random variables, and the normal distribution is a member of the exponential family. Furthermore, the explanatory variables enter a linear model through the linear predictor \\[ \\eta_i=\\bm{x}_i^T\\bm{\\beta}, \\quad i = 1, \\ldots, n. \\] Finally, the link between \\(E(\\bm{Y})=\\bm{\\mu}\\) and the linear predictor \\(\\bm{\\eta}\\) is through the (canonical) identity link function \\[ \\mu_i=\\eta_i, \\quad i = 1, \\ldots, n. \\] 5.4.2 Models for binary data In binary regression, we assume either \\(Y_i \\sim \\text{Bernoulli}(p_i)\\), or \\(Y_i \\sim \\text{binomial}(n_i, p_i)\\), where \\(n_i\\) are known. The objective is to model the success probability \\(p_i\\) as a function of the explanatory variables \\(\\bm x_i\\). When the canonical (logit) link is used, we have \\[\\text{logit}(p_i) = \\log \\frac{p_i}{1-p_i} = \\eta_i = \\bm{x}_i^T\\bm{\\beta}.\\] This implies \\[p_i = \\frac{ \\exp(\\eta_i) }{1+ \\exp(\\eta_i)} = \\frac{1}{1+ \\exp(-\\eta_i)}.\\] The function \\(F(\\eta) = \\frac{1}{1+ \\exp(-\\eta)}\\) is the cumulative distribution function (cdf) of a distribution called the logistic distribution. The cumulative distribution functions of other distributions are also commonly used to generate link functions for binary regression. For example, if we let \\[p_i = \\Phi(\\bm{x}_i^T \\bm{\\beta}) = \\Phi(\\eta_i),\\] where \\(\\Phi(\\cdot)\\) is the cdf of the standard normal distribution, then we get the link function \\[g(\\mu) = g(p) = \\Phi^{-1}(\\mu) = \\eta,\\] which is called the probit link. 5.4.3 Models for count data If \\(Y_i\\) represent counts of the number of times an event occurs in a fixed time (or a fixed region of space), we might model \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\). With the canonical (log) link, we have \\[\\log \\lambda_i = \\eta_i = \\bm{x}_i^T\\bm{\\beta},\\] or \\[\\lambda_i = \\exp\\{\\eta_i\\} = \\exp\\{\\bm{x}_i^T\\bm{\\beta}\\}.\\] This model is often called a log-linear model. Now suppose that \\(Y_i\\) represents a count of the number of events which occur in a given region \\(i\\), for instance the number of times a particular drug is prescribed on a given day, in a district \\(i\\) of a country. We might want to model the prescription rate per patient in the district \\(\\lambda_i^*\\). Write \\(N_i\\) is the number of patients registered in district \\(i\\), often called the exposure of observation \\(i\\). We model \\(Y_i \\sim \\text{Poisson}(N_i \\lambda_i^*)\\), where \\[\\log \\lambda_i^* = \\bm{x}_i^T\\bm{\\beta}.\\] Equivalently, we may write the model as \\(Y_i \\sim \\text{Poisson}(\\lambda_i)\\), where \\[\\log \\lambda_i = \\log N_i + \\bm{x}_i^T\\bm{\\beta},\\] (since \\(\\lambda_i = N_i \\lambda_i^*\\), so \\(\\log \\lambda_i = \\log N_i + \\log \\lambda_i^*\\)). The log-exposure \\(\\log N_i\\) appears as a fixed term in the linear predictor, without any associated parameter. Such a fixed term is called an offset. 5.5 Maximum likelihood estimation The regression coefficients \\({\\beta_1,\\ldots ,\\beta_p}\\) describe the pattern by which the response depends on the explanatory variables. We use the observed data \\({y_1,\\ldots ,y_n}\\) to estimate this pattern of dependence. As usual, we maximise the log-likelihood function which, from (5.2), can be written \\[\\begin{equation} \\ell(\\bm{\\beta},\\bm{\\phi})= \\sum_{i=1}^n{{y_i\\theta_i-b(\\theta_i)}\\over{a(\\phi_i)}} +\\sum_{i=1}^nc(y_i,\\phi_i) \\tag{5.6} \\end{equation}\\] and depends on \\(\\bm{\\beta}\\) through \\[\\begin{align*} \\theta_i &amp;= (b&#39;)^{-1}(\\mu_i), \\cr \\mu_i&amp;= g^{-1}(\\eta_i), \\cr \\eta_i&amp;=\\bm{x}_i^T\\bm{\\beta}=\\sum_{i=1}^p x_{ij} \\beta_j, \\quad i = 1, \\ldots, n. \\end{align*}\\] To find \\(\\hat{\\bm{\\beta}}\\), we consider the scores \\[ u_k(\\bm{\\beta})={\\partial\\over{\\partial\\beta_k}} \\ell(\\bm{\\beta},\\bm{\\phi})\\qquad k=1,\\ldots ,p \\] and then find \\(\\hat{\\bm{\\beta}}\\) to solve \\(u_k(\\hat{\\bm{\\beta}})=0\\) for \\(k=1,\\ldots ,p.\\) From (5.6) \\[\\begin{align*} u_k(\\bm{\\beta})&amp;= {\\partial\\over{\\partial\\beta_k}}\\ell(\\bm{\\beta},\\bm{\\phi})\\cr &amp;= {\\partial\\over{\\partial\\beta_k}}\\sum_{i=1}^n{{y_i\\theta_i-b(\\theta_i)}\\over{a(\\phi_i)}} +{\\partial\\over{\\partial\\beta_k}}\\sum_{i=1}^nc(y_i,\\phi_i)\\cr &amp;= \\sum_{i=1}^n{\\partial\\over{\\partial\\beta_k}} \\left[{{y_i\\theta_i-b(\\theta_i)}\\over{a(\\phi_i)}}\\right]\\cr &amp;=\\sum_{i=1}^n{\\partial\\over{\\partial\\theta_i}}\\left[{{y_i\\theta_i-b(\\theta_i)} \\over{a(\\phi_i)}}\\right]{{\\partial\\theta_i}\\over{\\partial\\mu_i}} {{\\partial\\mu_i}\\over{\\partial\\eta_i}}{{\\partial\\eta_i}\\over{\\partial\\beta_k}}\\cr &amp;= \\sum_{i=1}^n{{y_i-b&#39;(\\theta_i)} \\over{a(\\phi_i)}}{{\\partial\\theta_i}\\over{\\partial\\mu_i}} {{\\partial\\mu_i}\\over{\\partial\\eta_i}}{{\\partial\\eta_i}\\over{\\partial\\beta_k}}, \\quad{k=1,\\ldots ,p},\\cr \\end{align*}\\] where \\[\\begin{align*} {{\\partial\\theta_i}\\over{\\partial\\mu_i}}&amp;=\\left[{{\\partial\\mu_i}\\over{\\partial\\theta_i}}\\right]^{-1} ={1\\over{b&#39;&#39;(\\theta_i)}}\\cr {{\\partial\\mu_i}\\over{\\partial\\eta_i}}&amp;=\\left[{{\\partial\\eta_i}\\over{\\partial\\mu_i}}\\right]^{-1} ={1\\over{g&#39;(\\mu_i)}}\\cr {{\\partial\\eta_i}\\over{\\partial\\beta_k}}&amp;= {\\partial\\over{\\partial\\beta_k}}\\sum_{j=1}^p x_{ij}\\beta_j=x_{ik}. \\end{align*}\\] Therefore \\[\\begin{equation} u_k(\\bm{\\beta})= \\sum_{i=1}^n{{y_i-b&#39;(\\theta_i)}\\over{a(\\phi_i)}} {{x_{ik}}\\over{b&#39;&#39;(\\theta_i)g&#39;(\\mu_i)}} =\\sum_{i=1}^n{{y_i-\\mu_i}\\over{\\text{Var}(Y_i)}} {{x_{ik}}\\over{g&#39;(\\mu_i)}},\\quad{k=1,\\ldots ,p}, \\tag{5.7} \\end{equation}\\] which depends on \\(\\bm{\\beta}\\) through \\(\\mu_i\\equiv E(Y_i)\\) and \\(\\text{Var}(Y_i),\\) \\(i = 1, \\ldots, n\\). In theory, we solve the \\(p\\) simultaneous equations \\(u_k(\\hat{\\bm{\\beta}})=0,\\;{k=1,\\ldots ,p}\\) to evaluate \\(\\hat{\\bm{\\beta}}\\). In practice, these equations are usually non-linear and have no analytic solution. Therefore, we rely on numerical methods to solve them. First, we note that the Hessian and Fisher information matrices can be derived directly from (5.7). \\[ [\\bm{H}(\\bm{\\beta})]_{jk}={{\\partial^2}\\over{\\partial\\beta_j\\partial\\beta_k}}\\ell(\\bm{\\beta},\\bm{\\phi}) ={\\partial\\over{\\partial\\beta_j}}u_k(\\bm{\\beta}). \\] Therefore \\[\\begin{align*} [\\bm{H}(\\bm{\\beta})]_{jk} &amp;={\\partial\\over{\\partial\\beta_j}}\\sum_{i=1}^n{{y_i-\\mu_i}\\over{\\text{Var}(Y_i)}} {{x_{ik}}\\over{g&#39;(\\mu_i)}}\\cr &amp;=\\sum_{i=1}^n{{-{{\\partial\\mu_i}\\over{\\partial\\beta_j}}}\\over{\\text{Var}(Y_i)}} {{x_{ik}}\\over{g&#39;(\\mu_i)}} +\\sum_{i=1}^n(y_i-\\mu_i){\\partial\\over{\\partial\\beta_j}} \\left[{{x_{ik}}\\over{\\text{Var}(Y_i) g&#39;(\\mu_i)}}\\right] \\end{align*}\\] and \\[\\begin{align*} [{\\cal I}(\\bm{\\beta})]_{jk} &amp;=\\sum_{i=1}^n{{{{\\partial\\mu_i}\\over{\\partial\\beta_j}}}\\over{\\text{Var}(Y_i)}} {{x_{ik}}\\over{g&#39;(\\mu_i)}} -\\sum_{i=1}^n(E[Y_i]-\\mu_i){\\partial\\over{\\partial\\beta_j}} \\left[{{x_{ik}}\\over{\\text{Var}(Y_i) g&#39;(\\mu_i)}}\\right]\\cr &amp;=\\sum_{i=1}^n{{{{\\partial\\mu_i}\\over{\\partial\\beta_j}}}\\over{\\text{Var}(Y_i)}} {{x_{ik}}\\over{g&#39;(\\mu_i)}}\\cr &amp;=\\sum_{i=1}^n{{x_{ij}x_{ik}}\\over{\\text{Var}(Y_i)g&#39;(\\mu_i)^2}}. \\end{align*}\\] Hence we can write \\[\\begin{equation} {\\cal I}(\\bm{\\beta})=\\bm{X}^T\\bm{W}\\bm{X} \\tag{5.8} \\end{equation}\\] where \\[ \\bm{X}=\\begin{pmatrix} \\bm{x}_1^T\\cr\\vdots\\cr \\bm{x}_n^T \\end{pmatrix} =\\begin{pmatrix} x_{11}&amp;\\cdots&amp;x_{1p}\\cr\\vdots&amp;\\ddots&amp;\\vdots\\cr x_{n1}&amp;\\cdots&amp;x_{np} \\end{pmatrix}, \\] \\[ \\bm{W}={\\rm diag}(\\bm{w})= \\begin{pmatrix} w_1&amp;0&amp;\\cdots&amp;0\\cr 0&amp;w_2&amp;&amp;\\vdots\\cr \\vdots&amp;&amp;\\ddots&amp;0\\cr 0&amp;\\cdots&amp;0&amp;w_n \\end{pmatrix} \\] and \\[ w_i={1\\over{\\text{Var}(Y_i)g&#39;(\\mu_i)^2}},\\quad i = 1, \\ldots, n. \\] The Fisher information matrix \\(\\mathcal{I}(\\bm{\\beta})\\) depends on \\(\\bm{\\beta}\\) through \\(\\bm{\\mu}\\) and \\(\\text{Var}(Y_i),\\;i = 1, \\ldots, n\\). We notice that the score in (5.7) may now be written as \\[u_k(\\bm{\\beta})=\\sum_{i=1}^n(y_i-\\mu_i)x_{ik}w_ig&#39;(\\mu_i) =\\sum_{i=1}^n x_{ik}w_iz_i,\\quad{k=1,\\ldots ,p},\\] where \\[ z_i=(y_i-\\mu_i)g&#39;(\\mu_i),\\quad i = 1, \\ldots, n. \\] Therefore \\[\\begin{equation} \\bm{u}(\\bm{\\beta})=\\bm{X}^T\\bm{W}\\bm{z}. \\tag{5.9} \\end{equation}\\] One possible method to solve the \\(p\\) simultaneous equations \\({\\bm{u}}(\\hat{\\bm{\\beta}})={\\bf 0}\\) that give \\(\\hat{\\bm{\\beta}}\\) is the (multivariate) Newton-Raphson method. If \\(\\bm{\\beta}^{(m)}\\) is the current estimate of \\(\\hat{\\bm{\\beta}}\\) then the next estimate is \\[\\begin{equation} \\bm{\\beta}^{(m+1)}=\\bm{\\beta}^{(m)}-\\bm{H}(\\bm{\\beta}^{(m)})^{-1}\\bm{u}(\\bm{\\beta}^{(m)}). \\tag{5.10} \\end{equation}\\] In practice, an alternative to Newton-Raphson replaces \\(\\bm{H}(\\bm{\\theta})\\) in (5.10) with \\(E[\\bm{H}(\\bm{\\theta})]\\equiv-\\mathcal{I}(\\bm{\\beta})\\). Therefore, if \\(\\bm{\\beta}^{(m)}\\) is the current estimate of \\(\\hat{\\bm{\\beta}}\\) then the next estimate is \\[\\begin{equation} \\bm{\\beta}^{(m+1)}=\\bm{\\beta}^{(m)}+{\\cal I}(\\bm{\\beta}^{(m)})^{-1}\\bm{u}(\\bm{\\beta}^{(m)}). \\tag{5.11} \\end{equation}\\] The resulting iterative algorithm is called Fisher scoring. Notice that if we substitute (5.8) and (5.9) into (5.11) we get \\[\\begin{align*} \\bm{\\beta}^{(m+1)}&amp;=\\bm{\\beta}^{(m)}+[\\bm{X}^T\\bm{W}^{(m)}\\bm{X}]^{-1}\\bm{X}^T\\bm{W}^{(m)}\\bm{z}^{(m)}\\cr &amp;=[\\bm{X}^T\\bm{W}^{(m)}\\bm{X}]^{-1}[\\bm{X}^T\\bm{W}^{(m)}\\bm{X}\\bm{\\beta}^{(m)}+\\bm{X}^T\\bm{W}^{(m)}\\bm{z}^{(m)}]\\cr &amp;=[\\bm{X}^T\\bm{W}^{(m)}\\bm{X}]^{-1}\\bm{X}^T\\bm{W}^{(m)}[\\bm{X}\\bm{\\beta}^{(m)}+\\bm{z}^{(m)}]\\cr &amp;=[\\bm{X}^T\\bm{W}^{(m)}\\bm{X}]^{-1}\\bm{X}^T\\bm{W}^{(m)}[\\bm{\\eta}^{(m)}+\\bm{z}^{(m)}], \\end{align*}\\] where \\(\\bm{\\eta}^{(m)},\\,\\bm{W}^{(m)}\\) and \\(\\bm{z}^{(m)}\\) are all functions of \\(\\bm{\\beta}^{(m)}\\). Note that this is a weighted least squares equation, that is \\(\\bm{\\beta}^{(m+1)}\\) minimises the weighted sum of squares \\[ (\\bm{\\eta}+\\bm{z}-\\bm{X}\\bm{\\beta})^T\\bm{W}(\\bm{\\eta}+\\bm{z}-\\bm{X}\\bm{\\beta})= \\sum_{i=1}^n w_i\\left(\\eta_i+z_i-\\bm{x}_i^T\\bm{\\beta}\\right)^2 \\] as a function of \\(\\bm{\\beta}\\) where \\(w_1,\\ldots ,w_n\\) are the weights and \\(\\bm{\\eta}+\\bm{z}\\) is called the adjusted dependent variable. Therefore, the Fisher scoring algorithm proceeds as follows. Choose an initial estimate \\(\\bm{\\beta}^{(m)}\\) for \\(\\hat{\\bm{\\beta}}\\) at \\(m=0\\). Evaluate \\(\\bm{\\eta}^{(m)},\\,\\bm{W}^{(m)}\\) and \\(\\bm{z}^{(m)}\\) at \\(\\bm{\\beta}^{(m)}\\). Calculate \\[\\bm{\\beta}^{(m+1)} =[\\bm{X}^T\\bm{W}^{(m)}\\bm{X}]^{-1}\\bm{X}^T\\bm{W}^{(m)}[\\bm{\\eta}^{(m)}+\\bm{z}^{(m)}].\\] If \\(||\\bm{\\beta}^{(m+1)}-\\bm{\\beta}^{(m)} ||&gt; \\epsilon\\), for some prespecified (small) tolerance \\(\\epsilon\\) then set \\(m\\to m+1\\) and go to 2. Use \\(\\bm{\\beta}^{(m+1)}\\) as the solution for \\(\\hat{\\bm{\\beta}}\\). As this algorithm involves iteratively minimising a weighted sum of squares, it is sometimes known as iteratively (re)weighted least squares. Notes Recall that the canonical link function is \\(g(\\mu)=b^{&#39;-1}(\\mu)\\) and with this link \\(\\eta_i=g(\\mu_i)=\\theta_i\\). Then \\[ {1\\over{g&#39;(\\mu_i)}}={{\\partial\\mu_i}\\over{\\partial\\eta_i}} ={{\\partial\\mu_i}\\over{\\partial\\theta_i}}=b&#39;&#39;(\\theta_i),\\quad i = 1, \\ldots, n. \\] Therefore \\(\\text{Var}(Y_i)g&#39;(\\mu_i)=a(\\phi_i)\\) which does not depend on \\(\\bm{\\beta}\\), and hence \\[ {\\partial\\over{\\partial\\beta_j}}\\left[{{x_{ik}}\\over{\\text{Var}(Y_i)g&#39;(\\mu_i)}}\\right]=0 \\] for all \\(j=1,\\ldots ,p\\). It follows that \\(\\bm{H}(\\bm{\\theta})=-\\mathcal{I}(\\bm{\\beta})\\) and, for the canonical link, Newton-Raphson and Fisher scoring are equivalent. The linear model is a generalised linear model with identity link, \\(\\eta_i=g(\\mu_i)=\\mu_i\\) and \\(\\text{Var}(Y_i)=\\sigma^2\\) for all \\(i = 1, \\ldots, n\\). Therefore \\(w_i=[\\text{Var}(Y_i)g&#39;(\\mu_i)^2]^{-1}=\\sigma^{-2}\\) and \\(z_i=(y_i-\\mu_i)g&#39;(\\mu_i)=y_i-\\eta_i\\) for \\(i = 1, \\ldots, n\\). Hence \\(\\bm{z}+\\bm{\\eta}=\\bm{y}\\) and \\(\\bm{W}=\\sigma^{-2}\\bm{I}\\), neither of which depend on \\(\\bm{\\beta}\\). So the Fisher scoring algorithm converges in a single iteration to the usual least squares estimate. Estimation of an unknown scale parameter \\(\\sigma^2\\) is discussed later. A common (to all \\(i\\)) \\(\\sigma^2\\) has no effect on \\(\\hat{\\bm{\\beta}}\\). 5.6 Confidence intervals Recall from Section 2.6 that the maximum likelihood estimator \\(\\hat{\\bm{\\beta}}\\) is asymptotically normally distributed with mean \\(\\bm{\\beta}\\) (it is unbiased) and variance covariance matrix \\({\\cal I}(\\bm{\\beta})^{-1}\\). For ‘large enough \\(n\\)’ we treat this distribution as an approximation. Therefore, standard errors (estimated standard deviations) are given by \\[ s.e.(\\hat{\\beta}_i)=[{\\cal I}(\\hat{\\bm{\\beta}})^{-1}]_{ii}^{{1\\over 2}} =[(\\bm{X}^T\\hat{\\bm{W}}\\bm{X})^{-1}]_{ii}^{{1\\over 2}} \\qquad i=1,\\ldots ,p. \\] where the diagonal matrix \\(\\hat{\\bm{W}}={\\rm diag}(\\hat{\\bm{w}})\\) is evaluated at \\(\\hat{\\bm{\\beta}}\\), that is \\(\\hat{w}_i=(\\hat{\\text{Var}}(Y_i)g&#39;(\\hat{\\mu}_i)^2)^{-1}\\) where \\(\\hat{\\mu}_i\\) and \\(\\hat{\\text{Var}}(Y_i)\\) are evaluated at \\(\\hat{\\bm{\\beta}}\\) for \\(i = 1, \\ldots, n\\). Furthermore, if \\(\\text{Var}(Y_i)\\) depends on an unknown scale parameter, then this too must be estimated in the standard error. The asymptotic distribution of the maximum likelihood estimator can be used to provide approximate large sample confidence intervals. For given \\(\\alpha\\) we can find \\(z_{1-\\frac{\\alpha}{2}}\\) such that \\[ P\\left(-z_{1-\\frac{\\alpha}{2}}\\le {{\\hat{\\beta}_i-\\beta_i}\\over{[\\mathcal{I}(\\bm{\\beta})^{-1}]_{ii}^{1\\over 2}}}\\le z_{1-\\frac{\\alpha}{2}}\\right) =1-\\alpha. \\] Therefore \\[ P\\left(\\hat{\\beta}_i-z_{1-\\frac{\\alpha}{2}}[\\mathcal{I}(\\bm{\\beta})^{-1}]_{ii}^{1\\over 2}\\le\\beta_i \\le\\hat{\\beta}_i+z_{1-\\frac{\\alpha}{2}}[\\mathcal{I}(\\bm{\\beta})^{-1}]_{ii}^{1\\over 2} \\right) =1-\\alpha. \\] The endpoints of this interval cannot be evaluated because they also depend on the unknown parameter vector \\(\\bm{\\beta}\\). However, if we replace \\({\\cal I}(\\bm{\\beta})\\) by its MLE \\({\\cal I}(\\hat{\\bm{\\beta}})\\) we obtain the approximate large sample 100\\((1-\\alpha)\\)% confidence interval \\[ [\\hat{\\beta}_i-s.e.(\\hat{\\beta}_i)z_{1-\\frac{\\alpha}{2}}\\,,\\, \\hat{\\beta}_i+s.e.(\\hat{\\beta}_i)z_{1-\\frac{\\alpha}{2}}]. \\] For \\(\\alpha=0.10,0.05,0.01\\), \\(z_{1-\\frac{\\alpha}{2}}=1.64,1.96,2.58\\), respectively. 5.7 Comparing generalised linear models 5.7.1 The likelihood ratio test If we have a set of competing generalised linear models which might explain the dependence of the response on the explanatory variables, we will want to determine which of the models is most appropriate. Recall that we have three main requirements of a statistical model; plausibility, parsimony and goodness of fit, of which parsimony and goodness of fit are statistical issues. As with linear models, we proceed by comparing models pairwise using a likelihood ratio test. This kind of comparison is restricted to situations where one of the models, \\(H_0\\), is nested in the other, \\(H_1\\). Then the asymptotic distribution of the log likelihood ratio statistic under \\(H_0\\) is a chi-squared distribution with known degrees of freedom. For generalised linear models, ‘nested’ means that \\(H_0\\) and \\(H_1\\) are based on the same exponential family distribution, and have the same link function, but the explanatory variables present in \\(H_0\\) are a subset of those present in \\(H_1\\). We will assume that model \\(H_1\\) contains \\(p\\) linear parameters and model \\(H_0\\) a subset of \\(q&lt;p\\) of these. Without loss of generality, we can think of \\(H_1\\) as the model \\[ \\eta_i=\\sum_{j=1}^p x_{ij} \\beta_j \\qquad i = 1, \\ldots, n \\] and \\(H_0\\) is the same model with Then model \\(H_0\\) is a special case of model \\(H_1\\), where certain coefficients are set equal to zero, and therefore \\(\\Theta^{(0)}\\), the set of values of the canonical parameter \\(\\bm{\\theta}\\) allowed by \\(H_0\\), is a subset of \\(\\Theta^{(1)}\\), the set of values allowed by \\(H_1\\). Now, the log likelihood ratio statistic for a test of \\(H_0\\) against \\(H_1\\) is \\[\\begin{align} L_{01}&amp;\\equiv 2\\log \\left({{\\max_{\\bm{\\theta}\\in \\Theta^{(1)}} L(\\bm{\\theta})}\\over {\\max_{\\bm{\\theta}\\in \\Theta^{(0)}}L(\\bm{\\theta})}}\\right)\\cr &amp;=2\\log L(\\hat{\\bm{\\theta}}^{(1)})-2\\log L(\\hat{\\bm{\\theta}}^{(0)}), \\tag{5.12} \\end{align}\\] where \\(\\hat{\\bm{\\theta}}^{(1)}\\) and \\(\\hat{\\bm{\\theta}}^{(0)}\\) follow from \\(b&#39;(\\hat{\\theta}_i)=\\hat{\\mu}_i\\), \\(g(\\hat{\\mu}_i)=\\hat{\\eta_i}\\), \\(i = 1, \\ldots, n\\) where \\(\\hat{\\bm{\\eta}}\\) for each model is the linear predictor evaluated at the corresponding maximum likelihood estimate for \\(\\bm{\\beta}\\). Here, we assume that \\(a(\\phi_i),\\;i = 1, \\ldots, n\\) are known; unknown \\(a(\\phi)\\) is discussed in Section 5.9. Recall that we reject \\(H_0\\) in favour of \\(H_1\\) when \\(L_{01}\\) is ‘too large’ (the observed data are much more probable under \\(H_1\\) than \\(H_0\\)). To determine a threshold value \\(k\\) for \\(L_{01}\\), beyond which we reject \\(H_0\\), we set the size of the test \\(\\alpha\\) and use the result of Section 3.3 that, because \\(H_0\\) is nested in \\(H_1\\), \\(L_{01}\\) has an asymptotic chi-squared distribution with \\(p-q\\) degrees of freedom. For example, if \\(\\alpha=0.05\\), we reject \\(H_0\\) in favour of \\(H_1\\) when \\(L_{01}\\) is greater than the 95% point of the \\(\\chi^2_{p-q}\\) distribution. Note that setting up our model selection procedure in this way is consistent with our desire for parsimony. The simpler model is \\(H_0\\), and we do not reject \\(H_0\\) in favour of the more complex model \\(H_1\\) unless the data provide convincing evidence for \\(H_1\\) over \\(H_0\\), that is unless \\(H_1\\) fits the data significantly better. 5.8 Scaled deviance and the saturated model Consider a model where \\(\\bm{\\beta}\\) is \\(n\\)-dimensional, and therefore \\(\\bm{\\eta}=\\bm{X}\\bm{\\beta}\\). Assuming that \\(\\bm{X}\\) is invertible, then this model places no constraints on the linear predictor \\(\\bm{\\eta}=(\\eta_1,\\ldots ,\\eta_n)\\). It can take any value in \\(\\mathbb{R}^n\\). Correspondingly the means \\(\\bm{\\mu}\\) and the canonical parameters \\(\\bm{\\theta}\\) are unconstrained. The model is of dimension \\(n\\) and can be parameterised equivalently using \\(\\bm{\\beta}\\), \\(\\bm{\\eta}\\), \\(\\bm{\\mu}\\) or \\(\\bm{\\theta}\\). Such a model is called the saturated model. As the canonical parameters \\(\\bm{\\theta}\\) are unconstrained, we can calculate their maximum likelihood estimates \\(\\hat{\\bm{\\theta}}\\) directly from their likelihood (5.2) (without first having to calculate \\(\\hat{\\bm{\\beta}}\\)) \\[\\begin{equation} \\ell(\\bm{\\theta})=\\sum_{i=1}^n{{y_i\\theta_i-b(\\theta_i)} \\over{a(\\phi_i)}}+\\sum_{i=1}^nc(y_i,\\phi_i). \\tag{5.13} \\end{equation}\\] We obtain \\(\\hat{\\bm{\\theta}}\\) by first differentiating with respect to \\(\\theta_1,\\ldots ,\\theta_n\\) to give \\[ {\\partial\\over{\\partial\\theta_k}}\\ell(\\bm{\\theta})={{y_k-b&#39;(\\theta_k)} \\over{a(\\phi_k)}}\\qquad k=1,\\ldots ,n. \\] Therefore \\(b&#39;(\\hat{\\theta}_k)=y_k,\\;k=1,\\ldots ,n\\), and it follows immediately that \\(\\hat{\\mu}_k=y_k,\\;k=1,\\ldots ,n\\). Hence the saturated model fits the data perfectly, as the fitted values \\(\\hat{\\mu}_k\\) and observed values \\(y_k\\) are the same for every observation \\(k=1,\\ldots ,n\\). The saturated model is rarely of any scientific interest in its own right. It is highly parameterised, having as many parameters as there are observations. This goes against our desire for parsimony in a model. However, every other model is necessarily nested in the saturated model, and a test comparing a model \\(H_0\\) against the saturated model \\(H_S\\) can be interpreted as a goodness of fit test. If the saturated model, which fits the observed data perfectly, does not provide a significantly better fit than model \\(H_0\\), we can conclude that \\(H_0\\) is an acceptable fit to the data. The log likelihood ratio statistic for a test of \\(H_0\\) against \\(H_S\\) is, from (5.12) \\[ L_{0s}=2\\log L(\\hat{\\bm{\\theta}}^{(s)})-2\\log L(\\hat{\\bm{\\theta}}^{(0)}), \\] where \\(\\hat{\\bm{\\theta}}^{(s)}\\) follows from \\(b&#39;(\\hat{\\bm{\\theta}})=\\hat{\\bm{\\mu}}=\\bm{y}\\) and \\(\\hat{\\bm{\\theta}}^{(0)}\\) is a function of the corresponding maximum likelihood estimate for \\(\\bm{\\beta}=(\\beta_1,\\ldots ,\\beta_q)^T\\). Under \\(H_0\\), \\(L_{0s}\\) has an asymptotic chi-squared distribution with \\(n-q\\) degrees of freedom. Therefore, if \\(L_{0s}\\) is ‘too large’ (for example, larger than the 95% point of the \\(\\chi^2_{n-q}\\) distribution) then we reject \\(H_0\\) as a plausible model for the data, as it does not fit the data adequately. The degrees of freedom of model \\(H_0\\) is defined to be the degrees of freedom for this test, \\(n-q\\), the number of observations minus the number of linear parameters of \\(H_0\\). We call \\(L_{0s}\\) the scaled deviance (R calls it the residual deviance) of model \\(H_0\\). From (5.12) and (5.13) we can write the deviance of model \\(H_0\\) as \\[\\begin{equation} L_{0s}=2\\sum_{i=1}^n{{y_i[\\hat{\\theta}^{(s)}_i-\\hat{\\theta}^{(0)}_i] -[b(\\hat{\\theta}^{(s)}_i)-b(\\hat{\\theta}^{(0)}_i)]} \\over{a(\\phi_i)}}, \\tag{5.14} \\end{equation}\\] which can be calculated using the observed data, provided that \\(a(\\phi_i),\\; i = 1, \\ldots, n\\) is known. Notes The log likelihood ratio statistic (5.12) for testing \\(H_0\\) against a non-saturated alternative \\(H_1\\) can be written as \\[\\begin{align} L_{01}&amp;=2\\log L(\\hat{\\bm{\\theta}}^{(1)})-2\\log L(\\hat{\\bm{\\theta}}^{(0)})\\cr &amp;=[2\\log L(\\hat{\\bm{\\theta}}^{(s)})-2\\log L(\\hat{\\bm{\\theta}}^{(0)})] -[2\\log L(\\hat{\\bm{\\theta}}^{(s)})-2\\log L(\\hat{\\bm{\\theta}}^{(1)})]\\cr &amp;=L_{0s}-L_{1s}. \\tag{5.15} \\end{align}\\] Therefore the log likelihood ratio statistic for comparing two nested models is the difference of their deviances. Furthermore, as \\(p-q=(n-q)-(n-p)\\), the degrees of freedom for the test is the difference in degrees of freedom of the two models. The asymptotic theory used to derive the distribution of the log likelihood ratio statistic under \\(H_0\\) does not really apply to the goodness of fit test (comparison with the saturated model). However, for binomial or Poisson data, we can proceed as long as the relevant binomial or Poisson distributions are likely to be reasonably approximated by normal distributions (i.e. for binomials with large denominators or Poissons with large means). However, for Bernoulli data, we cannot use the scaled deviance as a goodness of fit statistic in this way. An alternative goodness of fit statistic for a model \\(H_0\\) is Pearson’s \\(X^2\\) given by \\[\\begin{equation} X^2=\\sum_{i=1}^n {{(y_i-\\hat{\\mu}_i^{(0)})^2}\\over{\\hat{\\text{Var}}(Y_i)}}. \\tag{5.16} \\end{equation}\\] \\(X^2\\) is small when the squared differences between observed and fitted values (scaled by variance) is small. Hence, large values of \\(X^2\\) correspond to poor fitting models. In fact, \\(X^2\\) and \\(L_{0s}\\) are asymptotically equivalent and under \\(H_0\\), \\(X^2\\), like \\(L_{0s}\\), has an asymptotic chi-squared distribution with \\(n-q\\) degrees of freedom. However, the asymptotics associated with \\(X^2\\) are often more reliable for small samples, so if there is a discrepancy between \\(X^2\\) and \\(L_{0s}\\), it is usually safer to base a test of goodness of fit on \\(X^2\\). Although the deviance for a model is expressed in (5.14) in terms of the maximum likelihood estimates of the canonical parameters, it is more usual to express it in terms of the maximum likelihood estimates \\(\\hat{\\mu}_i,\\; i = 1, \\ldots, n\\) of the mean parameters. For the saturated model, these are just the observed values \\(y_i,\\;i = 1, \\ldots, n\\), and for the model of interest, \\(H_0\\), we call them the fitted values. Hence, for a particular generalised linear model, the scaled deviance function describes how discrepancies between the observed and fitted values are penalised. Example 5.5 (Poisson) Suppose \\(Y_i\\sim \\text{Poisson}(\\lambda_i),\\;i = 1, \\ldots, n\\). Recall from Section 5.2 that \\(\\theta=\\log\\lambda\\), \\(b(\\theta)=\\exp\\theta\\), \\(\\mu=b&#39;(\\theta)=\\exp\\theta\\) and \\(\\text{Var}(Y)=a(\\phi)V(\\mu)=1\\cdot\\mu\\). Therefore, by (5.14) and (5.16) \\[\\begin{align*} L_{0s}&amp;=2\\sum_{i=1}^n y_i[\\log\\hat{\\mu}^{(s)}_i-\\log\\hat{\\mu}^{(0)}_i] -[\\hat{\\mu}^{(s)}_i-\\hat{\\mu}^{(0)}_i]\\cr &amp;=2\\sum_{i=1}^n y_i\\log \\left({{y_i}\\over{\\hat{\\mu}^{(0)}_i}}\\right) -y_i+\\hat{\\mu}^{(0)}_i \\end{align*}\\] and \\[ X^2=\\sum_{i=1}^n {{(y_i-\\hat{\\mu}_i^{(0)})^2}\\over{\\hat{\\mu}_i^{(0)}}}. \\] Example 5.6 (Binomial) Suppose \\(n_iY_i\\sim\\) Binomial\\((n_i,p_i),\\;i = 1, \\ldots, n\\). Recall from Section 5.2 that \\(\\theta=\\log{p\\over{1-p}}\\), \\(b(\\theta)=\\log(1+\\exp\\theta)\\), \\(\\mu=b&#39;(\\theta)={{\\exp\\theta}\\over{1+\\exp\\theta}}\\) and \\(\\text{Var}(Y)=a(\\phi)V(\\mu)={1\\over n}\\cdot\\mu(1-\\mu)\\). Therefore, by (5.14) and (5.16) \\[\\begin{align*} L_{0s}&amp;=2\\sum_{i=1}^n n_iy_i\\left[\\log{\\hat{\\mu}^{(s)}_i\\over{1-\\hat{\\mu}^{(s)}_i}} -\\log{\\hat{\\mu}^{(0)}_i\\over{1-\\hat{\\mu}^{(0)}_i}}\\right] + 2\\sum_{i=1}^n n_i \\left[\\log(1-\\hat{\\mu}^{(s)}_i)-\\log(1-\\hat{\\mu}^{(0)}_i) \\right]\\cr &amp;=2\\sum_{i=1}^n \\left[ n_iy_i\\log \\left({{y_i}\\over{\\hat{\\mu}^{(0)}_i}}\\right) +n_i(1-y_i) \\log \\left({{1-y_i}\\over{1-\\hat{\\mu}^{(0)}_i}}\\right) \\right] \\end{align*}\\] and \\[ X^2=\\sum_{i=1}^n {{n_i(y_i-\\hat{\\mu}_i^{(0)})^2}\\over{\\hat{\\mu}_i^{(0)} (1-\\hat{\\mu}^{(0)}_i)}}. \\] Bernoulli data are binomial with \\(n_i=1,\\;i = 1, \\ldots, n\\). 5.9 Models with unknown \\(a(\\phi)\\) The theory of Section 5.7 has assumed that \\(a(\\phi)\\) is known. This is the case for both the Poisson distribution (\\(a(\\phi)=1\\)) and the binomial distribution (\\(a(\\phi)=1/n\\)). Neither the scaled deviance (5.14) nor Pearson \\(X^2\\) statistic (5.16) can be evaluated unless \\(a(\\phi)\\) is known. Therefore, when \\(a(\\phi)\\) is not known, we cannot use the scaled deviance as a measure of goodness of fit, or to compare models using (5.15). For such models, there is no equivalent goodness of fit test, but we can develop a test for comparing nested models. Here we assume that \\(a(\\phi_i)=\\sigma^2/m_i,\\;i = 1, \\ldots, n\\) where \\(\\sigma^2\\) is a common unknown scale parameter and \\(m_1,\\ldots ,m_n\\) are known weights. (A linear model takes this form, as \\(\\text{Var}(Y_i)=\\sigma^2,\\;i = 1, \\ldots, n\\), so \\(m_i=1,\\;i = 1, \\ldots, n\\).) Under this assumption \\[\\begin{equation} L_{0s}={2\\over\\sigma^2}\\sum_{i=1}^nm_iy_i[\\hat{\\theta}^{(s)}_i-\\hat{\\theta}^{(0)}_i] -m_i[b(\\hat{\\theta}^{(s)}_i)-b(\\hat{\\theta}^{(0)}_i)] ={1\\over\\sigma^2}D_{0s}, \\tag{5.17} \\end{equation}\\] where \\(D_{0s}\\) is defined to be twice the sum above, which can be calculated using the observed data. We call \\(D_{0s}\\) the deviance of the model. In order to test nested models \\(H_0\\) and \\(H_1\\) as set up in Section 5.7.1, we calculate the test statistic \\[\\begin{align} &amp;F={{L_{01}/(p-q)}\\over{L_{1s}/(n-p)}}={{(L_{0s}-L_{1s})/(p-q)}\\over{L_{1s}/(n-p)}} \\hbox{\\hskip 1.2in}\\cr &amp;\\;={{\\left({1\\over\\sigma^2}D_{0s}-{1\\over\\sigma^2}D_{1s}\\right)/(p-q)} \\over{{1\\over\\sigma^2}D_{1s}/(n-p)}} ={{(D_{0s}-D_{1s})/(p-q)}\\over{D_{1s}/(n-p)}}. \\tag{5.18} \\end{align}\\] This statistic does not depend on the unknown scale parameter \\(\\sigma^2\\), so can be calculated using the observed data. Asymptotically, if \\(H_0\\) is true, we know that \\(L_{01} \\sim \\chi^2_{p-q}\\) and \\(L_{1s} \\sim \\chi^2_{n-p}\\). Furthermore, \\(L_{01}\\) and \\(L_{1s}\\) are independent (not proved here) so \\(F\\) has an asymptotic \\(F_{p-q, n-p}\\) distribution. Hence, we compare nested generalised linear models by calculating \\(F\\) and rejecting \\(H_0\\) in favour of \\(H_1\\) if \\(F\\) is too large (for example, greater than the 95% point of the relevant F distribution). The dependence of the maximum likelihood equations \\(\\bm{u}(\\hat{\\bm{\\beta}})={\\bf 0}\\) on \\(\\sigma^2\\) (where \\(\\bm{u}\\) is given by (5.7)) can be eliminated by multiplying through by \\(\\sigma^2\\). However, inference based on the maximum likelihood estimates, as described in Section 5.6, does require knowledge of \\(\\sigma^2\\). This is because asymptotically \\(\\text{Var}(\\hat{\\bm{\\beta}})\\) is the inverse of the Fisher information matrix \\({\\cal I}(\\bm{\\beta})=\\bm{X}^T\\bm{W}\\bm{X}\\), and this depends on \\(w_i={1\\over{\\text{Var}(Y_i)g&#39;(\\mu_i)^2}},\\) where \\(\\text{Var}(Y_i)=a(\\phi_i)b&#39;&#39;(\\theta_i)=\\sigma^2 b&#39;&#39;(\\theta_i)/m_i\\) here. Therefore, to calculate standard errors and confidence intervals, we need to supply an estimate \\(\\hat \\sigma^2\\) of \\(\\sigma^2\\). Generally, we do not use the maximum likelihood estimate. Instead, we notice that, from (5.17), \\(L_{0s}=D_{0s}/\\sigma^2\\), and we know that asymptotically, if model \\(H_0\\) is an adequate fit, \\(L_{0s}\\) has a \\(\\chi^2_{n-q}\\) distribution. Hence \\[ E(L_{0s})=E\\left({1\\over{\\sigma^2}}D_{0s}\\right)=n-q\\quad\\Rightarrow\\quad E\\left({1\\over{n-q}}D_{0s}\\right)=\\sigma^2. \\] Therefore the deviance of a model divided by its degrees of freedom is an asymptotically unbiased estimator of the scale parameter \\(\\sigma^2\\). Hence \\(\\hat\\sigma^2=D_{0s}/(n-q)\\). An alternative estimator of \\(\\sigma^2\\) is based on the Pearson \\(X^2\\) statistic. As \\(\\text{Var}(Y)=a(\\phi)V(\\mu)=\\sigma^2 V(\\mu)/m\\) here, then from (5.16) \\[\\begin{equation} X^2={1\\over\\sigma^2} \\sum_{i=1}^n {{m_i(y_i-\\hat{\\mu}_i^{(0)})^2}\\over{{V}(\\hat{\\mu}_i^{(0)})}}. \\tag{5.19} \\end{equation}\\] Again, if \\(H_0\\) is an adequate fit, \\(X^2\\) has an chi-squared distribution with \\(n-q\\) degrees of freedom, so \\[ \\hat \\sigma^2={1\\over{n-q}} \\sum_{i=1}^n {{m_i(y_i-\\hat{\\mu}_i^{(0)})^2}\\over{{V}(\\hat{\\mu}_i^{(0)})}} \\] is an alternative unbiased estimator of \\(\\sigma^2\\). This estimator tends to be more reliable in small samples. Example 5.7 (Normal) Suppose \\(Y_i\\sim N(\\mu_i,\\sigma^2),\\;i = 1, \\ldots, n\\). Recall from Section 5.2 that \\(\\theta=\\mu\\), \\(b(\\theta)=\\theta^2/2\\), \\(\\mu=b&#39;(\\theta)=\\theta\\) and \\(\\text{Var}(Y)=a(\\phi)V(\\mu)={\\sigma^2}\\cdot 1\\), so \\(m_i=1,\\;i = 1, \\ldots, n\\). Therefore, by (5.17), \\[\\begin{equation} D_{0s}=2\\sum_{i=1}^n y_i[\\hat{\\mu}^{(s)}_i-\\hat{\\mu}^{(0)}_i] -[\\frac{1}{2}{{\\hat{\\mu}}^{(s)^2}_i}-\\frac{1}{2}{{\\hat{\\mu}}^{(0)^2}_i}] =\\sum_{i=1}^n [y_i-\\hat{\\mu}^{(0)}_i]^2, \\tag{5.20} \\end{equation}\\] which is just the residual sum of squares for model \\(H_0\\). Therefore, we estimate \\(\\sigma^2\\) for a normal GLM by its residual sum of squares for the model divided by its degrees of freedom. From (5.19), the estimate for \\(\\sigma^2\\) based on \\(X^2\\) is identical. 5.10 Residuals Recall that for linear models, we define the residuals to be the differences between the observed and fitted values \\(y_i-\\hat{\\mu}^{(0)}_i,\\;i = 1, \\ldots, n\\). From (5.20) we notice that both the scaled deviance and Pearson \\(X^2\\) statistic for a normal GLM are the sum of the squared residuals divided by \\(\\sigma^2\\). We can generalise this to define residuals for other generalised linear models in a natural way. For any GLM we define the Pearson residuals to be \\[ r^P_i={{y_i-\\hat{\\mu}_i^{(0)}}\\over{\\hat{\\text{Var}}(Y_i)^{1\\over 2}}}\\qquad i = 1, \\ldots, n. \\] Then, from (5.16), \\(X^2\\) is the sum of the squared Pearson residuals. For any GLM we define the deviance residuals to be \\[r^D_i=\\text{sign}(y_i-\\hat{\\mu}_i^{(0)}) \\left[ 2 {{y_i[\\hat{\\theta}^{(s)}_i-\\hat{\\theta}^{(0)}_i] -[b(\\hat{\\theta}^{(s)}_i)-b(\\hat{\\theta}^{(0)}_i)]} \\over{a(\\phi_i)}}\\right]^{1\\over 2}, \\quad i = 1, \\ldots, n,\\] where \\(\\text{sign}(x)=1\\) if \\(x&gt;0\\) and \\(-1\\) if \\(x&lt;0\\). Then, from (5.14), the scaled deviance, \\(L_{0s}\\), is the sum of the squared deviance residuals. When \\(a(\\phi)=\\sigma^2/m\\) and \\(\\sigma^2\\) is unknown, as in Section 5.9, the residuals are based on (5.17) and (5.19), and the expressions above need to be multiplied through by \\(\\sigma^2\\) to eliminate dependence on the unknown scale parameter. Therefore, for a normal GLM the Pearson and deviance residuals are both equal to the usual residuals, \\(y_i-\\hat{\\mu}^{(0)}_i,\\;i = 1, \\ldots, n\\). Residual plots are most commonly of use in normal linear models, where they provide an essential check of the model assumptions. This kind of check is less important for a model without an unknown scale parameter as the scaled deviance provides a useful overall assessment of fit which takes into account most aspects of the model. However, when data have been collected in serial order, a plot of the deviance or Pearson residuals against the order may again be used as a check for potential serial correlation. Otherwise, residual plots are most useful when a model fails to fit (scaled deviance is too high). Then, examining the residuals may give an indication of the reason(s) for lack of fit. For example, there may be a small number of outlying observations. A plot of deviance or Pearson residuals against the linear predictor should produce something that looks like a random scatter. If not, then this may be due to incorrect link function, wrong scale for an explanatory variable, or perhaps a missing polynomial term in an explanatory variable. "],
["chap-categorical.html", "Chapter 6 Models for categorical data 6.1 Contingency tables 6.2 Log-linear models 6.3 Multinomial sampling 6.4 Product multinomial sampling 6.5 Interpreting log-linear models for two-way tables 6.6 Interpreting log-linear models for multiway tables 6.7 Simpson’s paradox", " Chapter 6 Models for categorical data 6.1 Contingency tables A particularly important application of generalised linear models is the analysis of categorical data. Here, the data are observations of one or more categorical variables on each of a number of units (often individuals). Therefore, each of the units are cross-classified by the categorical variables. For example, the job dataset gives the job satisfaction and income band of 901 individuals from the 1984 General Social Survey, which is summarised in Table . A cross-classification table like this is called a contingency table. This is a two-way table, as there are two classifying variables. It might also be describe as a \\(4\\times 4\\) contingency table (as each of the classifying variables takes one of four possible levels). Each position in a contingency table is called a cell and the number of individuals in a particular cell is the cell count. Partial classifications derived from the table are called margins. For a two-way table these are often displayed in the margins of the table, as in Table . These are one-way margins as they represent the classification of items by a single variable; income group and job satisfaction respectively. The lymphoma dataset gives information about 30 patients, classified by cell type of lymphoma, sex, and response to treatment, as shown in Table . This is an example of a three-way contingency table. It is a \\(2\\times 2\\times 2\\) or \\(2^3\\) table. For multiway tables, higher order margins may be calculated. For example, for lymphoma, the two-way Cell type/Sex margin is shown in Table . 6.2 Log-linear models We can model contingency table data using generalised linear models. To do this, we assume that the cell counts are observations of independent Poisson random variables. This is intuitively sensible as the cell counts are non-negative integers (the sample space for the Poisson distribution). Therefore, if the table has \\(n\\) cells, which we label \\(1,\\ldots ,n\\), then the observed cell counts \\(y_1,\\ldots ,y_n\\) are assumed to be observations of independent Poisson random variables \\(Y_1,\\ldots ,Y_n\\). We denote the means of these Poisson random variables by \\(\\mu_1,\\ldots ,\\mu_n\\). The canonical link function for the Poisson distribution is the log function, and we assume this link function throughout. A generalised linear model for Poisson data using the log link function is called a log-linear model, as we have already seen in Section 5.4.3. The explanatory variables in a log-linear model for contingency table data are the cross-classifying variables. As these variables are categorical, they are factors. As usual with factors, we can include interactions in the model as well as just main effects. Such a model will describe how the expected count in each cell depends on the classifying variables, and any interactions between them. Interpretation of these models will be discussed further in Section 6.5. Table shows the original data structure of the job dataset. It provides exactly the same data as the contingency table in Table , but in a different format, sometimes called long format. A log-linear model is just a Poisson GLM, where the response variable is Count, and Income and Satisfaction are explanatory variables. Table shows the lymphoma dataset in long format. Again, a log-linear model for the contingency table (Table ) is just a Poisson GLM for this data, where in this case the response variable is Cell, and Sex and Remis are explanatory variables. 6.3 Multinomial sampling Although the assumption of Poisson distributed observations is convenient for the purposes of modelling, it might not be a realistic assumption, because of the way in which the data have been collected. Frequently, when contingency table data are obtained, the total number of observations (the grand total, the sum of all the cell counts) is fixed in advance. In this case, no individual cell count can exceed the prespecified fixed total, so the assumption of Poisson sampling is invalid as the sample space is bounded. Furthermore, with a fixed total, the observations can no longer be observations of independent random variables. For example, consider the lymphoma contingency table from Table . It may be that these data were collected over a fixed period of time, and that in that time there happened to be 30 patients. In this case, the Poisson assumption is perfectly valid. However, it may have been decided at the outset to collect data on 30 patients, in which case the grand total is fixed at 30, and the Poisson assumption is not valid. When the grand total is fixed, a more appropriate distribution for the cell counts is the multinomial distribution. The multinomial distribution is the distribution of cell counts arising when a prespecified total of \\(N\\) items are each independently assigned to one of \\(n\\) cells, where the probability of being classified into cell \\(i\\) is \\(p_i\\), \\({i=1,\\ldots ,n}\\), so \\(\\sum_{i=1}^n p_i=1.\\) The probability function for the multinomial distribution is \\[\\begin{align} f_{\\bm{Y}}(\\bm{y};{\\bf p}) &amp;= P(Y_1=y_1,\\ldots ,Y_n=y_n) \\cr &amp;= \\begin{cases} N!\\prod_{i=1}^n \\frac{p_i^{y_i}}{y_i!} &amp; \\text{if $\\sum_{i=1}^n y_i=N$} \\\\ 0 &amp; \\text{otherwise.} \\end{cases} \\tag{6.1} \\end{align}\\] The binomial is the special case of the multinomial with two cells (\\(n=2\\)). We can still use a log-linear model for contingency table data when the data have been obtained by multinomial sampling. We model \\(\\log\\mu_i=\\log(Np_i)\\), \\({i=1,\\ldots ,n}\\) as a linear function of explanatory variables. However, such a model must preserve \\(\\sum_{i=1}^n \\mu_i=N\\), the grand total which is fixed in advance. From (6.1), the log-likelihood for a multinomial log-linear model is \\[ \\ell(\\bm{\\mu})= \\sum_{i=1}^n y_i\\log\\mu_i -N\\log N -\\sum_{i=1}^n \\log y_i!+\\log N!. \\] Therefore, the maximum likelihood estimates \\(\\hat{\\bm{\\mu}}\\) maximise \\(\\sum_{i=1}^n y_i\\log\\mu_i\\) subject to the constraints \\(\\sum_{i=1}^n \\mu_i=N=\\sum_{i=1}^n y_i\\) (multinomial sampling) and \\(\\log\\bm{\\mu}=\\bm{X}\\bm{\\beta}\\) (imposed by the model). For a Poisson log-linear model, \\[ L(\\bm{\\mu})=\\prod_{i=1}^n {{e^{-\\mu_i} \\mu_i^{y_i}}\\over{y_i!}}. \\] Therefore, \\[\\begin{align} \\ell(\\bm{\\mu})&amp;=-\\sum_{i=1}^n \\mu_i +\\sum_{i=1}^ny_i\\log\\mu_i-\\sum_{i=1}^n \\log y_i! \\tag{6.2} \\\\ &amp;=-\\sum_{i=1}^n \\exp(\\log\\mu_i) +\\sum_{i=1}^ny_i\\log\\mu_i-\\sum_{i=1}^n \\log y_i!. \\end{align}\\] Now any Poisson log-linear model with an intercept can be expressed as \\[ \\log\\mu_i=\\alpha + \\text{other terms depending on $i$},\\qquad {i=1,\\ldots ,n} \\] so that \\[\\begin{align} &amp;\\qquad{\\partial\\over{\\partial\\alpha}} \\ell(\\bm{\\mu})=-\\sum_{i=1}^n \\exp(\\log\\mu_i) +\\sum_{i=1}^ny_i. \\\\ &amp;\\Rightarrow\\qquad \\sum_{i=1}^n \\hat{\\mu}_i=\\sum_{i=1}^n y_i. \\tag{6.3} \\end{align}\\] From (6.2), we notice that, at \\(\\alpha=\\hat{\\alpha}\\) the log-likelihood takes the form \\[ \\ell(\\bm{\\mu})=-\\sum_{i=1}^n y_i +\\sum_{i=1}^ny_i\\log{\\mu}_i-\\sum_{i=1}^n \\log y_i!. \\] Hence, when we maximise the log-likelihood, for a Poisson log-linear model with intercept, with respect to the other log-linear parameters, we are maximising \\(\\sum_{i=1}^ny_i\\log{\\mu}_i\\) subject to the constraints \\(\\sum_{i=1}^n \\mu_i=\\sum_{i=1}^n y_i\\) from (6.3) and \\(\\log\\bm{\\mu}=\\bm{X}\\bm{\\beta}\\) (imposed by the model). Therefore, the maximum likelihood estimates for multinomial log-linear parameters are identical to those for Poisson log-linear parameters. Furthermore, the maximised log-likelihoods for both Poisson and multinomial models take the form \\(\\sum_{i=1}^ny_i\\log\\hat{\\mu}_i\\) as functions of the log-linear parameter estimates. Therefore any inferences based on maximised log-likelihoods (such as likelihood ratio tests) will be the same. Therefore, we can analyse contingency table data using Poisson log-linear models, even when the data has been obtained by multinomial sampling. All that is required is that we ensure that the Poisson model contains an intercept term. 6.4 Product multinomial sampling Sometimes margins other than just the grand total may be prespecified. For example, consider the lymphoma contingency table in Table . It may have been decided at the outset to collect data on 18 male patients and 12 female patients. Alternatively, perhaps the distribution of both the Sex and Cell type of the patients was fixed in advance as in Table . In cases where a margin is fixed by design, the data consist of a number of fixed total subgroups, defined by the fixed margin. Neither Poisson nor multinomial sampling assumptions are valid. The appropriate distribution combines a separate, independent multinomial for each subgroup. For example, if just the Sex margin is fixed as above, then the appropriate distribution for modelling the data is two independent multinomials, one for males with \\(N=18\\) and one for females with \\(N=12\\). Each of these multinomials has four cells, representing the cross-classification of the relevant patients by Cell Type and Remission. Alternatively, if it is the Cell type/Sex margin which has been fixed, then the appropriate distribution is four independent two-cell multinomials (binomials) representing the remission classification for each of the four fixed-total patient subgroups. When the data are modelled using independent multinomials, then the joint distribution of the cell counts \\(Y_1, \\ldots, Y_n\\) is the product of terms of the same form as (6.1), one for each fixed-total subgroup. We call this a distribution a product multinomial. Each subgroup has its own fixed total. The full joint density is a product of \\(n\\) terms, as before, with each cell count appearing exactly once. For example, if the Sex margin is fixed for lymphoma, then the product multinomial distribution has the form \\[f_{\\bm{Y}}(\\bm{y};{\\bf p})= \\begin{cases} N_m!\\prod_{i=1}^4 {{p_{mi}^{y_{mi}}}\\over{y_{mi}!}} N_f!\\prod_{i=1}^4 {{p_{fi}^{y_{fi}}}\\over{y_{fi}!}} &amp; \\text{if $\\sum_{i=1}^4 y_{mi}=N_m$ and $\\sum_{i=1}^4 y_{fi}=N_f$} \\\\ 0 &amp; \\text{otherwise,} \\end{cases} \\] where \\(N_m\\) and \\(N_f\\) are the two fixed marginal totals (18 and 12 respectively), \\(y_{m1},\\ldots ,y_{m4}\\) are the cell counts for the Cell type/Remission cross-classification for males and \\(y_{f1},\\ldots ,y_{f4}\\) are the corresponding cell counts for females. Here \\(\\sum_{i=1}^4 p_{mi}=\\sum_{i=1}^4 p_{fi}=1\\), \\(E(Y_{mi})=N_mp_{mi}\\), \\(i=1,\\ldots ,4\\), and \\(E(Y_{fi})=N_fp_{fi}\\), \\(i=1,\\ldots ,4\\). Using similar results to those used in Section 6.3 (but not proved here), we can analyse contingency table data using Poisson log-linear models, even when the data has been obtained by product multinomial sampling. However, we must ensure that the Poisson model contains a term corresponding to the fixed margin (and all marginal terms). Then, the estimated means for the specified margin are equal to the corresponding fixed totals. For example, for the lymphoma dataset, for inferences obtained using a Poisson model to be valid when the Sex margin is fixed in advance, the Poisson model must contain the Sex main effect (and the intercept). For inferences obtained using a Poisson model to be valid when the Cell type/Sex margin is fixed in advance, the Poisson model must contain the Cell type/Sex interaction, and all marginal terms (the Cell type main effect, the Sex main effect and the intercept). Therefore, when analysing product multinomial data using a Poisson log-linear model, certain terms must be present in any model we fit. If they are removed, the inferences would no longer be valid. 6.5 Interpreting log-linear models for two-way tables Log-linear models for contingency tables enable us to determine important properties concerning the joint distribution of the classifying variables. In particular, the form of our preferred log linear model for a table will have implications for how the variables are associated. Each combination of the classifying variables occurs exactly once in a contingency table. Therefore, the model with the highest order interaction (between all the variables) and all marginal terms (all other interactions) is the saturated model. The implication of this model is that every combination of levels of the variables has its own mean (probability) and that there are no relationships between these means (no structure). The variables are highly dependent. To consider the implications of simpler models, we first consider a two-way \\(r\\times c\\) table where the two classifying variables \\(R\\) and \\(C\\) have \\(r\\) and \\(c\\) levels respectively. The saturated model \\(R*C\\) implies that the two variables are associated. If we remove the RC interaction, we have the model \\(R+C\\), \\[ \\log\\mu_i=\\alpha+\\beta_R(r_i)+\\beta_C(c_i),\\qquad{i=1,\\ldots ,n} \\] where \\(n=rc\\) is the total number of cells in the table. Because of the equivalence of Poisson and multinomial sampling, we can think of each cell mean \\(\\mu_i\\) as equal to \\(Np_i\\) where \\(N\\) is the total number of observations in the table, and \\(p_i\\) is a cell probability. As each combination of levels of \\(R\\) and \\(C\\) is represented in exactly one cell, it is also convenient to replace the cell label \\(i\\) by the pair of labels \\(j\\) and \\(k\\) representing the corresponding levels of \\(R\\) and \\(C\\) respectively. Hence \\[ \\log p_{jk}=\\alpha+\\beta_R(j)+\\beta_C(k)-\\log N, \\quad j=1,\\ldots ,r,\\quad k=1,\\ldots ,c. \\] Therefore \\[P(R=j,C=k)=\\exp[\\alpha+\\beta_R(j)+\\beta_C(k)-\\log N], \\quad j=1,\\ldots ,r,\\quad k=1,\\ldots ,c,\\] so \\[\\begin{align*} 1&amp;=\\sum_{j=1}^r\\sum_{k=1}^c\\exp[\\alpha+\\beta_R(j)+\\beta_C(k)-\\log N]\\cr &amp;={1\\over N}\\exp[\\alpha]\\sum_{j=1}^r\\exp[\\beta_R(j)]\\sum_{k=1}^c\\exp[\\beta_C(k)]. \\end{align*}\\] Furthermore \\[\\begin{align*} P(R=j)&amp;=\\sum_{k=1}^c\\exp[\\alpha+\\beta_R(j)+\\beta_C(k)-\\log N]\\cr &amp;={1\\over N}\\exp[\\alpha]\\exp[\\beta_R(j)]\\sum_{k=1}^c\\exp[\\beta_C(k)], \\quad j=1,\\ldots,r, \\end{align*}\\] and \\[\\begin{align*} P(C=k)&amp;=\\sum_{j=1}^r\\exp[\\alpha+\\beta_R(j)+\\beta_C(k)-\\log N]\\cr &amp;={1\\over N}\\exp[\\alpha]\\exp[\\beta_C(k)]\\sum_{j=1}^r\\exp[\\beta_R(j)], \\quad k=1,\\ldots,c. \\end{align*}\\] Therefore \\[\\begin{align*} P(R=j)P(C=k)&amp;= {1\\over N}\\exp[\\alpha]\\exp[\\beta_C(k)]\\exp[\\beta_R(j)]\\times 1\\cr &amp;=P(R=j,C=k), \\quad j=1,\\ldots ,r,\\quad k=1,\\ldots ,c. \\end{align*}\\] Absence of the interaction \\(R*C\\) in a log-linear model implies that \\(R\\) and \\(C\\) are independent variables. Absence of main effects is generally less interesting, and main effects are typically not removed from a log-linear model. 6.6 Interpreting log-linear models for multiway tables In multiway tables, absence of a two-factor interaction does not necessarily mean that the two variables are independent. For example, consider the lymphoma dataset, with 3 binary classifying variables Sex (\\(S\\)), Cell type (\\(C\\)) and Remission (\\(R\\)). After comparing the fit of several possible models, we find that a reasonable log-linear model for these data is \\(R * C + C * S\\). Hence the interaction between remission and sex is absent. The fitted cell probabilities from this log-linear model are shown in Table . The estimated probabilities for the two-way Sex/Remission margin (together with the corresponding one-way margins) are shown in Table . It can immediately be seen that this model does not imply independence of \\(R\\) and \\(S\\), as \\(\\hat{P}(R,S)\\ne\\hat{P}(R)\\hat{P}(S)\\). What the model \\(R*C+C*S\\) implies is that \\(R\\) is independent of \\(S\\) conditional on \\(C\\), that is \\[ P(R,S|C)=P(R|C)P(S|C). \\] Another way of expressing this is \\[ P(R|S,C)=P(R|C), \\] that is, the probability of each level of \\(R\\) given a particular combination of \\(S\\) and \\(C\\), does not depend on which level \\(S\\) takes. Table shows the estimated conditional probabilities for the lymphoma data. The probability of remission depends only on a patient’s cell type, and not on their sex. In general, if we have an \\(r\\)-way contingency table with classifying variables \\(X_1,\\ldots ,X_r\\), then a log linear model which does not contain the \\(X_1 * X_2\\) interaction (and therefore by the principle of marginality contains no interaction involving both \\(X_1\\) and \\(X_2\\)) implies that \\(X_1\\) and \\(X_2\\) are conditionally independent given \\(X_3,\\ldots ,X_r\\), that is \\[ P(X_1,X_2|X_3,\\ldots ,X_r)=P(X_1|X_3,\\ldots ,X_r)P(X_2|X_3,\\ldots ,X_r). \\] The proof of this is similar to the proof in the two-way case. Again, an alternative way of expressing conditional independence is \\[ P(X_1|X_2,X_3,\\ldots ,X_r)=P(X_1|X_3,\\ldots ,X_r) \\] or \\[ P(X_2|X_1,X_3,\\ldots ,X_r)=P(X_2|X_3,\\ldots ,X_r). \\] Although for the lymphoma dataset \\(R\\) and \\(S\\) are conditionally independent given \\(C\\), they are not marginally independent. Using the marginal cell probabilities from Table , we find that the probability of remission is \\(0.30\\) for men and \\(0.55\\) for women. Male patients have a much lower probability of remission. The reason for this is that, although \\(R\\) and \\(S\\) are not directly associated, they are both associated with \\(C\\). Observing the estimated values it is clear that patients with nodular cell type have a greater probability of remission, and furthermore, that female patients are more likely to have this cell type than males. Hence women are more likely to have remission than men. Suppose the factors for a three-way tables are \\(X_1\\), \\(X_2\\) and \\(X_3\\). We can list all possible models and the implications for the conditional independence structure: Model 1 containing the terms \\(X_1, X_2, X_3\\). All factors are mutually independent. Model 2 containing the terms \\(X_1*X_2, X_3\\). The factor \\(X_3\\) is jointly independent of \\(X_1\\) and \\(X_2\\). Model 3 containing the terms \\(X_1*X_2, X_2*X_3\\). The factors \\(X_1\\) and \\(X_3\\) are conditionally independent given \\(X_2\\). Model 4 containing the terms \\(X_1*X_2, X_2*X_3, X_1*X_3\\). There is no conditional independence structure. This is the model without the highest order interaction term. Model 5 containing \\(X_1*X_2*X_3\\). This is the saturated model. No more simplification of dependence structure is possible. 6.7 Simpson’s paradox Conditional and marginal association of two variables can therefore often appear somewhat different. Sometimes, the association can be ‘reversed’ so that what looks like a positive association marginally, becomes a negative association conditionally. This is known as Simpson’s paradox. In 1972-74, a survey of women was carried out in an area of Newcastle. A follow-up survey was carried out 20 years later. Among the variables observed in the initial survey was whether or not the individual was a smoker and among those in the follow-up survey was whether the individual was still alive, or had died in the intervening period. A summary of the responses is shown in Table . Looking at this table, it appears that the non-smokers had a greater probability of dying. However, there is an important extra variable to be considered, related to both smoking habit and mortality – age (at the time of the initial survey). When we consider this variable, we get Table . Conditional on every age at outset, it is now the smokers who have a higher probability of dying. The marginal association is reversed in the table conditional on age, because mortality (obviously) and smoking are associated with age. There are proportionally many fewer smokers in the older age-groups (where the probability of death is greater). When making inferences about associations between variables, it is important that all other variables which are relevant are considered. Marginal inferences may lead to misleading conclusions. "]
]
