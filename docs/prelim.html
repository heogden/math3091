<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Preliminaries | MATH3091: Statistical Modelling II</title>
  <meta name="description" content="The course notes for MATH3091: Statistical Modelling II" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Preliminaries | MATH3091: Statistical Modelling II" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Preliminaries | MATH3091: Statistical Modelling II" />
  
  <meta name="twitter:description" content="The course notes for MATH3091: Statistical Modelling II" />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH3012: Statistical Modelling II</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="prelim.html"><a href="prelim.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a><ul>
<li class="chapter" data-level="1.1" data-path="prelim.html"><a href="prelim.html#elements-of-statistical-modelling"><i class="fa fa-check"></i><b>1.1</b> Elements of statistical modelling</a></li>
<li class="chapter" data-level="1.2" data-path="prelim.html"><a href="prelim.html#regression-models"><i class="fa fa-check"></i><b>1.2</b> Regression Models</a></li>
<li class="chapter" data-level="1.3" data-path="prelim.html"><a href="prelim.html#example-data-to-be-analysed"><i class="fa fa-check"></i><b>1.3</b> Example data to be analysed</a><ul>
<li class="chapter" data-level="1.3.1" data-path="prelim.html"><a href="prelim.html#nitric-nitric-acid"><i class="fa fa-check"></i><b>1.3.1</b> <code>nitric</code>: Nitric acid</a></li>
<li class="chapter" data-level="1.3.2" data-path="prelim.html"><a href="prelim.html#birth-weight-of-newborn-babies"><i class="fa fa-check"></i><b>1.3.2</b> <code>birth</code>: Weight of newborn babies</a></li>
<li class="chapter" data-level="1.3.3" data-path="prelim.html"><a href="prelim.html#survival-time-to-death"><i class="fa fa-check"></i><b>1.3.3</b> <code>survival</code>: Time to death</a></li>
<li class="chapter" data-level="1.3.4" data-path="prelim.html"><a href="prelim.html#beetle-mortality-from-carbon-disulphide"><i class="fa fa-check"></i><b>1.3.4</b> <code>beetle</code>: Mortality from carbon disulphide</a></li>
<li class="chapter" data-level="1.3.5" data-path="prelim.html"><a href="prelim.html#shuttle-challenger-disaster"><i class="fa fa-check"></i><b>1.3.5</b> <code>shuttle</code>: Challenger disaster</a></li>
<li class="chapter" data-level="1.3.6" data-path="prelim.html"><a href="prelim.html#heart-treatment-for-heart-attack"><i class="fa fa-check"></i><b>1.3.6</b> <code>heart</code>: Treatment for heart attack</a></li>
<li class="chapter" data-level="1.3.7" data-path="prelim.html"><a href="prelim.html#accident-road-traffic-accidents"><i class="fa fa-check"></i><b>1.3.7</b> <code>accident</code>: Road traffic accidents</a></li>
<li class="chapter" data-level="1.3.8" data-path="prelim.html"><a href="prelim.html#lymphoma-lymphoma-patients"><i class="fa fa-check"></i><b>1.3.8</b> <code>lymphoma</code>: Lymphoma patients</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="prelim.html"><a href="prelim.html#likelihood-based-statistical-theory"><i class="fa fa-check"></i><b>1.4</b> Likelihood-based statistical theory</a><ul>
<li class="chapter" data-level="1.4.1" data-path="prelim.html"><a href="prelim.html#the-likelihood-function"><i class="fa fa-check"></i><b>1.4.1</b> The likelihood function</a></li>
<li class="chapter" data-level="1.4.2" data-path="prelim.html"><a href="prelim.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>1.4.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.4.3" data-path="prelim.html"><a href="prelim.html#score"><i class="fa fa-check"></i><b>1.4.3</b> Score</a></li>
<li class="chapter" data-level="1.4.4" data-path="prelim.html"><a href="prelim.html#info"><i class="fa fa-check"></i><b>1.4.4</b> Information</a></li>
<li class="chapter" data-level="1.4.5" data-path="prelim.html"><a href="prelim.html#sn:asnmle"><i class="fa fa-check"></i><b>1.4.5</b> Asymptotic distribution of the MLE</a></li>
<li class="chapter" data-level="1.4.6" data-path="prelim.html"><a href="prelim.html#comparing-statistical-models"><i class="fa fa-check"></i><b>1.4.6</b> Comparing statistical models</a></li>
<li class="chapter" data-level="1.4.7" data-path="prelim.html"><a href="prelim.html#sn:lrt"><i class="fa fa-check"></i><b>1.4.7</b> The log-likelihood ratio statistic</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="prelim.html"><a href="prelim.html#sn:lm"><i class="fa fa-check"></i><b>1.5</b> Linear Models</a><ul>
<li class="chapter" data-level="1.5.1" data-path="prelim.html"><a href="prelim.html#introduction"><i class="fa fa-check"></i><b>1.5.1</b> Introduction</a></li>
<li class="chapter" data-level="1.5.2" data-path="prelim.html"><a href="prelim.html#maximum-likelihood-estimation-1"><i class="fa fa-check"></i><b>1.5.2</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="1.5.3" data-path="prelim.html"><a href="prelim.html#properties-of-the-mle"><i class="fa fa-check"></i><b>1.5.3</b> Properties of the MLE</a></li>
<li class="chapter" data-level="1.5.4" data-path="prelim.html"><a href="prelim.html#comparing-linear-models"><i class="fa fa-check"></i><b>1.5.4</b> Comparing linear models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>2</b> Generalised Linear Models</a><ul>
<li class="chapter" data-level="2.1" data-path="glm.html"><a href="glm.html#sn:ef"><i class="fa fa-check"></i><b>2.1</b> The Exponential family</a></li>
<li class="chapter" data-level="2.2" data-path="glm.html"><a href="glm.html#components-of-a-generalised-linear-model"><i class="fa fa-check"></i><b>2.2</b> Components of a generalised linear model</a><ul>
<li class="chapter" data-level="2.2.1" data-path="glm.html"><a href="glm.html#the-random-component"><i class="fa fa-check"></i><b>2.2.1</b> The random component</a></li>
<li class="chapter" data-level="2.2.2" data-path="glm.html"><a href="glm.html#the-systematic-or-structural-component"><i class="fa fa-check"></i><b>2.2.2</b> The systematic (or structural) component</a></li>
<li class="chapter" data-level="2.2.3" data-path="glm.html"><a href="glm.html#the-link-function"><i class="fa fa-check"></i><b>2.2.3</b> The link function</a></li>
<li class="chapter" data-level="2.2.4" data-path="glm.html"><a href="glm.html#the-linear-model"><i class="fa fa-check"></i><b>2.2.4</b> The linear model</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="glm.html"><a href="glm.html#maximum-likelihood-estimation-2"><i class="fa fa-check"></i><b>2.3</b> Maximum likelihood estimation</a></li>
<li class="chapter" data-level="2.4" data-path="glm.html"><a href="glm.html#sn:glminfer"><i class="fa fa-check"></i><b>2.4</b> Inference</a></li>
<li class="chapter" data-level="2.5" data-path="glm.html"><a href="glm.html#sn:compglm"><i class="fa fa-check"></i><b>2.5</b> Comparing generalised linear models</a><ul>
<li class="chapter" data-level="2.5.1" data-path="glm.html"><a href="glm.html#sn:glmlrt"><i class="fa fa-check"></i><b>2.5.1</b> The generalised likelihood ratio test</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="glm.html"><a href="glm.html#scaled-deviance-and-the-saturated-model"><i class="fa fa-check"></i><b>2.6</b> Scaled deviance and the saturated model</a></li>
<li class="chapter" data-level="2.7" data-path="glm.html"><a href="glm.html#sn:unknowndisp"><i class="fa fa-check"></i><b>2.7</b> Models with unknown <span class="math inline">\(a(\phi)\)</span></a></li>
<li class="chapter" data-level="2.8" data-path="glm.html"><a href="glm.html#residuals"><i class="fa fa-check"></i><b>2.8</b> Residuals</a></li>
<li class="chapter" data-level="2.9" data-path="glm.html"><a href="glm.html#example-binary-regression"><i class="fa fa-check"></i><b>2.9</b> Example: Binary Regression</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-categorical.html"><a href="chap-categorical.html"><i class="fa fa-check"></i><b>3</b> Categorical data</a><ul>
<li class="chapter" data-level="3.1" data-path="chap-categorical.html"><a href="chap-categorical.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:multinomial"><i class="fa fa-check"></i><b>3.2</b> Multinomial sampling</a></li>
<li class="chapter" data-level="3.3" data-path="chap-categorical.html"><a href="chap-categorical.html#product-multinomial-sampling"><i class="fa fa-check"></i><b>3.3</b> Product multinomial sampling</a></li>
<li class="chapter" data-level="3.4" data-path="chap-categorical.html"><a href="chap-categorical.html#sn:loginter"><i class="fa fa-check"></i><b>3.4</b> Interpreting log-linear models</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH3091: Statistical Modelling II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="prelim" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Preliminaries</h1>
<div id="elements-of-statistical-modelling" class="section level2">
<h2><span class="header-section-number">1.1</span> Elements of statistical modelling</h2>
<p>Probability and statistics can be characterised as the study of variability. In particular, statistical inference is the science of analysing statistical data, viewed as the outcome of some random process, in order to draw conclusions about that random process.</p>
<p>Statistical models help us to <em>understand</em> the random process by which observed data have been generated. This may be of interest in itself, but also allows us to make <em>predictions</em> and perhaps most importantly <em>decisions</em> contingent on our inferences concerning the process.</p>
<p>It is also important, as part of the modelling process, to acknowledge that our conclusions are only based on a (potentially small) sample of possible observations of the process and are therefore subject to error. The science of statistical inference therefore involves assessment of the uncertainties associated with the conclusions we draw.</p>
<p>Probability theory is the mathematics associated with randomness and uncertainty. We usually try to describe random processes using probability models. Then, statistical inference may involve estimating any unspecified features of a model, comparing competing models, and assessing the appropriateness of a model; all in the light of observed data.</p>
<p>In order to identify ‘good’ statistical models, we require some principles on which to base our modelling procedures. In general, we have three requirements of a statistical model</p>
<ul>
<li>Plausibility</li>
<li>Parsimony</li>
<li>Goodness of fit</li>
</ul>
<p>The first of these is not a statistical consideration, and a subject-matter expert usually needs to be consulted about this. For some objectives, like prediction, it might be considered unimportant. Parsimony and goodness of fit are statistical issues. Indeed, there is usually a trade-off between the two and our statistical modelling strategies will take account of this.</p>
</div>
<div id="regression-models" class="section level2">
<h2><span class="header-section-number">1.2</span> Regression Models</h2>
<p>Many statistical models, and all the ones we shall deal with in MATH3091, can be formulated as <em>regression</em> models.</p>
<p>In practical applications, we often distinguish between a <em>response</em> variable and a group of <em>explanatory</em> variables. The aim is to determine the pattern of dependence of the response variable on the explanatory variables. A regression model has the general form</p>
<blockquote>
<p>response = function(structure and randomness)</p>
</blockquote>
<p>The structural part of the model describes how the response depends on the explanatory variables and the random part defines the probability distribution of the response. Together, they produce the response and the statistical modeller’s task is to ‘separate’ these out.</p>
</div>
<div id="example-data-to-be-analysed" class="section level2">
<h2><span class="header-section-number">1.3</span> Example data to be analysed</h2>
<div id="nitric-nitric-acid" class="section level3">
<h3><span class="header-section-number">1.3.1</span> <code>nitric</code>: Nitric acid</h3>
<p>This data set relates to 21 successive days of operation of a plant oxidising ammonia to nitric acid. The response <code>yield</code> is ten times the percentage of ingoing ammonia that is lost as unabsorbed nitric acid (an indirect measure of the yield). The aim here is to study how the yield depends on flow of air to the plant (<code>flow</code>), temperature of the cooling water entering the absorption tower (<code>temp</code>) and concentration of nitric acid in the absorbing liquid (<code>conc</code>). These data will be analysed in worksheet 2 using multiple linear regression models.</p>
</div>
<div id="birth-weight-of-newborn-babies" class="section level3">
<h3><span class="header-section-number">1.3.2</span> <code>birth</code>: Weight of newborn babies</h3>
<p>This data set contains weights of 24 newborn babies. There are two explanatory variables, sex (<code>Sex</code>) and gestational age in weeks (<code>Age</code>) together with the response variable, birth weight in grams (<code>Weight</code>). The aim here is to study how birth weight depends on sex and gestational age. This data set will be analysed in worksheet 3 by using multiple linear regression models including both categorical and continuous explanatory variables.</p>
</div>
<div id="survival-time-to-death" class="section level3">
<h3><span class="header-section-number">1.3.3</span> <code>survival</code>: Time to death</h3>
<p>This data set, analysed in worksheet 4, contains survival times in 10 hour units (<code>time</code>) of 48 rats each allocated to one of 12 combinations of 3 poisons (<code>poison</code>) and 4 treatments (<code>treatment</code>). The aim here is to study how survival time depends on the poison and the treatment, and to determine whether there is an interaction between these two categorical variables.</p>
</div>
<div id="beetle-mortality-from-carbon-disulphide" class="section level3">
<h3><span class="header-section-number">1.3.4</span> <code>beetle</code>: Mortality from carbon disulphide</h3>
<p>This data set represents the number of beetles exposed (<code>exposed</code>) and number killed (<code>killed</code>) in eight groups exposed to different doses (<code>dose</code>) of a particular insecticide. Interest is focussed on how mortality is related to dose. It seems sensible to model the number of beetles killed in each group as the binomial random variable with probability of death depending on dose. This will be discussed in worksheet 5.</p>
</div>
<div id="shuttle-challenger-disaster" class="section level3">
<h3><span class="header-section-number">1.3.5</span> <code>shuttle</code>: Challenger disaster</h3>
<p>This data set concerns the 23 space shuttle flights before the Challenger disaster. The disaster is thought to have been caused by the failure of a number of O-rings, of which there were six in total. The data consist of four variables, the number of damaged O-rings for each pre-Challenger flight (<code>n_damaged</code>), together with the launch temperature in degrees Fahrenheit (<code>temp</code>), the pressure at which the pre-launch test of O-ring leakage was carried out (<code>pressure</code>) and the name of the orbiter (<code>orbiter</code>). The Challenger launch temperature on 20th January 1986 was 31F. The aim is to predict the probability of O-ring damage at the Challenger launch. This will be discussed in worksheet 6.</p>
</div>
<div id="heart-treatment-for-heart-attack" class="section level3">
<h3><span class="header-section-number">1.3.6</span> <code>heart</code>: Treatment for heart attack</h3>
<p>This data set represents the results of a clinical trial to assess the effectiveness of a thrombolytic (clot-busting) treatment for patients who have suffered an acute myocardial infarction (heart attack). There are four categorical explanatory variables, representing</p>
<ul>
<li>the <code>site</code> of infarction: anterior, inferior or other</li>
<li>the <code>time</code> between infarction and treatment: <span class="math inline">\(\le 12\)</span> or <span class="math inline">\(&gt;12\)</span> hours</li>
<li>whether the patient was already taking Beta-blocker medication prior to the infarction, <code>blocker</code>: yes or no</li>
<li>the <code>treatment</code> the patient was given: active or placebo.</li>
</ul>
<p>For each combination of these categorical variables, the dataset gives the total number of patients (<code>n_patients</code>), and the number who survived for for 35 days (<code>n_survived</code>). The aim is to find out how these categorical variables affect a patient’s chance of survival. These data will be analysed in worksheet 7.</p>
</div>
<div id="accident-road-traffic-accidents" class="section level3">
<h3><span class="header-section-number">1.3.7</span> <code>accident</code>: Road traffic accidents</h3>
<p>This example concerns the number of road accidents (<code>number</code>) and the volume of traffic (<code>volume</code>), on each of two roads in Cambridge (<code>road</code>), at various times of day (<code>time</code>, taking values <code>morning</code>, <code>midday</code> or <code>afternoon</code>). We should be able to answer questions like:</p>
<ol style="list-style-type: decimal">
<li>Is Mill Road more dangerous than Trumpington Road?</li>
<li>How does time of day affect the rate of road accident?</li>
</ol>
<p>These issues will be considered in worksheet 8.</p>
</div>
<div id="lymphoma-lymphoma-patients" class="section level3">
<h3><span class="header-section-number">1.3.8</span> <code>lymphoma</code>: Lymphoma patients</h3>
<p>The <code>lymphoma</code> data set represents 30 lymphoma patients classified by sex (<code>Sex</code>), cell type of lymphoma (<code>Cell</code>) and response to treatment (<code>Remis</code>). It is an example of data which may be represented as a three-way (<span class="math inline">\(2\times 2\times 2\)</span>) contingency table. The aim here is to study the complex dependence structures between the three classifying factors. This is taken up in worksheet 9.</p>
</div>
</div>
<div id="likelihood-based-statistical-theory" class="section level2">
<h2><span class="header-section-number">1.4</span> Likelihood-based statistical theory</h2>
<div id="the-likelihood-function" class="section level3">
<h3><span class="header-section-number">1.4.1</span> The likelihood function</h3>
<p>Probability distributions like the binomial, Poisson and normal, enable us to calculate probabilities, and other quantities of interest (e.g. expectations) for a probability model of a random process. Therefore, given the model, we can make statements about possible outcomes of the process.</p>
<p>Statistical inference is concerned with the inverse problem. Given outcomes of a random process (observed data), what conclusions (inferences) can we draw about the process itself?</p>
<p>We assume that the <span class="math inline">\(n\)</span> observations of the response <span class="math inline">\(\bm y=(y_1,\ldots ,y_n)^T\)</span> are observations of random variables <span class="math inline">\(\bm Y=(Y_1,\ldots ,Y_n)^T\)</span>, which have joint p.d.f. <span class="math inline">\(f_{\bm Y}\)</span> (joint p.f. for discrete variables). We use the observed data <span class="math inline">\(\bm y\)</span> to make inferences about <span class="math inline">\(f_{\bm Y}\)</span>.</p>
<p>We usually make certain assumptions about <span class="math inline">\(f_{\bm Y}\)</span>. In particular, we often assume that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <em>independent</em> random variables. Hence <span class="math display">\[
f_{\bm Y}(\bm y)=f_{Y_1}(y_1)f_{Y_2}(y_2)\cdots f_{Y_n}(y_n)
=\prod_{i=1}^n f_{Y_i}(y_i).
\]</span></p>
<p>In parametric statistical inference, we specify a joint distribution <span class="math inline">\(f_{\bm Y}\)</span>, for <span class="math inline">\(\bm Y\)</span>, which is known, except for the values of parameters <span class="math inline">\(\theta_1,\theta_2,\ldots ,\theta_p\)</span> (sometimes denoted by <span class="math inline">\(\bm \theta\)</span>). Then we use the observed data <span class="math inline">\(\bm y\)</span> to make inferences about <span class="math inline">\(\theta_1,\theta_2,\ldots ,\theta_p\)</span>. In this case, we usually write <span class="math inline">\(f_{\bm Y}\)</span> as <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span>, to make explicit the dependence on the unknown <span class="math inline">\(\bm \theta\)</span>.</p>
<p>Until now, we have thought of the joint density <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span> as a function of <span class="math inline">\(\bm{y}\)</span> for fixed <span class="math inline">\(\bm \theta\)</span>, which describes the relative probabilities of different possible values of <span class="math inline">\(\bm y\)</span>, given a particular set of parameters <span class="math inline">\(\bm \theta\)</span>. However, in statistical inference, we have observed <span class="math inline">\(y_1, \ldots, y_n\)</span> (values of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>). Knowledge of the probability of alternative possible realisations of <span class="math inline">\(\bm Y\)</span> is largely irrelevant. What we want to know about is <span class="math inline">\(\bm \theta\)</span>.</p>
<p>Our only link between the observed data <span class="math inline">\(y_1, \ldots, y_n\)</span> and <span class="math inline">\(\bm \theta\)</span> is through the function <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span>. Therefore, it seems sensible that parametric statistical inference should be based on this function. We can think of <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)\)</span> as a function of <span class="math inline">\(\bm \theta\)</span> for fixed <span class="math inline">\(\bm{y}\)</span>, which describes the relative <em>likelihoods</em> of different possible (sets of) <span class="math inline">\(\bm \theta,\)</span> given observed data <span class="math inline">\(y_1, \ldots, y_n\)</span>. We write <span class="math display">\[L(\bm \theta; \bm y) = f_{\bm Y}(\bm y;\bm \theta)\]</span> for this <em>likelihood</em>, which is a function of the unknown parameter <span class="math inline">\(\bm \theta\)</span>. For convenience, we often drop <span class="math inline">\(\bm y\)</span> from the notation, and write <span class="math inline">\(L(\bm \theta)\)</span>.</p>
<p>The likelihood function is of central importance in parametric statistical inference. It provides a means for comparing different possible values of <span class="math inline">\(\bm \theta\)</span>, based on the probabilities (or probability densities) that they assign to the observed data <span class="math inline">\(y_1, \ldots, y_n\)</span>.</p>
<p><strong>Notes</strong></p>
<ol style="list-style-type: decimal">
<li>Frequently it is more convenient to consider the <em>log-likelihood</em> function <span class="math inline">\(\ell(\bm \theta) = \log L(\bm \theta)\)</span>.</li>
<li>Nothing in the definition of the likelihood requires <span class="math inline">\(y_1, \ldots, y_n\)</span> to be observations of independent random variables, although we shall frequently make this assumption.</li>
<li>Any factors which depend on <span class="math inline">\(y_1, \ldots, y_n\)</span> alone (and not on <span class="math inline">\(\bm \theta\)</span>) can be ignored when writing down the likelihood. Such factors give no information about the relative likelihoods of different possible values of <span class="math inline">\(\bm \theta\)</span>.</li>
</ol>
<p><strong>Example (Bernoulli)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, independent identically distributed (i.i.d.) Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\theta=(p)\)</span> and the likelihood is <span class="math display">\[
L(p)=\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}=p^{\sum_{i=1}^n y_i}(1-p)^{n-\sum_{i=1}^n y_i}.
\]</span> The log-likelihood is <span class="math display">\[
\ell(p) = \log L(p) =n\bar y\log p+n(1-\bar y)\log(1-p).
\]</span></p>
<strong>Example (Normal)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and the likelihood is
<span class="math display">\[\begin{align*}
L(\mu,\sigma^2) &amp;=  \prod_{i=1}^n {1\over{\sqrt{2\pi\sigma^2}}}
\exp\left(-{1\over{2\sigma^2}}(y_i-\mu)^2\right) \\
&amp;=(2\pi\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right) \\
&amp;\propto (\sigma^2)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}}\sum(y_i-\mu)^2\right).
\end{align*}\]</span>
<p>The log-likelihood is <span class="math display">\[\ell(\mu, \sigma^2) = \log L(\mu,\sigma^2)=-{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]</span></p>
</div>
<div id="maximum-likelihood-estimation" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Maximum likelihood estimation</h3>
<p>One of the primary tasks of parametric statistical inference is <em>estimation</em> of the unknown parameters <span class="math inline">\(\theta_1, \ldots, \theta_p\)</span>. Consider the value of <span class="math inline">\(\bm \theta\)</span> which maximises the likelihood function. This is the ‘most likely’ value of <span class="math inline">\(\bm \theta\)</span>, the one which makes the observed data ‘most probable’. When we are searching for an estimate of <span class="math inline">\(\bm \theta\)</span>, this would seem to be a good candidate.</p>
<p>We call the value of <span class="math inline">\(\bm \theta\)</span> which maximises the likelihood <span class="math inline">\(L(\theta)\)</span> the <em>maximum likelihood estimate</em> (MLE) of <span class="math inline">\(\bm \theta\)</span>, denoted by <span class="math inline">\(\hat{\bm \theta}\)</span>. <span class="math inline">\(\hat{\bm \theta}\)</span> depends on <span class="math inline">\(\bm y\)</span>, as different observed data samples lead to different likelihood functions. The corresponding function of <span class="math inline">\(\bm Y\)</span> is called the <em>maximum likelihood estimator</em> and is also denoted by <span class="math inline">\(\hat{\bm \theta}\)</span>.</p>
<p>Note that as <span class="math inline">\(\bm \theta=(\theta_1, \ldots, \theta_p)\)</span>, the MLE for any component of <span class="math inline">\(\bm \theta\)</span> is given by the corresponding component of <span class="math inline">\(\hat{\bm \theta}=(\hat{\theta}_1,\ldots ,\hat{\theta}_p)^T\)</span>. Similarly, the MLE for any function of parameters <span class="math inline">\(g(\bm \theta)\)</span> is given by <span class="math inline">\(g(\hat{\bm \theta})\)</span>.</p>
<p>As <span class="math inline">\(\log\)</span> is a strictly increasing function, the value of <span class="math inline">\(\bm \theta\)</span> which maximises <span class="math inline">\(L(\bm \theta)\)</span> also maximises <span class="math inline">\(\ell(\bm \theta) = \log L (\bm \theta)\)</span>. It is almost always easier to maximise <span class="math inline">\(\ell(\bm \theta)\)</span>. This is achieved in the usual way; finding a stationary point by differentiating <span class="math inline">\(\ell(\bm \theta)\)</span> with respect to <span class="math inline">\(\theta_1, \ldots, \theta_p\)</span>, and solving the resulting <span class="math inline">\(p\)</span> simultaneous equations. It should also be checked that the stationary point is a maximum.</p>
<p><strong>Example (Bernoulli)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(p)\)</span> and the log-likelihood is <span class="math display">\[\ell(p)=n\bar y\log p+n(1-\bar y)\log(1-p).\]</span> Differentiating with respect to <span class="math inline">\(p\)</span>, <span class="math display">\[\frac{\partial}{\partial p} \ell(p) = \frac{n\bar y}{p}-\frac{n(1-\bar y)}{1-p}\]</span> so the MLE <span class="math inline">\(\hat p\)</span> solves <span class="math display">\[\frac{n\bar y}{\hat{p}} -{{n(1-\bar y)}\over{1-\hat{p}}} = 0.\]</span> Solving this for <span class="math inline">\(\hat{p}\)</span> gives <span class="math inline">\(\hat{p}=\bar y\)</span>. Note that <span class="math display">\[\frac{\partial^2}{\partial p^2} \ell(p)= {{-n\bar y}/p^2}-{{n(1-\bar y)}/({1-p})^2}&lt;0\]</span> everywhere, so the stationary point is clearly a maximum.</p>
<strong>Example (Normal)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and and the log-likelihood is <span class="math display">\[\ell(\mu,\sigma^2) = -{n\over 2}\log(2\pi)-{n\over 2}\log(\sigma^2)
-{1\over{2\sigma^2}}\sum(y_i-\mu)^2.\]</span> Differentiating with respect to <span class="math inline">\(\mu\)</span> <span class="math display">\[{\partial\over{\partial \mu}} \ell(\mu,\sigma^2)=
  {1\over{\sigma^2}}\sum(y_i-\mu)={{n(\bar y-\mu)}\over{\sigma^2}}\]</span> so <span class="math inline">\((\hat \mu, \hat \sigma^2)\)</span> solve
<span class="math display" id="eq:normalScoreMu">\[\begin{equation}
  \frac{n(\bar y-\hat{\mu})}{\hat \sigma^2} = 0.
  \tag{1.1}
\end{equation}\]</span>
Differentiating with respect to <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[\frac{\partial}{\partial \sigma^2} \ell (\mu,\sigma^2)=
- \frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum(y_i-\mu)^2,\]</span> so
<span class="math display" id="eq:normalScoreSs">\[\begin{equation}
  -{n\over {2\hat \sigma^2}}+{1\over{2(\hat \sigma^2)^2}}\sum(y_i-\hat{\mu})^2 = 0
  \tag{1.2}
\end{equation}\]</span>
<p>Solving <a href="prelim.html#eq:normalScoreMu">(1.1)</a> and <a href="prelim.html#eq:normalScoreSs">(1.2)</a>, we obtain <span class="math inline">\(\hat{\mu}= \bar y\)</span> and <span class="math display">\[\hat \sigma^2 =  {1\over n}\sum(y_i-\hat{\mu})^2=
{1\over n}\sum(y_i-\bar y)^2.\]</span></p>
<p>Strictly, to show that this stationary point is a maximum, we need to show that the Hessian matrix (the matrix of second derivatives with elements <span class="math inline">\([\bm{H}(\bm \theta)]_{ij}={{\partial^2}\over{\partial\theta_i\partial\theta_j}}\ell(\theta)\)</span>) is negative definite at <span class="math inline">\(\bm \theta=\hat{\bm \theta}\)</span>, that is <span class="math inline">\(\bm{a}^T \bm{H}(\hat{\bm \theta})\bm{a}&lt;0\)</span> for every <span class="math inline">\(\bm{a}\ne {\bf 0}\)</span>. Here <span class="math display">\[
\bm{H}(\hat{\mu},\hat \sigma^2)= \begin{pmatrix}
- \frac{n}{\hat \sigma^2 } &amp; 0 \cr
0   &amp;-\frac{n}{2(\hat \sigma^2)^2} \end{pmatrix}
\]</span> which is clearly negative definite.</p>
</div>
<div id="score" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Score</h3>
<p>Let <span class="math display">\[
u_i(\bm \theta)\equiv{{\partial}\over{\partial\theta_i}} \ell(\theta), \quad i=1,\ldots ,p
\]</span> and <span class="math inline">\(\bm{u}(\bm \theta)\equiv[u_1(\bm \theta),\ldots ,u_p(\bm \theta)]^T\)</span>. Then we call <span class="math inline">\(\bm{u}(\bm \theta)\)</span> the <em>vector of scores</em> or <em>score vector</em>. Where <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\bm \theta=(\theta)\)</span>, the <em>score</em> is the scalar defined as <span class="math display">\[
u(\theta)\equiv{{\partial}\over{\partial\theta}}\ell(\theta).
\]</span> The maximum likelihood estimate <span class="math inline">\(\hat{\bm \theta}\)</span> satisfies <span class="math display">\[u(\hat{\bm \theta})={\bm 0},\]</span> that is, <span class="math display">\[u_i(\hat{\bm \theta})=0, \quad i=1,\ldots ,p.\]</span> Note that <span class="math inline">\(u(\bm{\theta})\)</span> is a function of <span class="math inline">\(\bm \theta\)</span> for fixed (observed) <span class="math inline">\(\bm y\)</span>. However, if we replace <span class="math inline">\(y_1, \ldots, y_n\)</span> in <span class="math inline">\(u(\bm{\theta})\)</span>, by the corresponding random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span> then we obtain a vector of random variables <span class="math inline">\(U(\bm{\theta})\equiv [U_1(\bm \theta),\ldots ,U_p(\bm \theta)]^T\)</span>.</p>
<p>An important result in likelihood theory is that the expected score at the true (but unknown) value of <span class="math inline">\(\bm \theta\)</span> is zero, <em>i.e.</em> <span class="math display">\[E[U(\bm{\theta})]={\bf 0}\]</span> or <span class="math display">\[E[U_i(\bm \theta)]= 0,
\quad i=1,\ldots ,p,\]</span> provided that</p>
<ol style="list-style-type: decimal">
<li>The expectation exists.</li>
<li>The sample space for <span class="math inline">\(\bm Y\)</span> does not depend on <span class="math inline">\(\bm \theta\)</span>.</li>
</ol>

<p><strong>Example (Bernoulli)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(p)\)</span> and <span class="math display">\[u(p)=n\bar y/ p-n(1-\bar y)/(1-p).\]</span> Since <span class="math inline">\(E[U(p)] = 0\)</span>, we must have <span class="math inline">\(E[\bar Y]=p\)</span> (which we already know is correct).</p>
<strong>Example (Normal)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and
<span class="math display">\[\begin{align*}
u_1(\mu,\sigma^2)&amp;= {{n(\bar y-\mu)}/{\sigma^2}}\cr
u_2(\mu,\sigma^2)&amp;= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum_{i=1}^n{(y_i-\mu)^2}
\end{align*}\]</span>
<p>Since <span class="math inline">\(E[\bm U(\mu,\sigma^2)] = {\bm 0}\)</span>, we must have <span class="math inline">\(E[\bar Y]=\mu\)</span> and <span class="math inline">\(E[{\textstyle{1\over n}}\sum_{i=1}^n{(Y_i-\mu)^2}]=\sigma^2.\)</span></p>
</div>
<div id="info" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Information</h3>
<p>Suppose that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, whose joint p.d.f. <span class="math inline">\(L(\theta)\)</span> is completely specified except for the values of <span class="math inline">\(p\)</span> unknown parameters <span class="math inline">\(\bm \theta=(\theta_1, \ldots, \theta_p)^T\)</span>. Previously, we defined the Hessian matrix <span class="math inline">\(H(\bm{\theta})\)</span> to be the matrix with components <span class="math display">\[
[H(\bm{\theta})]_{ij}\equiv{{\partial^2}\over
{\partial\theta_i\partial\theta_j}} \ell(\theta)
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
\]</span> We call the matrix <span class="math inline">\(-H(\bm{\theta})\)</span> the <em>observed information matrix</em>. Where <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\bm \theta=(\theta)\)</span>, the <em>observed information</em> is a scalar defined as <span class="math display">\[
-H(\theta)\equiv-{{\partial}\over{\partial\theta^2}}\ell(\theta).
\]</span></p>
<!-- Here, we are interpreting $\bm \theta$ as the true (but unknown) value of -->
<!-- the parameter. -->
<p>As with the score, if we replace <span class="math inline">\(y_1, \ldots, y_n\)</span> in <span class="math inline">\(H(\bm{\theta})\)</span>, by the corresponding random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, we obtain a matrix of random variables. Then, we define the <em>expected information matrix</em> or <em>Fisher information matrix</em> <span class="math display">\[
[\mathcal{I}(\bm \theta)]_{ij}=E(-[H(\bm{\theta})]_{ij})
\qquad i=1,\ldots ,p;\;j=1,\ldots ,p.
\]</span></p>
<p>An important result in likelihood theory is that the variance-covariance matrix of the score vector is equal to the expected information matrix <em>i.e.</em> <span class="math display">\[\text{Var}[U(\bm{\theta})]=\mathcal{I}(\bm \theta)\]</span> or <span class="math display">\[\text{Var}[U(\bm{\theta})]_{ij}= [\mathcal{I}(\bm \theta)]_{ij}, 
\quad i=1,\ldots ,p, \quad j=1,\ldots ,p\]</span> provided that</p>
<ol style="list-style-type: decimal">
<li>The variance exists.</li>
<li>The sample space for <span class="math inline">\(\bm Y\)</span> does not depend on <span class="math inline">\(\bm \theta\)</span>.</li>
</ol>


<strong>Example (Bernoulli)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(p)\)</span> and
<span class="math display">\[\begin{align*}
u(p)&amp;= {{n\bar y}\over{ p}}-{{n(1-\bar y)}\over {(1-p)}}\cr
-H(p)&amp;= {{n\bar y}\over{ p^2}}+{{n(1-\bar y)}\over {(1-p)^2}}\cr
{\cal I}(p)&amp;= {{n}\over{ p}}+{{n}\over {(1-p)}}={{n}\over {p(1-p)}}.
\end{align*}\]</span>

<strong>Example (Normal)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. <span class="math inline">\(N(\mu,\sigma^2)\)</span> random variables. Here <span class="math inline">\(\bm \theta=(\mu,\sigma^2)\)</span> and
<span class="math display">\[\begin{align*}
u_1(\mu,\sigma^2) &amp;=  \frac{n(\bar y-\mu)}{\sigma^2} \\
u_2(\mu,\sigma^2) &amp;= -{n\over {2\sigma^2}}+{1\over{2(\sigma^2)^2}}\sum(y_i-\mu)^2.
\end{align*}\]</span>
<p>Therefore <span class="math display">\[
-\bm{H}(\mu,\sigma^2) = \begin{pmatrix}
\frac{n}{\sigma^2} &amp; \frac{n(\bar y-\mu)}{(\sigma^2)^2} \cr
\frac{n(\bar y-\mu)}{(\sigma^2)^2}&amp;
 \frac{1}{(\sigma^2)^3} \sum(y_i-\mu)^2- \frac{n}{2(\sigma^2)^2}
\end{pmatrix}
\]</span> <span class="math display">\[
{\cal I}(\mu,\sigma^2)= \begin{pmatrix}
\frac{n}{\sigma^2} &amp; 0 \cr
0&amp; \frac{n}{2(\sigma^2)^2}
\end{pmatrix}.
\]</span></p>
</div>
<div id="sn:asnmle" class="section level3">
<h3><span class="header-section-number">1.4.5</span> Asymptotic distribution of the MLE</h3>
<p>Maximum likelihood estimation is an attractive method of estimation for a number of reasons. It is intuitively sensible and usually reasonably straightforward to carry out. Even when the simultaneous equations we obtain by differentiating the log-likelihood function are impossible to solve directly, solution by numerical methods is usually feasible.</p>
<p>Perhaps the most compelling reason for considering maximum likelihood estimation is the asymptotic behaviour of maximum likelihood estimators.</p>
<p>Suppose that <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of independent random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, whose joint p.d.f. <span class="math inline">\(f_{\bm Y}(\bm y;\bm \theta)=\prod_{i=1}^n f_{Y_i}(y_i;\bm \theta)\)</span> is completely specified except for the values of an unknown parameter vector <span class="math inline">\(\bm \theta\)</span>, and that <span class="math inline">\(\hat{\bm \theta}\)</span> is the maximum likelihood estimator of <span class="math inline">\(\bm \theta\)</span>.</p>
<p>Then, as <span class="math inline">\(n\to\infty\)</span>, the distribution of <span class="math inline">\(\hat{\bm \theta}\)</span> tends to a multivariate normal distribution with mean vector <span class="math inline">\(\bm \theta\)</span> and variance covariance matrix <span class="math inline">\(\mathcal{I}(\bm \theta)^{-1}\)</span>.</p>
<p>Where <span class="math inline">\(p=1\)</span> and <span class="math inline">\(\bm \theta=(\theta)\)</span>, the distribution of the MLE <span class="math inline">\(\hat{\theta}\)</span> tends to <span class="math inline">\(N[\theta,1/{\cal I}(\theta)]\)</span>.</p>
<p>For ‘large enough <span class="math inline">\(n\)</span>’, we can treat the asymptotic distribution of the MLE as an approximation. The fact that <span class="math inline">\(E(\hat{\bm \theta})\approx\bm \theta\)</span> means that the maximum likelihood estimator is <em>approximately unbiased</em> for large samples. The variance of <span class="math inline">\(\hat{\bm \theta}\)</span> is approximately <span class="math inline">\(\mathcal{I}(\bm \theta)^{-1}\)</span>. It is possible to show that this is the smallest possible variance of any unbiased estimator of <span class="math inline">\(\bm \theta\)</span> (this result is called the Cramér–Rao lower bound, which we do not prove here). Therefore the MLE is the ‘best possible’ estimator in large samples (and therefore we hope also reasonable in small samples, though we should investigate this case by case).</p>
<p>The usefulness of an estimate is always enhanced if some kind of measure of its precision can also be provided. Usually, this will be a <em>standard error</em>, an estimate of the standard deviation of the associated estimator. For the maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span>, a standard error is given by <span class="math display">\[
s.e.(\hat{\theta})={1\over{{\cal I}(\hat{\theta})^{{1\over 2}}}},
\]</span> and for a vector parameter <span class="math inline">\(\bm \theta\)</span> <span class="math display">\[
s.e.(\hat{\theta}_i)=[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{{1\over 2}},
\quad i=1,\ldots ,p.
\]</span></p>
<p>An alternative summary of the information provided by the observed data about the location of a parameter <span class="math inline">\(\theta\)</span> and the associated precision is a <em>confidence interval</em>.</p>
<p>The asymptotic distribution of the maximum likelihood estimator can be used to provide approximate large sample confidence intervals. Asymptotically, <span class="math inline">\(\hat{\theta}_i\)</span> has a <span class="math inline">\(N(\theta_i,[\mathcal{I}(\bm \theta)^{-1}]_{ii})\)</span> distribution and we can find <span class="math inline">\(z_{1-\frac{\alpha}{2}}\)</span> such that <span class="math display">\[
P\left(- z_{1-\frac{\alpha}{2}}\le {{\hat{\theta}_i-\theta_i}\over{[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}}}\le
z_{1-\frac{\alpha}{2}}\right) = 1- \alpha.
\]</span> Therefore <span class="math display">\[
P\left(\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}\le\theta_i
\le\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[\mathcal{I}(\bm \theta)^{-1}]_{ii}^{1\over 2}
\right) = 1- \alpha.
\]</span> The endpoints of this interval cannot be evaluated because they also depend on the unknown parameter vector <span class="math inline">\(\bm \theta\)</span>. However, if we replace <span class="math inline">\(\mathcal{I}(\bm \theta)\)</span> by its MLE <span class="math inline">\({\cal I}(\hat{\bm \theta})\)</span> we obtain the approximate large sample <span class="math inline">\(100(1 - \alpha)\%\)</span> confidence interval <span class="math display">\[
[\hat{\theta}_i-z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2},
\hat{\theta}_i+z_{1-\frac{\alpha}{2}}[{\cal I}(\hat{\bm \theta})^{-1}]_{ii}^{1\over 2}].
\]</span> For <span class="math inline">\(\alpha=0.1,0.05,0.01\)</span>, <span class="math inline">\(z_{1-\frac{\alpha}{2}}=1.64,1.96,2.58\)</span>.</p>
<strong>Example (Bernoulli)</strong>. If <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables then asymptotically <span class="math inline">\(\hat{p}=\bar y\)</span> has a <span class="math inline">\(N(p,{p(1-p)}/ n)\)</span> distribution, and a large sample 95% confidence interval for <span class="math inline">\(p\)</span> is
<span class="math display">\[\begin{align*}
&amp; [\hat{p}- 1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2},
\hat{p}+1.96[{\cal I}(\hat{p})^{-1}]^{1\over 2}]
\cr
&amp;=
[\hat{p}-1.96[\hat{p}(1-\hat{p})/n]^{1\over 2},
\hat{p}+1.96[\hat{p}(1-\hat{p})/n]^{1\over 2}]\cr
&amp;=
[\bar y-1.96[\bar y(1-\bar y)/n]^{1\over 2},
\bar y+1.96[\bar y(1-\bar y)/n]^{1\over 2}].
\end{align*}\]</span>
</div>
<div id="comparing-statistical-models" class="section level3">
<h3><span class="header-section-number">1.4.6</span> Comparing statistical models</h3>
<p>If we have a set of competing probability models which might have generated the observed data, we may want to determine which of the models is most appropriate. In practice, we proceed by comparing models pairwise. Suppose that we have two competing alternatives, <span class="math inline">\(f^{(0)}_{\bm Y}\)</span> (model <span class="math inline">\(H_0\)</span>) and <span class="math inline">\(f^{(1)}_{\bm Y}\)</span> (model <span class="math inline">\(H_1\)</span>) for <span class="math inline">\(f_{\bm Y}\)</span>, the joint distribution of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>. The most common situation is where <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> both take the same parametric form, <span class="math inline">\(f_{\bm Y}(\bm{y};\bm \theta)\)</span> but with <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> for <span class="math inline">\(H_0\)</span> and <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span> for <span class="math inline">\(H_1\)</span>, where <span class="math inline">\(\Theta^{(0)}\)</span> and <span class="math inline">\(\Theta^{(1)}\)</span> are alternative sets of possible values for <span class="math inline">\(\bm \theta\)</span>.</p>
<p>A hypothesis test provides a mechanism for comparing the two competing statistical models, <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span>. A hypothesis test does not treat the two hypotheses (models) symmetrically. One hypothesis, <span class="math inline">\(H_0\)</span>, is accorded special status, and referred to as the <em>null hypothesis</em>. The null hypothesis is the reference model, and will be assumed to be appropriate unless the observed data strongly indicate that <span class="math inline">\(H_0\)</span> is inappropriate, and that <span class="math inline">\(H_1\)</span> (the <em>alternative</em> hypothesis) should be preferred.</p>
<p>Hence, the fact that a hypothesis test does not reject <span class="math inline">\(H_0\)</span> should not be taken as evidence that <span class="math inline">\(H_0\)</span> is true and <span class="math inline">\(H_1\)</span> is not, or that <span class="math inline">\(H_0\)</span> is better supported by the data than <span class="math inline">\(H_1\)</span>, merely that the data does not provide sufficient evidence to reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>.</p>
<p>A hypothesis test is defined by its <em>critical region</em> or <em>rejection region</em>, which we shall denote by <span class="math inline">\(C\)</span>. <span class="math inline">\(C\)</span> is a subset of <span class="math inline">\(\mathbb{R}^n\)</span> and is the set of possible <span class="math inline">\(\bm{y}\)</span> which would lead to rejection of <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>, <em>i.e.</em></p>
<ul>
<li>If <span class="math inline">\(\bm{y} \in C\)</span>, <span class="math inline">\(H_0\)</span> is rejected in favour of <span class="math inline">\(H_1\)</span>;</li>
<li>If <span class="math inline">\(\bm{y} \not\in C\)</span>, <span class="math inline">\(H_0\)</span> is not rejected.</li>
</ul>
<p>As <span class="math inline">\(\bm Y\)</span> is a random variable, there remains the possibility that a hypothesis test will produce an erroneous result. We define the <em>size</em> (or <em>significance level</em>) of the test <span class="math display">\[\alpha = \max_{\bm \theta\in\Theta^{(0)}}P(\bm Y\in C;\bm \theta)\]</span> This is the maximum probability of erroneously rejecting <span class="math inline">\(H_0\)</span>, over all possible distributions for <span class="math inline">\(\bm Y\)</span> implied by <span class="math inline">\(H_0\)</span>. We also define the power function <span class="math display">\[\omega(\bm \theta)= P(\bm Y\in C;\bm \theta)\]</span> It represents the probability of rejecting <span class="math inline">\(H_0\)</span> for a particular value of <span class="math inline">\(\bm \theta\)</span>. Clearly we would like to find a test with where <span class="math inline">\(\omega(\bm \theta)\)</span> is large for every <span class="math inline">\(\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}\)</span>, while at the same time avoiding erroneous rejection of <span class="math inline">\(H_0\)</span>. In other words, a good test will have small size, but large power.</p>
<p>The general hypothesis testing procedure is to fix <span class="math inline">\(\alpha\)</span> to be some small value (often 0.05), so that the probability of erroneous rejection of <span class="math inline">\(H_0\)</span> is limited. In doing this, we are giving <span class="math inline">\(H_0\)</span> precedence over <span class="math inline">\(H_1\)</span>. Given our specified <span class="math inline">\(\alpha\)</span>, we try to choose a test, defined by its rejection region <span class="math inline">\(C\)</span>, to make <span class="math inline">\(\omega(\bm \theta)\)</span> as large as possible for <span class="math inline">\(\bm \theta\in\Theta^{(1)}\setminus\Theta^{(0)}\)</span>.</p>
<p>Suppose that <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> both take the same parametric form, <span class="math inline">\(f_{\bm Y}(\bm{y};\bm \theta)\)</span> with <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> for <span class="math inline">\(H_0\)</span> and <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span> for <span class="math inline">\(H_1\)</span>, where <span class="math inline">\(\Theta^{(0)}\)</span> and <span class="math inline">\(\Theta^{(1)}\)</span> are alternative sets of possible values for <span class="math inline">\(\bm \theta\)</span>. A <em>generalised likelihood ratio test</em> of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span> has a critical region of the form <span class="math display">\[C=\left\{ \bm{y}: 
\frac{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)} 
&gt;k\right\}\]</span> where <span class="math inline">\(k\)</span> is determined by <span class="math inline">\(\alpha\)</span>, the size of the test, so <span class="math display">\[\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.\]</span> Therefore, we will only reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(H_1\)</span> offers a distribution for <span class="math inline">\(Y_1, \ldots, Y_n\)</span> which makes the observed data much more probable than any distribution under <span class="math inline">\(H_0\)</span>. This is intuitively appealing and tends to produce good tests (large power) across a wide range of examples.</p>
<p><strong>Example (Bernoulli)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Suppose that we require a size <span class="math inline">\(\alpha\)</span> test of the hypothesis <span class="math inline">\(H_0\)</span>: <span class="math inline">\(p=p_0\)</span> against the general alternative <span class="math inline">\(H_1\)</span>: ‘<span class="math inline">\(p\)</span> is unrestricted’ where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(p_0\)</span> are specified.</p>
Here <span class="math inline">\(\bm \theta=(p)\)</span>, <span class="math inline">\(\Theta^{(0)}=\{p_0\}\)</span> and <span class="math inline">\(\Theta^{(1)}=(0,1)\)</span> and the generalised likelihood ratio test rejects <span class="math inline">\(H_0\)</span> when <span class="math display">\[{{\max_{p\in(0,1)}L(p)}\over{\max_{p=p_0}L(p)}} &gt; k\]</span> or equivalently when <span class="math display">\[{{\bar y^{\sum_i y_i}(1-\bar y)^{n-\sum_i y_i}}\over {p_0^{\sum_i y_i}(1-p_0)^{n-\sum_i y_i}}} &gt; k\]</span> or
<span class="math display" id="eq:her1">\[\begin{equation}
\left({{\bar y}\over{p_0}}\right)^{n\bar y}
\left({{1-\bar y}\over{1-p_0}}\right)^{n(1-\bar y)} &gt;k. 
  \tag{1.3}
\end{equation}\]</span>
<p>Now the left hand side of <a href="prelim.html#eq:her1">(1.3)</a> is minimised as a function of <span class="math inline">\(\bar y\)</span> at <span class="math inline">\(\bar y=p_0\)</span> and increases as <span class="math inline">\(\bar y\)</span> moves away from <span class="math inline">\(p_0\)</span> in either direction. Therefore, the rejection region <a href="prelim.html#eq:her1">(1.3)</a> is equivalent to <span class="math display">\[
C=\left\{ \bm{y}:\bar y &gt; k&#39; \text{ or } \bar y &lt; k&#39;&#39;\right\}
\]</span> where <span class="math inline">\(k&#39;\)</span> and <span class="math inline">\(k&#39;&#39;\)</span> are chosen so that <span class="math display">\[
P(\bm{y}\in C;p_0)=\alpha.
\]</span> Therefore, we can use the binomial<span class="math inline">\((n,p_0)\)</span> distribution to find a precise rejection region for a test of specified size <span class="math inline">\(\alpha\)</span>.</p>
<p>Alternatively, if <span class="math inline">\(n\)</span> is large, we can use the asymptotic distribution of <span class="math inline">\(\bar y\)</span>, <span class="math inline">\(N(p_0,p_0[1-p_0]/n)\)</span>.</p>
</div>
<div id="sn:lrt" class="section level3">
<h3><span class="header-section-number">1.4.7</span> The log-likelihood ratio statistic</h3>
<p>A <em>generalised likelihood ratio test</em> of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span> has a critical region of the form <span class="math display">\[
C=\left\{ \bm{y}:{{\max_{\bm \theta\in \Theta^{(1)}} L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}} &gt;k\right\}
\]</span> where <span class="math inline">\(k\)</span> is determined by <span class="math inline">\(\alpha\)</span>, the size of the test, so <span class="math display">\[
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
\]</span></p>
<p>Therefore, in order to determine <span class="math inline">\(k\)</span>, we need to know the distribution of the likelihood ratio, or an equivalent statistic, under <span class="math inline">\(H_0\)</span>. In general, this will not be available to us. However, we can make use of an important asymptotic result.</p>
<p>First we notice that, as <span class="math inline">\(\log\)</span> is a strictly increasing function, the rejection region is equivalent to</p>
<p><span class="math display">\[
C=\left\{ \bm{y}: 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right) &gt;k&#39;\right\}
\]</span> where <span class="math display">\[
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm \theta)=\alpha.
\]</span> Now, provided that <span class="math inline">\(H_0\)</span> is <em>nested within</em> <span class="math inline">\(H_1\)</span>, in other words <span class="math inline">\(\Theta^{(0)}\subset\Theta^{(1)}\)</span> (<span class="math inline">\(\Theta^{(0)}\)</span> is a subspace of <span class="math inline">\(\Theta^{(1)}\)</span>) then under <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span>, asymptotically as <span class="math inline">\(n\to\infty\)</span> <span class="math display">\[
L_{01}\equiv 2\log \left({{\max_{\bm \theta\in \Theta^{(1)}}L(\bm \theta)}\over
{\max_{\bm \theta\in \Theta^{(0)}}L(\bm \theta)}}\right)
\]</span> has a chi-squared distribution with degrees of freedom equal to the difference in the dimensions of <span class="math inline">\(\Theta^{(1)}\)</span> and <span class="math inline">\(\Theta^{(0)}\)</span>.</p>

<p><strong>Example (Bernoulli)</strong>. <span class="math inline">\(y_1, \ldots, y_n\)</span> are observations of <span class="math inline">\(Y_1, \ldots, Y_n\)</span>, i.i.d. Bernoulli<span class="math inline">\((p)\)</span> random variables. Suppose that we require a size <span class="math inline">\(\alpha\)</span> test of the hypothesis <span class="math inline">\(H_0\)</span>: <span class="math inline">\(p=p_0\)</span> against the general alternative <span class="math inline">\(H_1\)</span>: ‘<span class="math inline">\(p\)</span> is unrestricted’ where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(p_0\)</span> are specified.</p>
<p>Here <span class="math inline">\(\bm \theta=(p)\)</span>, <span class="math inline">\(\Theta^{(0)}=\{p_0\}\)</span> and <span class="math inline">\(\Theta^{(1)}=(0,1)\)</span> and the log likelihood ratio statistic is <span class="math display">\[
L_{01}=2n\bar y\log\left({{\bar y}\over{p_0}}\right)
+2n(1-\bar y)\log\left({{1-\bar y}\over{1-p_0}}\right).
\]</span> As <span class="math inline">\(d_1=1\)</span> and <span class="math inline">\(d_0=0\)</span>, under <span class="math inline">\(H_0\)</span>, the log likelihood ratio statistic has an asymptotic <span class="math inline">\(\chi^2_1\)</span> distribution. For a log likelihood ratio test, we only reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> when the test statistic is too large (observed data are much more probable under model <span class="math inline">\(H_1\)</span> than under model <span class="math inline">\(H_0\)</span>), so in this case we reject <span class="math inline">\(H_0\)</span> when the observed value of the test statistic above is ‘too large’ to have come from a <span class="math inline">\(\chi^2_1\)</span> distribution. What we mean by ‘too large’ depends on the significance level <span class="math inline">\(\alpha\)</span> of the test. For example, if <span class="math inline">\(\alpha=0.05\)</span>, a common choice, then we should reject <span class="math inline">\(H_0\)</span> if the test statistic is greater than the 3.84, the 95% point of the <span class="math inline">\(\chi^2_1\)</span> distribution.</p>
<!-- \begin{figure}[hbt] -->
<!-- \begin{center} -->
<!-- \includegraphics[height=3in]{chsq1} -->
<!-- \end{center} -->
<!-- \caption{\protect\small\baselineskip .4 true cm   -->
<!-- The $\chi^2_1$ distribution } -->
<!-- \label{fig:chsq1} -->
<!-- \end{figure} -->
<!-- \end{exampl} -->
</div>
</div>
<div id="sn:lm" class="section level2">
<h2><span class="header-section-number">1.5</span> Linear Models</h2>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Introduction</h3>
<p>In practical applications, we often distinguish between a <em>response</em> variable and a group of <em>explanatory</em> variables. The aim is to determine the pattern of dependence of the response variable on the explanatory variables. We denote the <span class="math inline">\(n\)</span> observations of the response variable by <span class="math inline">\(\bm{y}=(y_1,y_2,\ldots ,y_n)^T\)</span>. These are assumed to be observations of <em>random variables</em> <span class="math inline">\(\bm Y=(Y_1,Y_2,\ldots ,Y_n)^T\)</span>. Associated with each <span class="math inline">\(y_i\)</span> is a vector <span class="math inline">\(\bm{x}_i=(x_{i1},x_{i2},\ldots ,x_{ip})^T\)</span> of values of <span class="math inline">\(p\)</span> explanatory variables.</p>
In a linear model, we assume that
<span class="math display" id="eq:lmNonMat">\[\begin{align}
Y_i&amp;= \beta_1 x_{i1} +\beta_2 x_{i2} +\ldots + \beta_p x_{ip} + \epsilon_i \cr
&amp;= \sum_{j=1}^p x_{ij} \beta_j + \epsilon_i \cr
&amp;=  \bm{x}_i^T\bm{\beta} + \epsilon_i \cr
&amp;= [\bm{X}\bm{\beta}]_i + \epsilon_i,\qquad i=1,\ldots ,n  \tag{1.4}
\end{align}\]</span>
where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> independently, <span class="math display">\[\bm{X}= \begin{pmatrix}
\bm{x}_1^T \\
\vdots \\
\bm{x}_n^T
\end{pmatrix}
=\begin{pmatrix}
x_{11} &amp; \cdots &amp;x_{1p}\cr
\vdots &amp; \ddots &amp;\vdots\cr
x_{n1} &amp;\cdots &amp;x_{np}
\end{pmatrix}\]</span> and <span class="math inline">\(\bm{\beta}=(\beta_1,\ldots ,\beta_p)^T\)</span> is a vector of fixed but unknown parameters describing the dependence of <span class="math inline">\(Y_i\)</span> on <span class="math inline">\(\bm{x}_i\)</span>. The four ways of describing the linear model in <a href="prelim.html#eq:lmNonMat">(1.4)</a> are equivalent, but the most economical is the matrix form
<span class="math display" id="eq:lmMat">\[\begin{equation}
\bm{Y}=\bm{X}\bm{\beta} + \bm{\epsilon}. \tag{1.5}
\end{equation}\]</span>
<p>where <span class="math inline">\(\bm{\epsilon}=(\epsilon_1,\epsilon_2,\ldots ,\epsilon_n)^T\)</span>.</p>
<p>The <span class="math inline">\(n\times p\)</span> matrix <span class="math inline">\(\bm{X}\)</span> consists of known (observed) constants and is called the <em>design matrix</em>. The <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\bm{X}\)</span> is <span class="math inline">\(\bm{x}_i^T\)</span>, the explanatory data corresponding to the <span class="math inline">\(i\)</span>th observation of the response. The <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\bm{X}\)</span> contains the <span class="math inline">\(n\)</span> observations of the <span class="math inline">\(j\)</span>th explanatory variable.</p>
<p>The error vector <span class="math inline">\(\bm{\epsilon}\)</span> has a multivariate normal distribution with mean vector <span class="math inline">\({\bf 0}\)</span> and variance covariance matrix <span class="math inline">\(\sigma^2\bm{I}\)</span>, since <span class="math inline">\(\text{Var}(\epsilon_i)=\sigma^2\)</span>, and <span class="math inline">\(\text{Cov}(\epsilon_i,\epsilon_j)=0\)</span> as <span class="math inline">\(\epsilon_1, \ldots, \epsilon_n\)</span> are independent of one another. It follows from <a href="prelim.html#eq:lmMat">(1.5)</a> that the distribution of <span class="math inline">\(\bm Y\)</span> is multivariate normal with mean vector <span class="math inline">\(\bm{X}\bm{\beta}\)</span> and variance covariance matrix <span class="math inline">\(\sigma^2\bm{I}\)</span>, <em>i.e.</em> <span class="math inline">\(\bm Y\sim N(\bm{X}\bm{\beta},\sigma^2\bm{I})\)</span>.</p>
<div id="example-the-null-model" class="section level4 unnumbered">
<h4>Example: The null model</h4>
<p>The null model <span class="math display">\[
Y_i=\beta_1 + \epsilon_i \qquad i = 1, \ldots, n
\]</span> <span class="math display">\[
\bm{X}=\begin{pmatrix}
1\cr
1\cr
\vdots
\cr 1
\end{pmatrix}
\qquad
\bm{\beta}=(\beta_1).
\]</span> One (dummy) explanatory variable. In practice, this variable is present in all models.</p>
</div>
<div id="example-simple-linear-regression" class="section level4 unnumbered">
<h4>Example: simple linear regression</h4>
<p>Simple linear regression <span class="math display">\[
Y_i=\beta_1+\beta_2 x_i + \epsilon_i \qquad i = 1, \ldots, n
\]</span> <span class="math display">\[
\bm{X}=\begin{pmatrix} 1&amp;x_1\cr 1&amp;x_2\cr \vdots&amp;\vdots\cr 1&amp;x_n \end{pmatrix}
\qquad \bm{\beta}=\begin{pmatrix} \beta_1\cr\beta_2 \end{pmatrix}
\]</span> Two explanatory variables; the dummy variable and one ‘real’ variable.</p>
</div>
<div id="example-polynomial-regression" class="section level4 unnumbered">
<h4>Example: Polynomial regression</h4>
<p><span class="math display">\[
Y_i=\beta_1+\beta_2 x_i+\beta_3 x_i^2 +\ldots +\beta_p x_i^{p-1} + \epsilon_i \qquad i = 1, \ldots, n
\]</span> <span class="math display">\[
\bm{X}= \begin{pmatrix}
1&amp;x_1&amp;x_1^2&amp;\cdots&amp;x_1^{p-1}\cr
1&amp;x_2&amp;x_2^2&amp;\cdots&amp;x_2^{p-1}\cr
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr
1&amp;x_n&amp;x_n^2&amp;\cdots&amp;x_n^{p-1} \end{pmatrix}
\qquad \bm{\beta}= \begin{pmatrix} \beta_1\cr\beta_2\cr\vdots\cr\beta_p \end{pmatrix}
\]</span> <span class="math inline">\(p\)</span> explanatory variables; the dummy variable and one ‘real’ variable, transformed to <span class="math inline">\(p-1\)</span> variables.</p>
</div>
<div id="example-multiple-regression" class="section level4 unnumbered">
<h4>Example: Multiple regression</h4>
<p><span class="math display">\[
Y_i=\beta_1+\beta_2 x_{i1}+\beta_3 x_{i2} +\ldots +\beta_p x_{i\,p-1} + \epsilon_i \qquad i = 1, \ldots, n
\]</span> <span class="math display">\[
\bm{X}= \begin{pmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1\,p-1}\cr
1&amp;x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2\,p-1}\cr
\vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\cr
1&amp;x_{n1}&amp;x_{n2}&amp;\cdots&amp;x_{n\,p-1} 
\end{pmatrix}
\qquad \bm{\beta}=\begin{pmatrix} \beta_1\cr\beta_2\cr\vdots\cr\beta_p \end{pmatrix}
\]</span> <span class="math inline">\(p\)</span> explanatory variables; the dummy variable and <span class="math inline">\(p-1\)</span> ‘real’ variables.</p>
</div>
</div>
<div id="maximum-likelihood-estimation-1" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Maximum likelihood estimation</h3>
<p>The regression coefficients <span class="math inline">\(\beta_1, \ldots, \beta_p\)</span> describe the pattern by which the response depends on the explanatory variables. We use the observed data <span class="math inline">\(y_1, \ldots, y_n\)</span> to <em>estimate</em> this pattern of dependence.</p>
The likelihood for a linear model is
<span class="math display" id="eq:lmLikelihood">\[\begin{equation}
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{x}_i^T\bm{\beta})^2\right).
\tag{1.6}
\end{equation}\]</span>
<p>This is maximised with respect to <span class="math inline">\((\bm{\beta},\sigma^2)\)</span> at <span class="math display">\[
\hat{\bm{\beta}}=(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
\]</span> and <span class="math display">\[
\hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n \left(y_i-\bm{x}_i^T\hat{\bm{\beta}}\right)^2.
\]</span></p>
<p>The corresponding fitted values are <span class="math display">\[\hat{\bm{y}}=\bm{X}\hat{\bm{\beta}}=\bm{X}(\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}\]</span> or <span class="math display">\[\hat{y}_i=\bm{x}_i^T\hat{\bm{\beta}}, \quad i = 1, \ldots, n.\]</span></p>
<p>The residuals <span class="math inline">\(\bm{r} = (r_1, \ldots, r_n)\)</span> are <span class="math inline">\(\bm{r}=\bm{y}-\bm{\hat y}\)</span> or <span class="math inline">\(r_i=y_i-\bm{x}_i^T\hat{\bm{\beta}}\)</span> for <span class="math inline">\(i = 1, \ldots, n.\)</span>. These residuals describe the variability in the observed responses <span class="math inline">\(y_1, \ldots, y_n\)</span> which has not been explained by the linear model. We call <span class="math display">\[D= \sum_{i=1}^n r_i^2 = \sum_{i=1}^n \left(y_i-\bm{x}_i^T\hat{\bm{\beta}}\right)^2\]</span> the <em>residual sum of squares</em> or <em>deviance</em> for the linear model.</p>
</div>
<div id="properties-of-the-mle" class="section level3">
<h3><span class="header-section-number">1.5.3</span> Properties of the MLE</h3>
<p>As <span class="math inline">\(\bm Y\)</span> is normally distributed, and <span class="math inline">\(\hat{\bm{\beta}}= (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{Y}\)</span> is a linear function of <span class="math inline">\(\bm Y\)</span>, then <span class="math inline">\(\hat{\bm{\beta}}\)</span> must also be normally distributed. We have <span class="math inline">\(E(\hat{\bm{\beta}})=\bm{\beta}\)</span> and <span class="math inline">\(\text{Var}(\hat{\bm{\beta}})=\sigma^2(\bm{X}^T\bm{X})^{-1}\)</span>, so <span class="math display">\[\hat{\bm{\beta}} \sim N(\bm{\beta}, \sigma^2(\bm{X}^T\bm{X})^{-1}).\]</span></p>
<p>It is possible to prove (although we shall not do so here) that <span class="math display">\[
{D\over\sigma^2}\sim\chi^2_{n-p}
\]</span> which implies that <span class="math display">\[
E(\hat \sigma^2)={{n-p}\over n}\sigma^2,
\]</span> so the maximum likelihood estimator is biased for <span class="math inline">\(\sigma^2\)</span> (although still asymptotically unbiased as <span class="math inline">\({{n-p}\over n}\to 1\)</span> as <span class="math inline">\(n\to\infty\)</span>). We often use the unbiased estimator of <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
\tilde \sigma^2={D\over {n-p}}={1\over {n-p}}\sum_{i=1}^n r_i^2.
\]</span> The denominator <span class="math inline">\(n-p\)</span>, the number of observations minus the number of linear coefficients in the model is called the <em>degrees of freedom</em> of the model. Therefore, we estimate the residual variance by the deviance divided by the degrees of freedom.</p>
</div>
<div id="comparing-linear-models" class="section level3">
<h3><span class="header-section-number">1.5.4</span> Comparing linear models</h3>
<p>If we have a set of competing linear models which might explain the dependence of the response on the explanatory variables, we will want to determine which of the models is most appropriate.</p>
<p>As described previously, we proceed by comparing models pairwise using a generalised likelihood ratio test. For linear models this kind of comparison is restricted to situations where one of the models, <span class="math inline">\(H_0\)</span>, is <em>nested</em> in the other, <span class="math inline">\(H_1\)</span>. This usually means that the explanatory variables present in <span class="math inline">\(H_0\)</span> are a subset of those present in <span class="math inline">\(H_1\)</span>. In this case model <span class="math inline">\(H_0\)</span> is a special case of model <span class="math inline">\(H_1\)</span>, where certain coefficients are set equal to zero. We let <span class="math inline">\(\bm \theta\)</span> represent the collection of linear parameters for model <span class="math inline">\(H_1\)</span>, together with the residual variance <span class="math inline">\(\sigma^2\)</span>, and let <span class="math inline">\(\Theta^{(1)}\)</span> be the unrestricted parameter space for <span class="math inline">\(\bm \theta\)</span>. Then <span class="math inline">\(\Theta^{(0)}\)</span> is the parameter space corresponding to model <span class="math inline">\(H_0\)</span>, <em>i.e.</em> with the appropriate coefficients constrained to zero.</p>
<p>We will assume that model <span class="math inline">\(H_1\)</span> contains <span class="math inline">\(p\)</span> linear parameters and model <span class="math inline">\(H_0\)</span> a subset of <span class="math inline">\(q&lt;p\)</span> of these. Without loss of generality, we can think of <span class="math inline">\(H_1\)</span> as the model <span class="math display">\[
Y_i=\sum_{j=1}^p x_{ij} \beta_j + \epsilon_i, \quad i = 1, \ldots, n
\]</span> and <span class="math inline">\(H_0\)</span> being the same model with <span class="math display">\[\beta_{q+1}=\beta_{q+2}=\cdots=\beta_p=0.
\]</span></p>
Now, a generalised likelihood ratio test of <span class="math inline">\(H_0\)</span> against <span class="math inline">\(H_1\)</span> has a critical region of the form <span class="math display">\[
C=\left\{ \bm{y}:{{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(1)}}L(\bm{\beta},\sigma^2)}\over
{\max_{(\bm{\beta},\sigma^2)\in \Theta^{(0)}}L(\bm{\beta},\sigma^2)}} &gt;k\right\}
\]</span> where <span class="math inline">\(k\)</span> is determined by <span class="math inline">\(\alpha\)</span>, the size of the test, so <span class="math display">\[
\max_{\bm \theta\in\Theta^{(0)}}P(\bm{y}\in C;\bm{\beta},\sigma^2)=\alpha.
\]</span> For a linear model, <span class="math display">\[
L(\bm{\beta},\sigma^2)=\left(2\pi\sigma^2\right)^{-{n\over 2}}
\exp\left(-{1\over{2\sigma^2}} \sum_{i=1}^n (y_i-\bm{x}_i^T\bm{\beta})^2\right).
\]</span> This is maximised with respect to <span class="math inline">\((\bm{\beta},\sigma^2)\)</span> at <span class="math inline">\(\bm{\beta}=\hat{\bm{\beta}}\)</span> and <span class="math inline">\(\sigma^2=\hat \sigma^2=D/n\)</span>. Therefore
<span class="math display">\[\begin{align*}
\max_{\bm{\beta},\sigma^2} L(\bm{\beta},\sigma^2)&amp;=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over{2D}} \sum_{i=1}^n (y_i-\bm{x}_i^T\hat{\bm{\beta}})^2\right)\cr
&amp;=(2\pi D/n)^{-{n\over 2}}
\exp\left(-{n\over2}\right).
\end{align*}\]</span>
<p>This form applies for both <span class="math inline">\(\bm \theta\in\Theta^{(0)}\)</span> and <span class="math inline">\(\bm \theta\in\Theta^{(1)}\)</span>, with only the model changing. Let the deviances under models <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_1\)</span> be denoted by <span class="math inline">\(D_0\)</span> and <span class="math inline">\(D_1\)</span> respectively. Then the critical region for the generalised likelihood ratio test is of the form <span class="math display">\[{{(2\pi D_1/n)^{-{n\over 2}}}\over{(2\pi D_0/n)^{-{n\over 2}}}}&gt;k\]</span> so <span class="math display">\[\left({{D_0}\over{D_1}}\right)^{n\over 2}&gt;k,\]</span> and <span class="math display">\[\left({{D_0}\over{D_1}}-1\right){{n-p}\over{p-q}}&gt;k&#39;\]</span> for some <span class="math inline">\(k&#39;\)</span>. Rearranging, <span class="math display">\[{{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}&gt;k&#39;.\]</span> We refer to the left hand side of this inequality as the <span class="math inline">\(F\)</span>-statistic. We reject the simpler model <span class="math inline">\(H_0\)</span> in favour of the more complex model <span class="math inline">\(H_1\)</span> if <span class="math inline">\(F\)</span> is ‘too large’.</p>
<p>As we have required <span class="math inline">\(H_0\)</span> to be nested in <span class="math inline">\(H_1\)</span>, <span class="math inline">\(F \sim F_{p-q, \, n-p}\)</span> when <span class="math inline">\(H_0\)</span> is true. To see this, note that <span class="math display">\[
{{D_0}\over\sigma^2}={{D_0-D_1}\over\sigma^2}+{{D_1}\over\sigma^2}.
\]</span> Furthermore, under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(D_1/\sigma^2 \sim \chi^2_{n-p}\)</span> and <span class="math inline">\(D_0/\sigma^2 \sim \chi^2_{n-q}\)</span>. It is possible to show (although we will not do so here) that under <span class="math inline">\(H_0\)</span>, <span class="math inline">\((D_0-D_1)/\sigma^2\)</span> and <span class="math inline">\(D_0/\sigma^2\)</span> are independent. Therefore, from the properties of the chi-squared distribution, it follows that under <span class="math inline">\(H_0\)</span>, <span class="math inline">\((D_0-D_1)/\sigma^2 \sim \chi^2_{p-q}\)</span>, and <span class="math inline">\(F \sim F_{p-q,\,n-p}\)</span> distribution.</p>
<p>Therefore, the precise critical region can be evaluated given the size, <span class="math inline">\(\alpha\)</span>, of the test. We reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span> when <span class="math display">\[
{{(D_0-D_1)/(p-q)}\over{D_1/(n-p)}}&gt;k
\]</span> where <span class="math inline">\(k\)</span> is the <span class="math inline">\(100(1-\alpha)\%\)</span> point of the <span class="math inline">\(F_{p-q,\,n-p}\)</span> distribution.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/heogden/math3091/edit/master/01-prelim.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH3091.pdf", "MATH3091.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
